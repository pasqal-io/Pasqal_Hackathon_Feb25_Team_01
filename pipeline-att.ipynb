{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, GATConv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "n_clinical = 38 \n",
    "n_image_nodes = 6*6\n",
    "n_nodes = n_clinical + n_image_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples:  84\n",
      "Test Samples:  21\n",
      "Train labels shape: torch.Size([84])\n",
      "Test labels shape: torch.Size([21])\n"
     ]
    }
   ],
   "source": [
    "# Load Ground-Truth Values\n",
    "train_labels = pd.read_csv(\"data/labels/train_labels.csv\")\n",
    "train_labels = train_labels.iloc[:, 1].tolist()                 # (n_train,)\n",
    "test_labels = pd.read_csv(\"data/labels/test_labels.csv\")\n",
    "test_labels = test_labels.iloc[:, 1].tolist()                   # (n_test,)\n",
    "\n",
    "n_train = len(train_labels) # 84\n",
    "n_test = len(test_labels)   # 21\n",
    "\n",
    "print('Training Samples: ', n_train)\n",
    "print('Test Samples: ', n_test)\n",
    "\n",
    "# Convert to tensors\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "print(\"Train labels shape:\", train_labels.shape)                # Should be (n_train,)\n",
    "print(\"Test labels shape:\", test_labels.shape)                  # Should be (n_test,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Image Embeddings:  (84, 6, 6, 128)\n",
      "Train Clinical Embeddings:  (84, 38, 128)\n",
      "Test Image Embeddings:  (21, 6, 6, 128)\n",
      "Test Clinical Embeddings:  (21, 38, 128)\n"
     ]
    }
   ],
   "source": [
    "# Load and normalise Embeddings\n",
    "train_image_embeddings = np.load(\"data/image_embeddings/train_image_embeddings.npy\")             # (n_train, 6, 6, embed_dim)\n",
    "train_clinical_embeddings = np.load(\"data/clinical_data/train_embeddings.npy\")          # (n_train, 38, embed_dim)\n",
    "test_image_embeddings = np.load(\"data/image_embeddings/test_image_embeddings.npy\")               # (n_test, 6, 6, embed_dim)\n",
    "test_clinical_embeddings = np.load(\"data/clinical_data/test_embeddings.npy\")            # (n_test, 38, embed_dim)\n",
    "\n",
    "print(\"Train Image Embeddings: \", train_image_embeddings.shape)\n",
    "print(\"Train Clinical Embeddings: \", train_clinical_embeddings.shape)\n",
    "print(\"Test Image Embeddings: \",test_image_embeddings.shape)\n",
    "print(\"Test Clinical Embeddings: \", test_clinical_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureAttention(torch.nn.Module):\n",
    "    def __init__(self, clinical_dim, image_dim):\n",
    "        super(FeatureAttention, self).__init__()\n",
    "        self.attn_layer = torch.nn.Linear(clinical_dim + image_dim, 2)\n",
    "\n",
    "    def forward(self, clinical, image):\n",
    "        # Flatten features to (batch_size, clinical_dim + image_dim) for attention scoring\n",
    "        clinical_flat = clinical.mean(dim=-1)       # Shape: (batch_size, clinical_dim)\n",
    "        image_flat = image.mean(dim=-1)             # Shape: (batch_size, image_dim)\n",
    "\n",
    "        combined = torch.cat([clinical_flat, image_flat], dim=1)                        # Shape: (batch_size, clinical_dim + image_dim)\n",
    "        attn_weights = torch.softmax(self.attn_layer(combined), dim=1)                  # Learn weight for each feature type\n",
    "\n",
    "        # Expand attention weights and apply to original features\n",
    "        attn_clinical = attn_weights[:, 0].unsqueeze(1).unsqueeze(-1) * clinical        # Shape: (batch, clinical_dim, 128)\n",
    "        attn_image = attn_weights[:, 1].unsqueeze(1).unsqueeze(-1) * image              # Shape: (batch, image_dim, 128)\n",
    "\n",
    "        return torch.cat([attn_clinical, attn_image], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_attention = FeatureAttention(n_clinical, n_image_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Train Image Embeddings:  torch.Size([84, 36, 128])\n",
      "Combined Train Embeddings:  torch.Size([84, 74, 128])\n",
      "Reshaped Test Image Embeddings:  torch.Size([21, 36, 128])\n",
      "Combined Test Embeddings:  torch.Size([21, 74, 128])\n"
     ]
    }
   ],
   "source": [
    "# Reshape image embeddings to match size of clinical embeddings\n",
    "train_image_features = torch.tensor(train_image_embeddings.reshape(n_train, 36, embed_dim))                             # Shape: [n_train, 36, embed_dim]\n",
    "test_image_features = torch.tensor(test_image_embeddings.reshape(n_test, 36, embed_dim))                                # Shape: [n_test, 36, embed_dim]\n",
    "\n",
    "# Combine clinical and image features\n",
    "# train_patient_features = torch.cat([torch.tensor(train_clinical_embeddings), train_image_features], dim=1)              # Shape: [n_train, 74, embed_dim]\n",
    "# test_patient_features = torch.cat([torch.tensor(test_clinical_embeddings), test_image_features], dim=1)                 # Shape: [n_test, 74, embed_dim]\n",
    "\n",
    "# Feature Attention \n",
    "train_patient_features = feature_attention(torch.tensor(train_clinical_embeddings).float(), train_image_features.float())  \n",
    "test_patient_features = feature_attention(torch.tensor(test_clinical_embeddings).float(), test_image_features.float())  \n",
    "\n",
    "print('Reshaped Train Image Embeddings: ', train_image_features.shape)\n",
    "print('Combined Train Embeddings: ', train_patient_features.shape)\n",
    "print('Reshaped Test Image Embeddings: ', test_image_features.shape)\n",
    "print('Combined Test Embeddings: ', test_patient_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patient_edges(n_clinical, n_nodes):\n",
    "    \"\"\"\n",
    "    Creates bidirectional edges between clinical nodes and image nodes.\n",
    "    Adds a self-edge to each node.\n",
    "\n",
    "    Total edges = n_nodes (self-edges) + 2 * n_clinical * n_image_nodes (bidirectional edges)\n",
    "\n",
    "    Parameters:\n",
    "    - n_clinical: number of clinical nodes (for a specific patient)\n",
    "    - n_image_nodes: number of image nodes (for a specific patient)\n",
    "    \"\"\"\n",
    "    node_ids = np.expand_dims(np.arange(n_nodes, dtype=int), 0)\n",
    "    # self-edges = preserves some features of each own node during a graph convolution\n",
    "    self_edges = np.concatenate((node_ids, node_ids), 0)\n",
    "\n",
    "    # clinical nodes\n",
    "    c_array_asc = np.expand_dims(np.arange(n_clinical), 0)\n",
    "    all_edges = self_edges[:]\n",
    "\n",
    "    for i in range(n_clinical, n_nodes):\n",
    "        # image nodes\n",
    "        i_array = np.expand_dims(np.array([i]*n_clinical), 0)\n",
    "\n",
    "        # image --> clinical\n",
    "        inter_edges_ic = np.concatenate((i_array, c_array_asc), 0)\n",
    "        # clinical --> image\n",
    "        inter_edges_ci = np.concatenate((c_array_asc, i_array), 0)\n",
    "\n",
    "        # bidirectional edges\n",
    "        inter_edges_i = np.concatenate((inter_edges_ic, inter_edges_ci), 1)\n",
    "        all_edges = np.concatenate((all_edges, inter_edges_i), 1)\n",
    "\n",
    "    return torch.tensor(all_edges, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_list(patient_features, patient_labels):\n",
    "    \"\"\"\n",
    "    Generates a sub-graph for each patient given its embeddings\n",
    "\n",
    "    Parameters:\n",
    "    - patient_features: combined clinical and image embeddings of one patient\n",
    "    - patient_labels: groud truth values\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for i in range(len(patient_labels)):\n",
    "        # Create the graph for each patient\n",
    "        patient_edges = create_patient_edges(n_clinical, n_nodes)   # Shape: [2, num_edges]\n",
    "        patient_y = patient_labels[i]                               # Target label for this patient\n",
    "\n",
    "        data = Data(x=patient_features[i], edge_index=patient_edges, y=patient_y)\n",
    "        data_list.append(data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Patients:  84\n",
      "Test Patients:  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usama.Khatab\\Projects\\miniconda3\\envs\\hackathon\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_data_list = get_data_list(train_patient_features, train_labels)\n",
    "test_data_list = get_data_list(test_patient_features, test_labels)\n",
    "\n",
    "# Batch size 1 for individual patients\n",
    "train_loader = DataLoader(train_data_list, batch_size=1, shuffle=False, num_workers=0)  \n",
    "test_loader = DataLoader(test_data_list, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Train Patients: \", len(train_loader))\n",
    "print(\"Test Patients: \", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "We define the Graph Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)          # Second GCN layer\n",
    "        self.fc = torch.nn.Linear(hidden_channels, 1)                   # Fully connected layer for binary classification\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Apply graph convolution\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Global pooling (mean) across all nodes\n",
    "        x = global_mean_pool(x, batch)  # This will aggregate node features into one scalar per graph\n",
    "        \n",
    "        # Pass the aggregated feature through a fully connected layer to get a single logit\n",
    "        x = self.fc(x)  # Output size is (batch_size, 1)\n",
    "        return x  # Output a single logit for each patient (before applying sigmoid in loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Graph Attention Network\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, heads=2, dropout=0.5):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, dropout=dropout)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, 1)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)          # Aggregate node features\n",
    "        x = self.fc(x)                          # Binary classification output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Model Parameters\n",
    "learning_rate = 0.0001\n",
    "w_decay = 5e-4\n",
    "hidden_channels = 128\n",
    "\n",
    "# Initialize Model\n",
    "model = GCN(in_channels=embed_dim, hidden_channels=hidden_channels)\n",
    "# model = GAT(in_channels=embed_dim, hidden_channels=hidden_channels)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=w_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.6912400261277244\n",
      "Epoch 2/300, Loss: 0.6872171681551706\n",
      "Epoch 3/300, Loss: 0.684031455999329\n",
      "Epoch 4/300, Loss: 0.6809438119331995\n",
      "Epoch 5/300, Loss: 0.6777506130082267\n",
      "Epoch 6/300, Loss: 0.6743085199878329\n",
      "Epoch 7/300, Loss: 0.6705001244942347\n",
      "Epoch 8/300, Loss: 0.6662016993477231\n",
      "Epoch 9/300, Loss: 0.6612743203129087\n",
      "Epoch 10/300, Loss: 0.6555624909344173\n",
      "Epoch 11/300, Loss: 0.6488734597251529\n",
      "Epoch 12/300, Loss: 0.6409781727762449\n",
      "Epoch 13/300, Loss: 0.6316124821702639\n",
      "Epoch 14/300, Loss: 0.620543819453035\n",
      "Epoch 15/300, Loss: 0.607545524480797\n",
      "Epoch 16/300, Loss: 0.5924180788653237\n",
      "Epoch 17/300, Loss: 0.5750202387571335\n",
      "Epoch 18/300, Loss: 0.5553141218565759\n",
      "Epoch 19/300, Loss: 0.5333904663012141\n",
      "Epoch 20/300, Loss: 0.5095269639222395\n",
      "Epoch 21/300, Loss: 0.48414180481008123\n",
      "Epoch 22/300, Loss: 0.45774180477573756\n",
      "Epoch 23/300, Loss: 0.430853257221835\n",
      "Epoch 24/300, Loss: 0.4039263108833915\n",
      "Epoch 25/300, Loss: 0.37734432163692655\n",
      "Epoch 26/300, Loss: 0.35143597468378995\n",
      "Epoch 27/300, Loss: 0.32638684873070034\n",
      "Epoch 28/300, Loss: 0.3023017624038316\n",
      "Epoch 29/300, Loss: 0.27926591752717894\n",
      "Epoch 30/300, Loss: 0.25734542438849095\n",
      "Epoch 31/300, Loss: 0.2365710264116171\n",
      "Epoch 32/300, Loss: 0.21696242264338902\n",
      "Epoch 33/300, Loss: 0.19853345446643375\n",
      "Epoch 34/300, Loss: 0.18129941193564308\n",
      "Epoch 35/300, Loss: 0.16524590214803106\n",
      "Epoch 36/300, Loss: 0.15037329642412564\n",
      "Epoch 37/300, Loss: 0.13665498728819547\n",
      "Epoch 38/300, Loss: 0.12406916099800062\n",
      "Epoch 39/300, Loss: 0.11256379915756129\n",
      "Epoch 40/300, Loss: 0.10208450343149404\n",
      "Epoch 41/300, Loss: 0.09258010660275995\n",
      "Epoch 42/300, Loss: 0.08398348379636272\n",
      "Epoch 43/300, Loss: 0.07623125339159742\n",
      "Epoch 44/300, Loss: 0.06925134958666083\n",
      "Epoch 45/300, Loss: 0.06297471823026649\n",
      "Epoch 46/300, Loss: 0.057334800550181954\n",
      "Epoch 47/300, Loss: 0.05227117379850131\n",
      "Epoch 48/300, Loss: 0.047727264350257995\n",
      "Epoch 49/300, Loss: 0.04365014292471599\n",
      "Epoch 50/300, Loss: 0.039989542056684424\n",
      "Epoch 51/300, Loss: 0.036701962074619676\n",
      "Epoch 52/300, Loss: 0.03374479507121058\n",
      "Epoch 53/300, Loss: 0.03108486266518455\n",
      "Epoch 54/300, Loss: 0.02868878552440687\n",
      "Epoch 55/300, Loss: 0.026527648307299905\n",
      "Epoch 56/300, Loss: 0.02457651764451016\n",
      "Epoch 57/300, Loss: 0.02281228415085934\n",
      "Epoch 58/300, Loss: 0.021214324273473938\n",
      "Epoch 59/300, Loss: 0.01976530056847315\n",
      "Epoch 60/300, Loss: 0.01845008140835922\n",
      "Epoch 61/300, Loss: 0.017254137253898773\n",
      "Epoch 62/300, Loss: 0.016165299363603395\n",
      "Epoch 63/300, Loss: 0.015172370832477185\n",
      "Epoch 64/300, Loss: 0.014266367263362986\n",
      "Epoch 65/300, Loss: 0.013438331897694817\n",
      "Epoch 66/300, Loss: 0.012681030221193353\n",
      "Epoch 67/300, Loss: 0.011987205078358307\n",
      "Epoch 68/300, Loss: 0.011350897118280131\n",
      "Epoch 69/300, Loss: 0.010766621217910489\n",
      "Epoch 70/300, Loss: 0.010229817310963491\n",
      "Epoch 71/300, Loss: 0.009736233323478484\n",
      "Epoch 72/300, Loss: 0.009281655185900274\n",
      "Epoch 73/300, Loss: 0.008862851078793895\n",
      "Epoch 74/300, Loss: 0.008476590632199077\n",
      "Epoch 75/300, Loss: 0.008120288812538743\n",
      "Epoch 76/300, Loss: 0.007791432839786424\n",
      "Epoch 77/300, Loss: 0.007487745989194428\n",
      "Epoch 78/300, Loss: 0.007207211193894008\n",
      "Epoch 79/300, Loss: 0.006947758817096613\n",
      "Epoch 80/300, Loss: 0.006707928048940846\n",
      "Epoch 81/300, Loss: 0.00648576711596217\n",
      "Epoch 82/300, Loss: 0.0062801381139122115\n",
      "Epoch 83/300, Loss: 0.006089722682580552\n",
      "Epoch 84/300, Loss: 0.005913410224387744\n",
      "Epoch 85/300, Loss: 0.005750028653156256\n",
      "Epoch 86/300, Loss: 0.005598586187607864\n",
      "Epoch 87/300, Loss: 0.005458139433141609\n",
      "Epoch 88/300, Loss: 0.005327955702731353\n",
      "Epoch 89/300, Loss: 0.005207151434314298\n",
      "Epoch 90/300, Loss: 0.0050948460347648335\n",
      "Epoch 91/300, Loss: 0.004990495593509487\n",
      "Epoch 92/300, Loss: 0.004893482590237649\n",
      "Epoch 93/300, Loss: 0.004803259336767951\n",
      "Epoch 94/300, Loss: 0.004719229497779798\n",
      "Epoch 95/300, Loss: 0.004640745980872279\n",
      "Epoch 96/300, Loss: 0.004567451823320533\n",
      "Epoch 97/300, Loss: 0.00449882156230853\n",
      "Epoch 98/300, Loss: 0.004434516436612327\n",
      "Epoch 99/300, Loss: 0.00437391945065645\n",
      "Epoch 100/300, Loss: 0.004316854126002346\n",
      "Epoch 101/300, Loss: 0.0042628367156707795\n",
      "Epoch 102/300, Loss: 0.00421156720265225\n",
      "Epoch 103/300, Loss: 0.004162650470659303\n",
      "Epoch 104/300, Loss: 0.004115837287220832\n",
      "Epoch 105/300, Loss: 0.004070995585217523\n",
      "Epoch 106/300, Loss: 0.004027878960620174\n",
      "Epoch 107/300, Loss: 0.00398617409215755\n",
      "Epoch 108/300, Loss: 0.00394573819288607\n",
      "Epoch 109/300, Loss: 0.003906286910413266\n",
      "Epoch 110/300, Loss: 0.0038678370540202152\n",
      "Epoch 111/300, Loss: 0.0038301731396391653\n",
      "Epoch 112/300, Loss: 0.0037931922837569367\n",
      "Epoch 113/300, Loss: 0.0037568696872248048\n",
      "Epoch 114/300, Loss: 0.0037211308653717252\n",
      "Epoch 115/300, Loss: 0.0036860054096905515\n",
      "Epoch 116/300, Loss: 0.003651320222031375\n",
      "Epoch 117/300, Loss: 0.0036170191975716513\n",
      "Epoch 118/300, Loss: 0.0035831260289082172\n",
      "Epoch 119/300, Loss: 0.0035497007253243716\n",
      "Epoch 120/300, Loss: 0.0035165521318222524\n",
      "Epoch 121/300, Loss: 0.0034838934269871623\n",
      "Epoch 122/300, Loss: 0.0034517022537069613\n",
      "Epoch 123/300, Loss: 0.0034198669286308273\n",
      "Epoch 124/300, Loss: 0.003388487243899449\n",
      "Epoch 125/300, Loss: 0.0033575597317394568\n",
      "Epoch 126/300, Loss: 0.0033272067349961226\n",
      "Epoch 127/300, Loss: 0.00329725582304534\n",
      "Epoch 128/300, Loss: 0.0032677590596947646\n",
      "Epoch 129/300, Loss: 0.0032388383418774105\n",
      "Epoch 130/300, Loss: 0.003210320000237386\n",
      "Epoch 131/300, Loss: 0.0031823673079565025\n",
      "Epoch 132/300, Loss: 0.003155029413868587\n",
      "Epoch 133/300, Loss: 0.003128345441230825\n",
      "Epoch 134/300, Loss: 0.003102319914736048\n",
      "Epoch 135/300, Loss: 0.003076817925916874\n",
      "Epoch 136/300, Loss: 0.003052011598551222\n",
      "Epoch 137/300, Loss: 0.0030278096665781397\n",
      "Epoch 138/300, Loss: 0.0030042219023016514\n",
      "Epoch 139/300, Loss: 0.0029814135726891912\n",
      "Epoch 140/300, Loss: 0.0029593113877126598\n",
      "Epoch 141/300, Loss: 0.002937794670417913\n",
      "Epoch 142/300, Loss: 0.0029168994291727097\n",
      "Epoch 143/300, Loss: 0.0028967076594285787\n",
      "Epoch 144/300, Loss: 0.0028769502930680617\n",
      "Epoch 145/300, Loss: 0.0028579858033134812\n",
      "Epoch 146/300, Loss: 0.0028394477053336146\n",
      "Epoch 147/300, Loss: 0.0028216529524884423\n",
      "Epoch 148/300, Loss: 0.002804274337688098\n",
      "Epoch 149/300, Loss: 0.0027876491928639424\n",
      "Epoch 150/300, Loss: 0.0027715659257193607\n",
      "Epoch 151/300, Loss: 0.002756255820930251\n",
      "Epoch 152/300, Loss: 0.002741298907451329\n",
      "Epoch 153/300, Loss: 0.002727095764471631\n",
      "Epoch 154/300, Loss: 0.002713291494122435\n",
      "Epoch 155/300, Loss: 0.0027000305775194042\n",
      "Epoch 156/300, Loss: 0.0026874744708471725\n",
      "Epoch 157/300, Loss: 0.0026753576145589855\n",
      "Epoch 158/300, Loss: 0.002663938212078522\n",
      "Epoch 159/300, Loss: 0.0026528917930720907\n",
      "Epoch 160/300, Loss: 0.0026424482093716506\n",
      "Epoch 161/300, Loss: 0.002632468150425536\n",
      "Epoch 162/300, Loss: 0.0026230249377909957\n",
      "Epoch 163/300, Loss: 0.002614062082145773\n",
      "Epoch 164/300, Loss: 0.002605473728830735\n",
      "Epoch 165/300, Loss: 0.002597310842117752\n",
      "Epoch 166/300, Loss: 0.0025895236583978872\n",
      "Epoch 167/300, Loss: 0.0025822422606194906\n",
      "Epoch 168/300, Loss: 0.002575367600738467\n",
      "Epoch 169/300, Loss: 0.00256874601503417\n",
      "Epoch 170/300, Loss: 0.002562604673108754\n",
      "Epoch 171/300, Loss: 0.002556900124156008\n",
      "Epoch 172/300, Loss: 0.002551325725800693\n",
      "Epoch 173/300, Loss: 0.0025462750817972675\n",
      "Epoch 174/300, Loss: 0.002541450790799683\n",
      "Epoch 175/300, Loss: 0.002536817407114237\n",
      "Epoch 176/300, Loss: 0.0025324726365873913\n",
      "Epoch 177/300, Loss: 0.002528585427734513\n",
      "Epoch 178/300, Loss: 0.002524806095564037\n",
      "Epoch 179/300, Loss: 0.002521362391148397\n",
      "Epoch 180/300, Loss: 0.002518109672088204\n",
      "Epoch 181/300, Loss: 0.002515208862210524\n",
      "Epoch 182/300, Loss: 0.0025124934746123644\n",
      "Epoch 183/300, Loss: 0.0025101000584895486\n",
      "Epoch 184/300, Loss: 0.0025077977553727188\n",
      "Epoch 185/300, Loss: 0.002505676055945861\n",
      "Epoch 186/300, Loss: 0.0025036352966535127\n",
      "Epoch 187/300, Loss: 0.002501899639268924\n",
      "Epoch 188/300, Loss: 0.0025001725142872254\n",
      "Epoch 189/300, Loss: 0.0024986588423849924\n",
      "Epoch 190/300, Loss: 0.0024973274454775187\n",
      "Epoch 191/300, Loss: 0.0024960949708873244\n",
      "Epoch 192/300, Loss: 0.0024950387831354634\n",
      "Epoch 193/300, Loss: 0.0024940729736477978\n",
      "Epoch 194/300, Loss: 0.002493345863036839\n",
      "Epoch 195/300, Loss: 0.002492573586096114\n",
      "Epoch 196/300, Loss: 0.0024920948612166933\n",
      "Epoch 197/300, Loss: 0.0024916458872515554\n",
      "Epoch 198/300, Loss: 0.0024912040170463677\n",
      "Epoch 199/300, Loss: 0.0024908500984002742\n",
      "Epoch 200/300, Loss: 0.0024905454932065496\n",
      "Epoch 201/300, Loss: 0.0024903257468135314\n",
      "Epoch 202/300, Loss: 0.00249015138961979\n",
      "Epoch 203/300, Loss: 0.0024899599865464204\n",
      "Epoch 204/300, Loss: 0.00248987992511372\n",
      "Epoch 205/300, Loss: 0.0024898224797392948\n",
      "Epoch 206/300, Loss: 0.002489714313880887\n",
      "Epoch 207/300, Loss: 0.0024898558978661426\n",
      "Epoch 208/300, Loss: 0.0024897351972651364\n",
      "Epoch 209/300, Loss: 0.002489819121148598\n",
      "Epoch 210/300, Loss: 0.002489829550800745\n",
      "Epoch 211/300, Loss: 0.002489895179851807\n",
      "Epoch 212/300, Loss: 0.002489833639629069\n",
      "Epoch 213/300, Loss: 0.0024899624685141525\n",
      "Epoch 214/300, Loss: 0.0024901605192575345\n",
      "Epoch 215/300, Loss: 0.002490015757844265\n",
      "Epoch 216/300, Loss: 0.00249005123194885\n",
      "Epoch 217/300, Loss: 0.002490219580024478\n",
      "Epoch 218/300, Loss: 0.0024901892712434694\n",
      "Epoch 219/300, Loss: 0.0024902503065234107\n",
      "Epoch 220/300, Loss: 0.002490243662794542\n",
      "Epoch 221/300, Loss: 0.0024904288614682932\n",
      "Epoch 222/300, Loss: 0.0024904719910063293\n",
      "Epoch 223/300, Loss: 0.002490564225549211\n",
      "Epoch 224/300, Loss: 0.0024906047117924787\n",
      "Epoch 225/300, Loss: 0.0024906859538359207\n",
      "Epoch 226/300, Loss: 0.002490792722190755\n",
      "Epoch 227/300, Loss: 0.0024908261823275964\n",
      "Epoch 228/300, Loss: 0.002490907226534286\n",
      "Epoch 229/300, Loss: 0.0024910166871666747\n",
      "Epoch 230/300, Loss: 0.0024910427323231402\n",
      "Epoch 231/300, Loss: 0.002491101300360502\n",
      "Epoch 232/300, Loss: 0.0024910582610789874\n",
      "Epoch 233/300, Loss: 0.002491101495771935\n",
      "Epoch 234/300, Loss: 0.002491106327873976\n",
      "Epoch 235/300, Loss: 0.0024913415303065753\n",
      "Epoch 236/300, Loss: 0.00249122687342779\n",
      "Epoch 237/300, Loss: 0.0024911923846957507\n",
      "Epoch 238/300, Loss: 0.002491290982236803\n",
      "Epoch 239/300, Loss: 0.002491244249540614\n",
      "Epoch 240/300, Loss: 0.002491235309814025\n",
      "Epoch 241/300, Loss: 0.00249116863584017\n",
      "Epoch 242/300, Loss: 0.0024912700477909226\n",
      "Epoch 243/300, Loss: 0.0024911501857440314\n",
      "Epoch 244/300, Loss: 0.0024911318298888674\n",
      "Epoch 245/300, Loss: 0.002491104709709665\n",
      "Epoch 246/300, Loss: 0.002491131001728978\n",
      "Epoch 247/300, Loss: 0.0024911023438107804\n",
      "Epoch 248/300, Loss: 0.0024911582559503757\n",
      "Epoch 249/300, Loss: 0.002491116953024175\n",
      "Epoch 250/300, Loss: 0.0024910852369516975\n",
      "Epoch 251/300, Loss: 0.0024911298005463323\n",
      "Epoch 252/300, Loss: 0.002491073919372866\n",
      "Epoch 253/300, Loss: 0.0024911142623057544\n",
      "Epoch 254/300, Loss: 0.0024911390230824637\n",
      "Epoch 255/300, Loss: 0.0024909901392761854\n",
      "Epoch 256/300, Loss: 0.002491059314577017\n",
      "Epoch 257/300, Loss: 0.0024909358826334937\n",
      "Epoch 258/300, Loss: 0.00249085892534952\n",
      "Epoch 259/300, Loss: 0.0024907306655482637\n",
      "Epoch 260/300, Loss: 0.0024907339728174735\n",
      "Epoch 261/300, Loss: 0.002490525930365298\n",
      "Epoch 262/300, Loss: 0.0024905390347114355\n",
      "Epoch 263/300, Loss: 0.0024904593016496315\n",
      "Epoch 264/300, Loss: 0.0024902864181404723\n",
      "Epoch 265/300, Loss: 0.002490138675143715\n",
      "Epoch 266/300, Loss: 0.0024899135944343427\n",
      "Epoch 267/300, Loss: 0.00248975598944006\n",
      "Epoch 268/300, Loss: 0.0024897056794846626\n",
      "Epoch 269/300, Loss: 0.002489442423038348\n",
      "Epoch 270/300, Loss: 0.002489310229975672\n",
      "Epoch 271/300, Loss: 0.0024891922957110744\n",
      "Epoch 272/300, Loss: 0.002488916249127388\n",
      "Epoch 273/300, Loss: 0.002488740561169661\n",
      "Epoch 274/300, Loss: 0.0024885393462942113\n",
      "Epoch 275/300, Loss: 0.002488263304344617\n",
      "Epoch 276/300, Loss: 0.0024880423026323634\n",
      "Epoch 277/300, Loss: 0.0024878283039860044\n",
      "Epoch 278/300, Loss: 0.0024876497419384015\n",
      "Epoch 279/300, Loss: 0.002487429830103966\n",
      "Epoch 280/300, Loss: 0.002487175119529725\n",
      "Epoch 281/300, Loss: 0.0024869351366197364\n",
      "Epoch 282/300, Loss: 0.002486674266947375\n",
      "Epoch 283/300, Loss: 0.002486509466507414\n",
      "Epoch 284/300, Loss: 0.0024862433312072036\n",
      "Epoch 285/300, Loss: 0.0024860192981924186\n",
      "Epoch 286/300, Loss: 0.002485760137937004\n",
      "Epoch 287/300, Loss: 0.002485532136450361\n",
      "Epoch 288/300, Loss: 0.0024852094853226197\n",
      "Epoch 289/300, Loss: 0.0024849717680072423\n",
      "Epoch 290/300, Loss: 0.0024846136425150604\n",
      "Epoch 291/300, Loss: 0.0024843868846108393\n",
      "Epoch 292/300, Loss: 0.0024840910917278266\n",
      "Epoch 293/300, Loss: 0.002483847564950078\n",
      "Epoch 294/300, Loss: 0.002483581133501069\n",
      "Epoch 295/300, Loss: 0.0024833026084577015\n",
      "Epoch 296/300, Loss: 0.0024829715082974177\n",
      "Epoch 297/300, Loss: 0.002482754916811592\n",
      "Epoch 298/300, Loss: 0.0024824266315807194\n",
      "Epoch 299/300, Loss: 0.0024820853632696697\n",
      "Epoch 300/300, Loss: 0.0024817544970660707\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "train_losses = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for data in train_loader:                                               # Iterate over each batch (here, each batch is one patient)\n",
    "                                                                            # Data object contains 'x' (features), 'edge_index' (graph edges), 'y' (labels)\n",
    "        patient_features = data.x                                           # Shape: (num_nodes, in_channels)\n",
    "        patient_edges = data.edge_index                                     # Shape: (2, num_edges)\n",
    "        patient_label = data.y.float()                                      # Target label\n",
    "        batch = data.batch\n",
    "\n",
    "        # Ensure correct format\n",
    "        patient_features = patient_features.float()\n",
    "        patient_edges = patient_edges.to(torch.long)                 \n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(patient_features, patient_edges, batch)               # Output shape: (1, 1)\n",
    "        \n",
    "        # Binary Classification Loss\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(output.view(-1), patient_label)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Print loss after each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 61.904761904761905%\n",
      "Precision: 0.5714285714285714\n",
      "Recall: 0.4444444444444444\n",
      "F1-Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "model.eval() \n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:                            # Iterate over each batch (i.e. one patient)\n",
    "        patient_features = data.x                       # Get features (shape: [num_nodes, in_channels])\n",
    "        patient_edges = data.edge_index                 # Get edges (shape: [2, num_edges])\n",
    "        patient_label = data.y.float()                  # Get label (shape: [1])\n",
    "\n",
    "        # Ensure correct format\n",
    "        patient_features = patient_features.float()    \n",
    "        patient_edges = patient_edges.to(torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(patient_features, patient_edges, data.batch)  # Use the batch info to aggregate across nodes\n",
    "\n",
    "        # Apply sigmoid to the output logits and get the predicted class (0 or 1)\n",
    "        pred = torch.sigmoid(output.squeeze())\n",
    "        predicted_class = (pred >= 0.5).float()                     # Threshold at 0.5 to classify as 0 or 1\n",
    "        \n",
    "        # Collect the labels and predictions for metrics\n",
    "        all_labels.append(patient_label.cpu().numpy())\n",
    "        all_predictions.append(predicted_class.cpu().numpy())\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct += (predicted_class == patient_label).sum().item()\n",
    "        total += patient_label.size(0)  # Increment by the number of samples in this batch\n",
    "\n",
    "# Accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy}%\")\n",
    "\n",
    "# Calculate Metrics\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "Test classification with clinical and image embeddings only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)             # Binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Clinical-Only Model\n",
      "Train Features:  torch.Size([84, 4864])\n",
      "Test Features:  torch.Size([21, 4864])\n",
      "Train Labels:  torch.Size([84, 1])\n",
      "Test Labels:  torch.Size([21, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usama.Khatab\\Projects\\miniconda3\\envs\\hackathon\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.803877358280477\n",
      "Epoch 2/300, Loss: 0.7997226320828\n",
      "Epoch 3/300, Loss: 0.6204201769349831\n",
      "Epoch 4/300, Loss: 0.5803864388061422\n",
      "Epoch 5/300, Loss: 0.64750689614032\n",
      "Epoch 6/300, Loss: 0.5474838894525809\n",
      "Epoch 7/300, Loss: 0.5760040030221544\n",
      "Epoch 8/300, Loss: 0.531743679000231\n",
      "Epoch 9/300, Loss: 0.5507922566695405\n",
      "Epoch 10/300, Loss: 0.48694355097333236\n",
      "Epoch 11/300, Loss: 0.45033072339304325\n",
      "Epoch 12/300, Loss: 0.4861655347617454\n",
      "Epoch 13/300, Loss: 0.4507966545754157\n",
      "Epoch 14/300, Loss: 0.4371262299412462\n",
      "Epoch 15/300, Loss: 0.40577627597217325\n",
      "Epoch 16/300, Loss: 0.400844454025251\n",
      "Epoch 17/300, Loss: 0.35254744726019754\n",
      "Epoch 18/300, Loss: 0.3637155131819738\n",
      "Epoch 19/300, Loss: 0.4163280426868737\n",
      "Epoch 20/300, Loss: 0.3707482304286844\n",
      "Epoch 21/300, Loss: 0.3262354060827333\n",
      "Epoch 22/300, Loss: 0.2845192091359162\n",
      "Epoch 23/300, Loss: 0.37181694310841823\n",
      "Epoch 24/300, Loss: 0.32526792774795393\n",
      "Epoch 25/300, Loss: 0.34869789733740963\n",
      "Epoch 26/300, Loss: 0.3257591322317672\n",
      "Epoch 27/300, Loss: 0.2594759286488668\n",
      "Epoch 28/300, Loss: 0.24686492872102195\n",
      "Epoch 29/300, Loss: 0.23854708661535606\n",
      "Epoch 30/300, Loss: 0.27478682989944253\n",
      "Epoch 31/300, Loss: 0.28113779341588596\n",
      "Epoch 32/300, Loss: 0.2627821148217945\n",
      "Epoch 33/300, Loss: 0.2700133163561225\n",
      "Epoch 34/300, Loss: 0.23518591998391225\n",
      "Epoch 35/300, Loss: 0.22666896681543372\n",
      "Epoch 36/300, Loss: 0.15788999666701134\n",
      "Epoch 37/300, Loss: 0.15415800126031132\n",
      "Epoch 38/300, Loss: 0.18990247911036326\n",
      "Epoch 39/300, Loss: 0.23510837581274777\n",
      "Epoch 40/300, Loss: 0.18660239986620364\n",
      "Epoch 41/300, Loss: 0.16889322417562996\n",
      "Epoch 42/300, Loss: 0.12789500998723977\n",
      "Epoch 43/300, Loss: 0.15591341169723136\n",
      "Epoch 44/300, Loss: 0.2176937585864019\n",
      "Epoch 45/300, Loss: 0.2542624570271608\n",
      "Epoch 46/300, Loss: 0.186275746102528\n",
      "Epoch 47/300, Loss: 0.16337065789041985\n",
      "Epoch 48/300, Loss: 0.13428241244391986\n",
      "Epoch 49/300, Loss: 0.1520240598621881\n",
      "Epoch 50/300, Loss: 0.1428845301736831\n",
      "Epoch 51/300, Loss: 0.15650494748720553\n",
      "Epoch 52/300, Loss: 0.11602353944038303\n",
      "Epoch 53/300, Loss: 0.15073833466256778\n",
      "Epoch 54/300, Loss: 0.11347206759646003\n",
      "Epoch 55/300, Loss: 0.11802519932049702\n",
      "Epoch 56/300, Loss: 0.14581582391683887\n",
      "Epoch 57/300, Loss: 0.11252822527705489\n",
      "Epoch 58/300, Loss: 0.1180370915665749\n",
      "Epoch 59/300, Loss: 0.08942555404250015\n",
      "Epoch 60/300, Loss: 0.09882136977834662\n",
      "Epoch 61/300, Loss: 0.1181081911983854\n",
      "Epoch 62/300, Loss: 0.11153804895334049\n",
      "Epoch 63/300, Loss: 0.16949790893571387\n",
      "Epoch 64/300, Loss: 0.14367213702115578\n",
      "Epoch 65/300, Loss: 0.11130516752490656\n",
      "Epoch 66/300, Loss: 0.11328290031035847\n",
      "Epoch 67/300, Loss: 0.09572292588134565\n",
      "Epoch 68/300, Loss: 0.07884517347062434\n",
      "Epoch 69/300, Loss: 0.07461150029468884\n",
      "Epoch 70/300, Loss: 0.05926248791732621\n",
      "Epoch 71/300, Loss: 0.10333642904000313\n",
      "Epoch 72/300, Loss: 0.09012136322215929\n",
      "Epoch 73/300, Loss: 0.1028570738327147\n",
      "Epoch 74/300, Loss: 0.05557360364882405\n",
      "Epoch 75/300, Loss: 0.0869413677923448\n",
      "Epoch 76/300, Loss: 0.09892008284739828\n",
      "Epoch 77/300, Loss: 0.06500272254997833\n",
      "Epoch 78/300, Loss: 0.04159165414129499\n",
      "Epoch 79/300, Loss: 0.04381818046773337\n",
      "Epoch 80/300, Loss: 0.08885415043586073\n",
      "Epoch 81/300, Loss: 0.0662283069811633\n",
      "Epoch 82/300, Loss: 0.06552927218019844\n",
      "Epoch 83/300, Loss: 0.09040967827044304\n",
      "Epoch 84/300, Loss: 0.07106198993248343\n",
      "Epoch 85/300, Loss: 0.04790136755872347\n",
      "Epoch 86/300, Loss: 0.11619802623583815\n",
      "Epoch 87/300, Loss: 0.09910741934154255\n",
      "Epoch 88/300, Loss: 0.11426552840069146\n",
      "Epoch 89/300, Loss: 0.0692513446225611\n",
      "Epoch 90/300, Loss: 0.04534087976732396\n",
      "Epoch 91/300, Loss: 0.06812619103542888\n",
      "Epoch 92/300, Loss: 0.045946111547622306\n",
      "Epoch 93/300, Loss: 0.0854037250872361\n",
      "Epoch 94/300, Loss: 0.10592926049217646\n",
      "Epoch 95/300, Loss: 0.08926378368628801\n",
      "Epoch 96/300, Loss: 0.08756728785347587\n",
      "Epoch 97/300, Loss: 0.05179899804733675\n",
      "Epoch 98/300, Loss: 0.05751034806170646\n",
      "Epoch 99/300, Loss: 0.048782516849901605\n",
      "Epoch 100/300, Loss: 0.029835848989636288\n",
      "Epoch 101/300, Loss: 0.04482447506827571\n",
      "Epoch 102/300, Loss: 0.058985215503565905\n",
      "Epoch 103/300, Loss: 0.037288952862695623\n",
      "Epoch 104/300, Loss: 0.08312505251211963\n",
      "Epoch 105/300, Loss: 0.05581237992351414\n",
      "Epoch 106/300, Loss: 0.05567222111756854\n",
      "Epoch 107/300, Loss: 0.047193564012647925\n",
      "Epoch 108/300, Loss: 0.06469066776890132\n",
      "Epoch 109/300, Loss: 0.06113663510490869\n",
      "Epoch 110/300, Loss: 0.0345523128068132\n",
      "Epoch 111/300, Loss: 0.06675256079537029\n",
      "Epoch 112/300, Loss: 0.03931774171783312\n",
      "Epoch 113/300, Loss: 0.0253710597900059\n",
      "Epoch 114/300, Loss: 0.06999919584907133\n",
      "Epoch 115/300, Loss: 0.028647735732015178\n",
      "Epoch 116/300, Loss: 0.06866832133777631\n",
      "Epoch 117/300, Loss: 0.05648043652845666\n",
      "Epoch 118/300, Loss: 0.050958422945343716\n",
      "Epoch 119/300, Loss: 0.0674708835772008\n",
      "Epoch 120/300, Loss: 0.05385321482457547\n",
      "Epoch 121/300, Loss: 0.042285759206254624\n",
      "Epoch 122/300, Loss: 0.02204581250876306\n",
      "Epoch 123/300, Loss: 0.03343558011694945\n",
      "Epoch 124/300, Loss: 0.029056980247851187\n",
      "Epoch 125/300, Loss: 0.025608841884767097\n",
      "Epoch 126/300, Loss: 0.02900914840691168\n",
      "Epoch 127/300, Loss: 0.024515061841862192\n",
      "Epoch 128/300, Loss: 0.06131834576258012\n",
      "Epoch 129/300, Loss: 0.04131208901278013\n",
      "Epoch 130/300, Loss: 0.027180521524876156\n",
      "Epoch 131/300, Loss: 0.057753803551401195\n",
      "Epoch 132/300, Loss: 0.04705534738896927\n",
      "Epoch 133/300, Loss: 0.08257947629527666\n",
      "Epoch 134/300, Loss: 0.023635108971714454\n",
      "Epoch 135/300, Loss: 0.0713831250896004\n",
      "Epoch 136/300, Loss: 0.13404779775006945\n",
      "Epoch 137/300, Loss: 0.03954080800952915\n",
      "Epoch 138/300, Loss: 0.029975541352167887\n",
      "Epoch 139/300, Loss: 0.024364530925235817\n",
      "Epoch 140/300, Loss: 0.042366425160066695\n",
      "Epoch 141/300, Loss: 0.044380012063460296\n",
      "Epoch 142/300, Loss: 0.04018871128205121\n",
      "Epoch 143/300, Loss: 0.03716820805652212\n",
      "Epoch 144/300, Loss: 0.042781615156753126\n",
      "Epoch 145/300, Loss: 0.02192937450629197\n",
      "Epoch 146/300, Loss: 0.09691259608519635\n",
      "Epoch 147/300, Loss: 0.05359341638149353\n",
      "Epoch 148/300, Loss: 0.1303759110431623\n",
      "Epoch 149/300, Loss: 0.052519341074615866\n",
      "Epoch 150/300, Loss: 0.021875887478289696\n",
      "Epoch 151/300, Loss: 0.03329022083022649\n",
      "Epoch 152/300, Loss: 0.023366033105442458\n",
      "Epoch 153/300, Loss: 0.0240874875875836\n",
      "Epoch 154/300, Loss: 0.01579347446789055\n",
      "Epoch 155/300, Loss: 0.006129292553174436\n",
      "Epoch 156/300, Loss: 0.045198632094934896\n",
      "Epoch 157/300, Loss: 0.023790624571562193\n",
      "Epoch 158/300, Loss: 0.025752291741115402\n",
      "Epoch 159/300, Loss: 0.02480037313261566\n",
      "Epoch 160/300, Loss: 0.06127321213552413\n",
      "Epoch 161/300, Loss: 0.0560477330949221\n",
      "Epoch 162/300, Loss: 0.019855863368040075\n",
      "Epoch 163/300, Loss: 0.025768689557406417\n",
      "Epoch 164/300, Loss: 0.015997245112969898\n",
      "Epoch 165/300, Loss: 0.006099252501987469\n",
      "Epoch 166/300, Loss: 0.0250330851107909\n",
      "Epoch 167/300, Loss: 0.03535888932690615\n",
      "Epoch 168/300, Loss: 0.03807116595937406\n",
      "Epoch 169/300, Loss: 0.022804994124251736\n",
      "Epoch 170/300, Loss: 0.13127672068355828\n",
      "Epoch 171/300, Loss: 0.053441193887561834\n",
      "Epoch 172/300, Loss: 0.03764889565329878\n",
      "Epoch 173/300, Loss: 0.03542931845357141\n",
      "Epoch 174/300, Loss: 0.10924611097060535\n",
      "Epoch 175/300, Loss: 0.049907972638266074\n",
      "Epoch 176/300, Loss: 0.04578330893574992\n",
      "Epoch 177/300, Loss: 0.02871199209150067\n",
      "Epoch 178/300, Loss: 0.013649127114361852\n",
      "Epoch 179/300, Loss: 0.007969576198738912\n",
      "Epoch 180/300, Loss: 0.02663564605562842\n",
      "Epoch 181/300, Loss: 0.07394228532602848\n",
      "Epoch 182/300, Loss: 0.009054281883903755\n",
      "Epoch 183/300, Loss: 0.02319769269618074\n",
      "Epoch 184/300, Loss: 0.010130036535795537\n",
      "Epoch 185/300, Loss: 0.005094428711356579\n",
      "Epoch 186/300, Loss: 0.016454594983323256\n",
      "Epoch 187/300, Loss: 0.015252167750195207\n",
      "Epoch 188/300, Loss: 0.014267776887337782\n",
      "Epoch 189/300, Loss: 0.011505286141443062\n",
      "Epoch 190/300, Loss: 0.014482862712448676\n",
      "Epoch 191/300, Loss: 0.07562274181073199\n",
      "Epoch 192/300, Loss: 0.03098280743320044\n",
      "Epoch 193/300, Loss: 0.005982281878791119\n",
      "Epoch 194/300, Loss: 0.008991263648657247\n",
      "Epoch 195/300, Loss: 0.03047795349694355\n",
      "Epoch 196/300, Loss: 0.12414614468738536\n",
      "Epoch 197/300, Loss: 0.05840239728177042\n",
      "Epoch 198/300, Loss: 0.03365774027140565\n",
      "Epoch 199/300, Loss: 0.03395861708191427\n",
      "Epoch 200/300, Loss: 0.02791251208664121\n",
      "Epoch 201/300, Loss: 0.014168737651825834\n",
      "Epoch 202/300, Loss: 0.008693571571268049\n",
      "Epoch 203/300, Loss: 0.030624272614958297\n",
      "Epoch 204/300, Loss: 0.016776434657971285\n",
      "Epoch 205/300, Loss: 0.025744164991937286\n",
      "Epoch 206/300, Loss: 0.051154059870637494\n",
      "Epoch 207/300, Loss: 0.060760171360728844\n",
      "Epoch 208/300, Loss: 0.04229272251030465\n",
      "Epoch 209/300, Loss: 0.02532739899001741\n",
      "Epoch 210/300, Loss: 0.012507926613856008\n",
      "Epoch 211/300, Loss: 0.011759405286348536\n",
      "Epoch 212/300, Loss: 0.006693773034071492\n",
      "Epoch 213/300, Loss: 0.015168774875951621\n",
      "Epoch 214/300, Loss: 0.01800384837555161\n",
      "Epoch 215/300, Loss: 0.019272518563535727\n",
      "Epoch 216/300, Loss: 0.02794828218298357\n",
      "Epoch 217/300, Loss: 0.014594421869791149\n",
      "Epoch 218/300, Loss: 0.034579441483738496\n",
      "Epoch 219/300, Loss: 0.01842973006701429\n",
      "Epoch 220/300, Loss: 0.009725316975453762\n",
      "Epoch 221/300, Loss: 0.0114116628442778\n",
      "Epoch 222/300, Loss: 0.03164897430095134\n",
      "Epoch 223/300, Loss: 0.023290542223492815\n",
      "Epoch 224/300, Loss: 0.023942344039195092\n",
      "Epoch 225/300, Loss: 0.003266090524415045\n",
      "Epoch 226/300, Loss: 0.06084915603343517\n",
      "Epoch 227/300, Loss: 0.04042582493178263\n",
      "Epoch 228/300, Loss: 0.037701556812413356\n",
      "Epoch 229/300, Loss: 0.03862537888097445\n",
      "Epoch 230/300, Loss: 0.03820723449688545\n",
      "Epoch 231/300, Loss: 0.01104100389206163\n",
      "Epoch 232/300, Loss: 0.0073275671583516045\n",
      "Epoch 233/300, Loss: 0.01671068075765532\n",
      "Epoch 234/300, Loss: 0.03703967181380463\n",
      "Epoch 235/300, Loss: 0.001795079544852791\n",
      "Epoch 236/300, Loss: 0.026555575190685368\n",
      "Epoch 237/300, Loss: 0.01586226548615633\n",
      "Epoch 238/300, Loss: 0.014084552417595103\n",
      "Epoch 239/300, Loss: 0.005968681134046335\n",
      "Epoch 240/300, Loss: 0.017209805462628274\n",
      "Epoch 241/300, Loss: 0.024572747047157\n",
      "Epoch 242/300, Loss: 0.009364102195547477\n",
      "Epoch 243/300, Loss: 0.00666412526839696\n",
      "Epoch 244/300, Loss: 0.042444878294344554\n",
      "Epoch 245/300, Loss: 0.01211533700780485\n",
      "Epoch 246/300, Loss: 0.03826634265038693\n",
      "Epoch 247/300, Loss: 0.015407365690678904\n",
      "Epoch 248/300, Loss: 0.022308973372548346\n",
      "Epoch 249/300, Loss: 0.019432636044669747\n",
      "Epoch 250/300, Loss: 0.010650827706883399\n",
      "Epoch 251/300, Loss: 0.020593775562004996\n",
      "Epoch 252/300, Loss: 0.03185615273041095\n",
      "Epoch 253/300, Loss: 0.051937424370841936\n",
      "Epoch 254/300, Loss: 0.065520182479843\n",
      "Epoch 255/300, Loss: 0.017194933527802124\n",
      "Epoch 256/300, Loss: 0.03305570474167067\n",
      "Epoch 257/300, Loss: 0.059100773963767714\n",
      "Epoch 258/300, Loss: 0.047226325660864245\n",
      "Epoch 259/300, Loss: 0.010793705383562554\n",
      "Epoch 260/300, Loss: 0.053571909090943545\n",
      "Epoch 261/300, Loss: 0.03218059812718349\n",
      "Epoch 262/300, Loss: 0.07870103420335987\n",
      "Epoch 263/300, Loss: 0.037378853450097824\n",
      "Epoch 264/300, Loss: 0.02305862799050676\n",
      "Epoch 265/300, Loss: 0.020364935053022053\n",
      "Epoch 266/300, Loss: 0.009033642630118183\n",
      "Epoch 267/300, Loss: 0.015981600612146367\n",
      "Epoch 268/300, Loss: 0.017921928953386682\n",
      "Epoch 269/300, Loss: 0.010489720272825681\n",
      "Epoch 270/300, Loss: 0.026929559148644193\n",
      "Epoch 271/300, Loss: 0.010249193237889895\n",
      "Epoch 272/300, Loss: 0.008136935762042526\n",
      "Epoch 273/300, Loss: 0.009140192192395532\n",
      "Epoch 274/300, Loss: 0.010235664559440392\n",
      "Epoch 275/300, Loss: 0.03347187844450914\n",
      "Epoch 276/300, Loss: 0.006011230090704193\n",
      "Epoch 277/300, Loss: 0.014222829393733047\n",
      "Epoch 278/300, Loss: 0.009965872752683427\n",
      "Epoch 279/300, Loss: 0.07332267561572758\n",
      "Epoch 280/300, Loss: 0.008318633961557203\n",
      "Epoch 281/300, Loss: 0.014486451135215649\n",
      "Epoch 282/300, Loss: 0.037711365103140354\n",
      "Epoch 283/300, Loss: 0.012314160465064748\n",
      "Epoch 284/300, Loss: 0.019866250787464575\n",
      "Epoch 285/300, Loss: 0.018926300237468007\n",
      "Epoch 286/300, Loss: 0.012631766639052902\n",
      "Epoch 287/300, Loss: 0.021294775601104883\n",
      "Epoch 288/300, Loss: 0.022931102792463114\n",
      "Epoch 289/300, Loss: 0.016375250141141523\n",
      "Epoch 290/300, Loss: 0.029390825220289345\n",
      "Epoch 291/300, Loss: 0.004119276990691796\n",
      "Epoch 292/300, Loss: 0.013412339137538\n",
      "Epoch 293/300, Loss: 0.0014783024506997713\n",
      "Epoch 294/300, Loss: 0.004194175081620598\n",
      "Epoch 295/300, Loss: 0.03612723321711178\n",
      "Epoch 296/300, Loss: 0.01443463147243273\n",
      "Epoch 297/300, Loss: 0.007185770573827167\n",
      "Epoch 298/300, Loss: 0.0038486081578873615\n",
      "Epoch 299/300, Loss: 0.030276641945176593\n",
      "Epoch 300/300, Loss: 0.03973751971095007\n",
      "Clinical-Only Model\n",
      "Test Accuracy: 76.19047619047619%\n",
      "Precision: 0.7\n",
      "Recall: 0.7777777777777778\n",
      "F1-Score: 0.7368421052631579\n",
      "\n",
      "Training Image-Only Model\n",
      "Train Features:  torch.Size([84, 4608])\n",
      "Test Features:  torch.Size([21, 4608])\n",
      "Train Labels:  torch.Size([84, 1])\n",
      "Test Labels:  torch.Size([21, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usama.Khatab\\AppData\\Local\\Temp\\ipykernel_59512\\4210919272.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_features = torch.tensor(feature_set.reshape(len(feature_set), -1))\n",
      "C:\\Users\\Usama.Khatab\\AppData\\Local\\Temp\\ipykernel_59512\\4210919272.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_features = torch.tensor((test_clinical_embeddings if modality == 'Clinical' else test_image_features).reshape(len(test_labels), -1))\n",
      "c:\\Users\\Usama.Khatab\\Projects\\miniconda3\\envs\\hackathon\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.6839666242400805\n",
      "Epoch 2/300, Loss: 0.32685298499252113\n",
      "Epoch 3/300, Loss: 0.19409036507741326\n",
      "Epoch 4/300, Loss: 0.12494173629342445\n",
      "Epoch 5/300, Loss: 0.09374570477354739\n",
      "Epoch 6/300, Loss: 0.06318495553430348\n",
      "Epoch 7/300, Loss: 0.04915811710747048\n",
      "Epoch 8/300, Loss: 0.04062048251008881\n",
      "Epoch 9/300, Loss: 0.030472637165249103\n",
      "Epoch 10/300, Loss: 0.025647175758855327\n",
      "Epoch 11/300, Loss: 0.02059334204975693\n",
      "Epoch 12/300, Loss: 0.0185509802208149\n",
      "Epoch 13/300, Loss: 0.02111927098650042\n",
      "Epoch 14/300, Loss: 0.013477226388723855\n",
      "Epoch 15/300, Loss: 0.009254881093164883\n",
      "Epoch 16/300, Loss: 0.011483783117173949\n",
      "Epoch 17/300, Loss: 0.007659144029492468\n",
      "Epoch 18/300, Loss: 0.01236080957945281\n",
      "Epoch 19/300, Loss: 0.007858550844719588\n",
      "Epoch 20/300, Loss: 0.0059053982972511136\n",
      "Epoch 21/300, Loss: 0.006666207724698498\n",
      "Epoch 22/300, Loss: 0.004900211034899777\n",
      "Epoch 23/300, Loss: 0.005916265957022829\n",
      "Epoch 24/300, Loss: 0.0038025492989011886\n",
      "Epoch 25/300, Loss: 0.004049372100607364\n",
      "Epoch 26/300, Loss: 0.003440386306985082\n",
      "Epoch 27/300, Loss: 0.004563716689777461\n",
      "Epoch 28/300, Loss: 0.003200478751837398\n",
      "Epoch 29/300, Loss: 0.0032389501147484963\n",
      "Epoch 30/300, Loss: 0.002660247973142846\n",
      "Epoch 31/300, Loss: 0.003311733884965222\n",
      "Epoch 32/300, Loss: 0.0028304792170144666\n",
      "Epoch 33/300, Loss: 0.0022619516683854662\n",
      "Epoch 34/300, Loss: 0.0022266965499208404\n",
      "Epoch 35/300, Loss: 0.001734057991208309\n",
      "Epoch 36/300, Loss: 0.0032242175171588662\n",
      "Epoch 37/300, Loss: 0.001197647959218427\n",
      "Epoch 38/300, Loss: 0.002177704266462873\n",
      "Epoch 39/300, Loss: 0.0012046910559157758\n",
      "Epoch 40/300, Loss: 0.0011968253597517884\n",
      "Epoch 41/300, Loss: 0.0016847832152095741\n",
      "Epoch 42/300, Loss: 0.0018644738885476336\n",
      "Epoch 43/300, Loss: 0.0012017986091666984\n",
      "Epoch 44/300, Loss: 0.0016959834610718514\n",
      "Epoch 45/300, Loss: 0.001369857813079718\n",
      "Epoch 46/300, Loss: 0.0020951614214503762\n",
      "Epoch 47/300, Loss: 0.0011608400834466877\n",
      "Epoch 48/300, Loss: 0.001083819267391629\n",
      "Epoch 49/300, Loss: 0.0017057252034430263\n",
      "Epoch 50/300, Loss: 0.000836861341608277\n",
      "Epoch 51/300, Loss: 0.0020389273894433507\n",
      "Epoch 52/300, Loss: 0.0007315897730785807\n",
      "Epoch 53/300, Loss: 0.0009642824393755031\n",
      "Epoch 54/300, Loss: 0.000527830391531227\n",
      "Epoch 55/300, Loss: 0.000970196672873109\n",
      "Epoch 56/300, Loss: 0.0005891335647339431\n",
      "Epoch 57/300, Loss: 0.0013293855228030658\n",
      "Epoch 58/300, Loss: 0.0005238875499620528\n",
      "Epoch 59/300, Loss: 0.0010229611946323573\n",
      "Epoch 60/300, Loss: 0.0006645562462630377\n",
      "Epoch 61/300, Loss: 0.0003616258180510981\n",
      "Epoch 62/300, Loss: 0.000849278898672743\n",
      "Epoch 63/300, Loss: 0.000460797969596104\n",
      "Epoch 64/300, Loss: 0.0009435606708187887\n",
      "Epoch 65/300, Loss: 0.0007838817235318899\n",
      "Epoch 66/300, Loss: 0.0005501453315473338\n",
      "Epoch 67/300, Loss: 0.0007917050287731053\n",
      "Epoch 68/300, Loss: 0.000818467466010426\n",
      "Epoch 69/300, Loss: 0.0006031574761506318\n",
      "Epoch 70/300, Loss: 0.0005358682714835421\n",
      "Epoch 71/300, Loss: 0.0006357426426670069\n",
      "Epoch 72/300, Loss: 0.00041975520048524793\n",
      "Epoch 73/300, Loss: 0.00027645670143291073\n",
      "Epoch 74/300, Loss: 0.0003991249015701301\n",
      "Epoch 75/300, Loss: 0.001089016698572431\n",
      "Epoch 76/300, Loss: 0.0005857891474295771\n",
      "Epoch 77/300, Loss: 0.0004176099484993756\n",
      "Epoch 78/300, Loss: 0.0005672908921659266\n",
      "Epoch 79/300, Loss: 0.0014308384024606842\n",
      "Epoch 80/300, Loss: 0.00031339698618155457\n",
      "Epoch 81/300, Loss: 0.0003497449640676786\n",
      "Epoch 82/300, Loss: 0.0006310084020079947\n",
      "Epoch 83/300, Loss: 0.0004874726633861232\n",
      "Epoch 84/300, Loss: 0.00043648153713672313\n",
      "Epoch 85/300, Loss: 0.0003606304379686554\n",
      "Epoch 86/300, Loss: 0.0003403252765377964\n",
      "Epoch 87/300, Loss: 0.00029439280000414803\n",
      "Epoch 88/300, Loss: 0.0005699102693673316\n",
      "Epoch 89/300, Loss: 0.0003826419665826007\n",
      "Epoch 90/300, Loss: 0.0004948265906453539\n",
      "Epoch 91/300, Loss: 0.0003173184120110288\n",
      "Epoch 92/300, Loss: 0.0004007653280757901\n",
      "Epoch 93/300, Loss: 0.00038272698087577205\n",
      "Epoch 94/300, Loss: 0.000312888582049893\n",
      "Epoch 95/300, Loss: 0.0005635940538392484\n",
      "Epoch 96/300, Loss: 0.0003401614257771122\n",
      "Epoch 97/300, Loss: 0.00024539583758403135\n",
      "Epoch 98/300, Loss: 0.00019929725807985154\n",
      "Epoch 99/300, Loss: 0.0006659582878831944\n",
      "Epoch 100/300, Loss: 0.00035108841502002567\n",
      "Epoch 101/300, Loss: 0.0008269031604475023\n",
      "Epoch 102/300, Loss: 0.0004812636288095616\n",
      "Epoch 103/300, Loss: 0.0004264582422780653\n",
      "Epoch 104/300, Loss: 0.00052839248117945\n",
      "Epoch 105/300, Loss: 0.00026031550287898753\n",
      "Epoch 106/300, Loss: 0.00025148113051559883\n",
      "Epoch 107/300, Loss: 0.00031108457029340624\n",
      "Epoch 108/300, Loss: 0.0007191843624586422\n",
      "Epoch 109/300, Loss: 0.00035503377871139364\n",
      "Epoch 110/300, Loss: 0.00038036558209022413\n",
      "Epoch 111/300, Loss: 0.0005541049168192288\n",
      "Epoch 112/300, Loss: 0.0003770697291967635\n",
      "Epoch 113/300, Loss: 0.00036363271708569504\n",
      "Epoch 114/300, Loss: 0.00027983843857964664\n",
      "Epoch 115/300, Loss: 0.00024223019465080276\n",
      "Epoch 116/300, Loss: 0.00020620544339803156\n",
      "Epoch 117/300, Loss: 0.0022092148492190303\n",
      "Epoch 118/300, Loss: 0.0003150829900521655\n",
      "Epoch 119/300, Loss: 0.0005545420146983052\n",
      "Epoch 120/300, Loss: 0.00014774592861634855\n",
      "Epoch 121/300, Loss: 0.00033141379596608137\n",
      "Epoch 122/300, Loss: 0.00029387713732917506\n",
      "Epoch 123/300, Loss: 0.0004383076402064542\n",
      "Epoch 124/300, Loss: 0.0002615084622819537\n",
      "Epoch 125/300, Loss: 0.0008139713363679473\n",
      "Epoch 126/300, Loss: 0.00042848342234530755\n",
      "Epoch 127/300, Loss: 0.00027712494796674374\n",
      "Epoch 128/300, Loss: 0.00023561464866338002\n",
      "Epoch 129/300, Loss: 0.0002320551577645245\n",
      "Epoch 130/300, Loss: 0.0002397710977172705\n",
      "Epoch 131/300, Loss: 0.00020219923469348765\n",
      "Epoch 132/300, Loss: 0.0003102049419196261\n",
      "Epoch 133/300, Loss: 0.00019781006987336304\n",
      "Epoch 134/300, Loss: 0.00018377691019771141\n",
      "Epoch 135/300, Loss: 0.0003133160548571823\n",
      "Epoch 136/300, Loss: 0.00029652978951647907\n",
      "Epoch 137/300, Loss: 0.00015863242396108035\n",
      "Epoch 138/300, Loss: 0.00019826141911621114\n",
      "Epoch 139/300, Loss: 0.00042825534683677534\n",
      "Epoch 140/300, Loss: 0.00031131541537233163\n",
      "Epoch 141/300, Loss: 0.00011215377047817705\n",
      "Epoch 142/300, Loss: 0.0003050569719784151\n",
      "Epoch 143/300, Loss: 0.00012167805350857396\n",
      "Epoch 144/300, Loss: 0.00014989746195395593\n",
      "Epoch 145/300, Loss: 0.0003773974253274811\n",
      "Epoch 146/300, Loss: 0.00026575923674047414\n",
      "Epoch 147/300, Loss: 0.0002893382396522492\n",
      "Epoch 148/300, Loss: 0.00039571433163733074\n",
      "Epoch 149/300, Loss: 0.0002796693841044238\n",
      "Epoch 150/300, Loss: 0.00020420219747773247\n",
      "Epoch 151/300, Loss: 0.00022517710786879825\n",
      "Epoch 152/300, Loss: 0.00035238469484780025\n",
      "Epoch 153/300, Loss: 0.00011746213982438427\n",
      "Epoch 154/300, Loss: 0.00021117859752112488\n",
      "Epoch 155/300, Loss: 0.00025744260346508935\n",
      "Epoch 156/300, Loss: 0.000324549131303256\n",
      "Epoch 157/300, Loss: 0.0006482115442468213\n",
      "Epoch 158/300, Loss: 0.0004273432796272929\n",
      "Epoch 159/300, Loss: 0.0001047074327330797\n",
      "Epoch 160/300, Loss: 0.00012612391020495024\n",
      "Epoch 161/300, Loss: 0.0002169192140979331\n",
      "Epoch 162/300, Loss: 0.00032260854279753646\n",
      "Epoch 163/300, Loss: 0.00013555664580979152\n",
      "Epoch 164/300, Loss: 0.00011639925308127799\n",
      "Epoch 165/300, Loss: 0.000385549449560025\n",
      "Epoch 166/300, Loss: 0.0001474077884028683\n",
      "Epoch 167/300, Loss: 0.00038596190752131286\n",
      "Epoch 168/300, Loss: 0.0001212080556326119\n",
      "Epoch 169/300, Loss: 0.00021872530914914177\n",
      "Epoch 170/300, Loss: 0.0002471677132886456\n",
      "Epoch 171/300, Loss: 0.000302562837651198\n",
      "Epoch 172/300, Loss: 9.806788001813042e-05\n",
      "Epoch 173/300, Loss: 0.0001502804805731051\n",
      "Epoch 174/300, Loss: 0.00033171143632779307\n",
      "Epoch 175/300, Loss: 0.00048327865591160516\n",
      "Epoch 176/300, Loss: 0.00023208598698805835\n",
      "Epoch 177/300, Loss: 8.238039771286359e-05\n",
      "Epoch 178/300, Loss: 8.948349842356828e-05\n",
      "Epoch 179/300, Loss: 0.00014598786631041315\n",
      "Epoch 180/300, Loss: 0.0001899314431396602\n",
      "Epoch 181/300, Loss: 0.0001263681534720007\n",
      "Epoch 182/300, Loss: 0.00016295054562872244\n",
      "Epoch 183/300, Loss: 0.0002759797254839721\n",
      "Epoch 184/300, Loss: 0.00016079894145355701\n",
      "Epoch 185/300, Loss: 9.327665175286524e-05\n",
      "Epoch 186/300, Loss: 0.00014966313166071238\n",
      "Epoch 187/300, Loss: 0.00020347637774867234\n",
      "Epoch 188/300, Loss: 0.00011191357969087519\n",
      "Epoch 189/300, Loss: 0.00016824679662568492\n",
      "Epoch 190/300, Loss: 0.0001357083263730968\n",
      "Epoch 191/300, Loss: 0.00029995949378030807\n",
      "Epoch 192/300, Loss: 0.00017492650439781832\n",
      "Epoch 193/300, Loss: 0.0002292152935507679\n",
      "Epoch 194/300, Loss: 0.00014444010569123055\n",
      "Epoch 195/300, Loss: 0.0001439749949489292\n",
      "Epoch 196/300, Loss: 0.00011684449587705325\n",
      "Epoch 197/300, Loss: 0.00017026800689311558\n",
      "Epoch 198/300, Loss: 0.0002429697686544499\n",
      "Epoch 199/300, Loss: 0.00017656826851591184\n",
      "Epoch 200/300, Loss: 0.00016185780732929934\n",
      "Epoch 201/300, Loss: 0.0001567387367743375\n",
      "Epoch 202/300, Loss: 0.00015986955185631757\n",
      "Epoch 203/300, Loss: 0.00022786174571086195\n",
      "Epoch 204/300, Loss: 0.00011645153847354772\n",
      "Epoch 205/300, Loss: 8.251608260661805e-05\n",
      "Epoch 206/300, Loss: 0.00042023960179635297\n",
      "Epoch 207/300, Loss: 0.0001231212309966912\n",
      "Epoch 208/300, Loss: 0.00014765586253942398\n",
      "Epoch 209/300, Loss: 0.00016637466565539412\n",
      "Epoch 210/300, Loss: 0.00014918249352985092\n",
      "Epoch 211/300, Loss: 0.0006280620747168154\n",
      "Epoch 212/300, Loss: 0.000163211761475136\n",
      "Epoch 213/300, Loss: 0.0001340600760115879\n",
      "Epoch 214/300, Loss: 7.049471974746828e-05\n",
      "Epoch 215/300, Loss: 0.0001420934597192566\n",
      "Epoch 216/300, Loss: 0.00032911399989127554\n",
      "Epoch 217/300, Loss: 0.00013883604069310165\n",
      "Epoch 218/300, Loss: 0.00014856717731426272\n",
      "Epoch 219/300, Loss: 0.00020876543138199856\n",
      "Epoch 220/300, Loss: 0.00031057311837614403\n",
      "Epoch 221/300, Loss: 9.444174756291699e-05\n",
      "Epoch 222/300, Loss: 0.00014369317827260595\n",
      "Epoch 223/300, Loss: 0.00030045544033405864\n",
      "Epoch 224/300, Loss: 0.00012644028581857266\n",
      "Epoch 225/300, Loss: 0.0003524708589751465\n",
      "Epoch 226/300, Loss: 0.0002332927382369967\n",
      "Epoch 227/300, Loss: 0.00011006633274575284\n",
      "Epoch 228/300, Loss: 0.00016212102944726392\n",
      "Epoch 229/300, Loss: 8.095074006709253e-05\n",
      "Epoch 230/300, Loss: 0.00017380092690603513\n",
      "Epoch 231/300, Loss: 0.00014149894633571795\n",
      "Epoch 232/300, Loss: 0.00014467792559104607\n",
      "Epoch 233/300, Loss: 0.00012647507212489492\n",
      "Epoch 234/300, Loss: 0.00011593580597193292\n",
      "Epoch 235/300, Loss: 0.00012190564766796575\n",
      "Epoch 236/300, Loss: 0.0001267415006737898\n",
      "Epoch 237/300, Loss: 0.0001671561034937681\n",
      "Epoch 238/300, Loss: 0.0002853984685151951\n",
      "Epoch 239/300, Loss: 0.00045324862483069647\n",
      "Epoch 240/300, Loss: 9.147261974937866e-05\n",
      "Epoch 241/300, Loss: 0.00017615201286949467\n",
      "Epoch 242/300, Loss: 8.862375339132218e-05\n",
      "Epoch 243/300, Loss: 9.698181143686078e-05\n",
      "Epoch 244/300, Loss: 0.00010668360257333409\n",
      "Epoch 245/300, Loss: 0.00011032242739219438\n",
      "Epoch 246/300, Loss: 0.00013763548658508014\n",
      "Epoch 247/300, Loss: 0.00020715368722739363\n",
      "Epoch 248/300, Loss: 9.168923989181603e-05\n",
      "Epoch 249/300, Loss: 0.00025664571323932454\n",
      "Epoch 250/300, Loss: 0.00018724802234412356\n",
      "Epoch 251/300, Loss: 0.00011691287002809266\n",
      "Epoch 252/300, Loss: 9.625849785800864e-05\n",
      "Epoch 253/300, Loss: 0.0001436222393293361\n",
      "Epoch 254/300, Loss: 0.00014980348837117808\n",
      "Epoch 255/300, Loss: 0.00011129993147715059\n",
      "Epoch 256/300, Loss: 0.00018009094092966005\n",
      "Epoch 257/300, Loss: 0.000199308186179758\n",
      "Epoch 258/300, Loss: 0.0003462245789643349\n",
      "Epoch 259/300, Loss: 0.00017992485159249819\n",
      "Epoch 260/300, Loss: 7.787639877592023e-05\n",
      "Epoch 261/300, Loss: 0.0001355480011076522\n",
      "Epoch 262/300, Loss: 0.00018026891001175573\n",
      "Epoch 263/300, Loss: 0.00014750523550037067\n",
      "Epoch 264/300, Loss: 0.00018119456639783697\n",
      "Epoch 265/300, Loss: 0.00010608690888678853\n",
      "Epoch 266/300, Loss: 0.00019740669778190947\n",
      "Epoch 267/300, Loss: 0.00015003424441436172\n",
      "Epoch 268/300, Loss: 0.0006241801886783804\n",
      "Epoch 269/300, Loss: 9.060401119303798e-05\n",
      "Epoch 270/300, Loss: 0.00011015230561690308\n",
      "Epoch 271/300, Loss: 0.00016670928350569016\n",
      "Epoch 272/300, Loss: 0.00014213395503150293\n",
      "Epoch 273/300, Loss: 0.0001678147642574552\n",
      "Epoch 274/300, Loss: 0.00014782879516197394\n",
      "Epoch 275/300, Loss: 9.74059221425265e-05\n",
      "Epoch 276/300, Loss: 0.0002788569189403252\n",
      "Epoch 277/300, Loss: 0.00010076374537908756\n",
      "Epoch 278/300, Loss: 0.00021508182887407048\n",
      "Epoch 279/300, Loss: 0.00011948259807647603\n",
      "Epoch 280/300, Loss: 0.00014121738184750635\n",
      "Epoch 281/300, Loss: 0.00014311843697212807\n",
      "Epoch 282/300, Loss: 0.00011843286271236087\n",
      "Epoch 283/300, Loss: 0.00019166233829892576\n",
      "Epoch 284/300, Loss: 0.00020334815721006244\n",
      "Epoch 285/300, Loss: 0.0001437373212219653\n",
      "Epoch 286/300, Loss: 8.478598301782843e-05\n",
      "Epoch 287/300, Loss: 0.00012303777172046048\n",
      "Epoch 288/300, Loss: 0.00013670975681211503\n",
      "Epoch 289/300, Loss: 9.45460562378843e-05\n",
      "Epoch 290/300, Loss: 0.00016401995649503864\n",
      "Epoch 291/300, Loss: 9.429549089603367e-05\n",
      "Epoch 292/300, Loss: 0.00010093420280224816\n",
      "Epoch 293/300, Loss: 0.00024908591035844284\n",
      "Epoch 294/300, Loss: 0.00016461790962577334\n",
      "Epoch 295/300, Loss: 0.00025084054501476433\n",
      "Epoch 296/300, Loss: 8.479281718196252e-05\n",
      "Epoch 297/300, Loss: 0.0001853853316886906\n",
      "Epoch 298/300, Loss: 0.0001365017339705806\n",
      "Epoch 299/300, Loss: 0.00017236541281452302\n",
      "Epoch 300/300, Loss: 0.00020563625719239282\n",
      "Image-Only Model\n",
      "Test Accuracy: 47.61904761904762%\n",
      "Precision: 0.4166666666666667\n",
      "Recall: 0.5555555555555556\n",
      "F1-Score: 0.47619047619047616\n"
     ]
    }
   ],
   "source": [
    "# Experiment: Train Clinical-only and Image-only Models\n",
    "for modality, feature_set in [('Clinical', train_clinical_embeddings), ('Image', train_image_features)]:\n",
    "    print(f\"\\nTraining {modality}-Only Model\")\n",
    "    \n",
    "    train_labels = train_labels.clone().detach().float().view(-1, 1)\n",
    "    test_labels = test_labels.clone().detach().float().view(-1, 1)\n",
    "    train_features = torch.tensor(feature_set.reshape(len(feature_set), -1))\n",
    "    test_features = torch.tensor((test_clinical_embeddings if modality == 'Clinical' else test_image_features).reshape(len(test_labels), -1))\n",
    "\n",
    "    print(\"Train Features: \", train_features.shape)\n",
    "    print(\"Test Features: \", test_features.shape)\n",
    "    print(\"Train Labels: \", train_labels.shape)\n",
    "    print(\"Test Labels: \", test_labels.shape)\n",
    "    \n",
    "    train_dataset = TensorDataset(train_features, train_labels)\n",
    "    test_dataset = TensorDataset(test_features, test_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    test_model = MLP(input_dim=train_features.shape[1])\n",
    "    test_optimizer = torch.optim.Adam(test_model.parameters(), lr=learning_rate, weight_decay=w_decay)\n",
    "    \n",
    "    epochs = 300\n",
    "    for epoch in range(epochs):\n",
    "        test_model.train()\n",
    "        total_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            test_optimizer.zero_grad()\n",
    "            output = test_model(features.float())\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(output.view(-1), labels.view(-1))\n",
    "            loss.backward()\n",
    "            test_optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "    \n",
    "    test_model.eval()\n",
    "    all_labels, all_predictions = [], []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            output = test_model(features.float())\n",
    "\n",
    "            pred = torch.sigmoid(output.squeeze()) >= 0.5\n",
    "            \n",
    "            all_labels.append(labels.cpu().numpy().flatten())\n",
    "            all_predictions.append(pred.cpu().numpy().flatten())\n",
    "\n",
    "            # Count correct predictions\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)  # Increment by the number of samples in this batch\n",
    "    \n",
    "    print(f\"{modality}-Only Model\")\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy}%\")\n",
    "\n",
    "    # Calculate Metrics\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
