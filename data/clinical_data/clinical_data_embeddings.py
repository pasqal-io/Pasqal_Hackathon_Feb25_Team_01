# -*- coding: utf-8 -*-
"""clinical_data_embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FQWu5w79MEBun420UrttKRQHQb7Eb2jT

# Clinical Data Embeddings
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import logging
import sys

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFECV

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler

logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',
                    level=logging.INFO,
                    stream=sys.stdout)

# PVEM Model
class PVEMClassifier(nn.Module):
    def __init__(self, input_dim, embedding_dim=128, num_classes=2):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(input_dim, embedding_dim))
        self.bias = nn.Parameter(torch.randn(input_dim, embedding_dim))
        self.dropout = nn.Dropout(p=0.3)
        self.classifier = nn.Linear(input_dim * embedding_dim, num_classes)

    def forward(self, x):
        embeddings = x.unsqueeze(-1) * self.weight + self.bias
        flattened_embeddings = embeddings.reshape(x.shape[0], -1)
        flattened_embeddings = self.dropout(flattened_embeddings)
        logits = self.classifier(flattened_embeddings)
        return embeddings, logits
    

class DataPreprocessing: 
    def __init__(self, data_path, target_column="Censored_0_progressed_1", id_column = "TCIA_ID"):
        self.data_path = data_path
        self.target_column = target_column
        self.id_column = id_column
        self.scaler = StandardScaler()

    def preprocesses_data(self):
        df = pd.read_csv(self.data_path)
        print("Shape of DataFrame:", df.shape)

        y = df[self.target_column]
        df = df.drop(columns=[self.target_column])      # Removing target from features

        categorical_cols = df.select_dtypes(include=["object"]).columns
        for col in categorical_cols:
            df[col] = LabelEncoder().fit_transform(df[col].astype(str))

        # Handling missing values (fill with mean)
        df.fillna(df.mean(), inplace=True)

        # Normalizing the data
        df_scaled = pd.DataFrame(self.scaler.fit_transform(df), columns=df.columns)

        #Computing Feature Importance using Random Forest
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(df_scaled, y)

        # feature importances
        feature_importance = pd.Series(rf.feature_importances_, index=df_scaled.columns).sort_values(ascending=False)

        #  Checking the features having low importance (less than 0.005) in contributing to the target variable
        low_importance_threshold = 0.005
        low_importance_features = feature_importance[feature_importance < low_importance_threshold].index.tolist()

        # Dropping features that have very low importance
        df_selected = df_scaled.drop(columns=low_importance_features)

        # Recursive Feature Elimination (RFE)
        rfe = RFECV(estimator=rf, step=1, cv=5, scoring='accuracy')
        rfe.fit(df_selected, y)

        # Computing correlation matrix
        corr_matrix = df_selected.corr()

        # Setting threshold for high correlation
        threshold = 0.9

        # Finding highly correlated features
        high_corr_pairs = set()
        for i in range(len(corr_matrix.columns)):
            for j in range(i):
                if abs(corr_matrix.iloc[i, j]) > threshold:
                    high_corr_pairs.add((corr_matrix.columns[i], corr_matrix.columns[j]))

        # Extracting features to drop (keeping one from each correlated pair)
        features_to_drop = set([pair[1] for pair in high_corr_pairs])

        # Dropping highly correlated features
        df_final_reduced = df_selected.drop(columns=features_to_drop)

        df_selected_normalized = self.scaler.fit_transform(df_final_reduced)

        # Converting to PyTorch tensor
        patient_data = torch.tensor(df_selected_normalized, dtype=torch.float32)    # Shape: (105, num_features)
        patient_ids = df[self.id_column].values                                     # Shape: (105,) ids
        target_labels = torch.tensor(y.values, dtype=torch.long)                    # Shape: (105,) for classification

        return patient_data, target_labels, patient_ids



class ClinicalDataEmbeddings:
    def __init__(self, embedding_dim, target_column="Censored_0_progressed_1"):
        self.embedding_dim = embedding_dim
        self.target_column = target_column
        self.scaler = StandardScaler()

    def train_model(self, train_data, train_labels, num_epochs=500, lr=0.001):
        num_features = train_data.shape[1]
        self.model = PVEMClassifier(num_features, embedding_dim=128, num_classes=2)

        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=lr)

        for epoch in range(num_epochs):
            optimizer.zero_grad()
            embeddings, logits = self.model(train_data)
            loss = criterion(logits, train_labels)
            loss.backward()
            optimizer.step()

            if epoch % 50 == 0:
                print(f"Epoch {epoch}, Loss: {loss.item():.6f}")

    def generate_and_save_embeddings(self, data, labels, isTrain=True):
        """Generates embeddings for data and saves them along with labels."""
        with torch.no_grad():
            embeddings, _ = self.model(data)

        # Save embeddings and labels with patient IDs
        if isTrain: 
            print("Train Embeddings shape: ", embeddings.shape)
            np.save("data/clinical_data/train_embeddings.npy", embeddings.detach().cpu().numpy())
        else: 
            print("Test Embeddings shape: ", embeddings.shape)
            np.save("data/clinical_data/test_embeddings.npy", embeddings.detach().cpu().numpy())

        print("Embeddings & labels saved successfully!")