{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, GATConv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "n_clinical = 38 \n",
    "n_image_nodes = 6*6\n",
    "n_nodes = n_clinical + n_image_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples:  84\n",
      "Test Samples:  21\n",
      "Train labels shape: torch.Size([84])\n",
      "Test labels shape: torch.Size([21])\n"
     ]
    }
   ],
   "source": [
    "# Load Ground-Truth Values\n",
    "train_labels = pd.read_csv(\"data/labels/train_labels.csv\")\n",
    "train_labels = train_labels.iloc[:, 1].tolist()                 # (n_train,)\n",
    "test_labels = pd.read_csv(\"data/labels/test_labels.csv\")\n",
    "test_labels = test_labels.iloc[:, 1].tolist()                   # (n_test,)\n",
    "\n",
    "n_train = len(train_labels) # 84\n",
    "n_test = len(test_labels)   # 21\n",
    "\n",
    "print('Training Samples: ', n_train)\n",
    "print('Test Samples: ', n_test)\n",
    "\n",
    "# Convert to tensors\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "print(\"Train labels shape:\", train_labels.shape)                # Should be (n_train,)\n",
    "print(\"Test labels shape:\", test_labels.shape)                  # Should be (n_test,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "    return (embeddings - embeddings.mean()) / (embeddings.std() + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Image Embeddings:  (84, 6, 6, 128)\n",
      "Train Clinical Embeddings:  (84, 38, 128)\n",
      "Test Image Embeddings:  (21, 6, 6, 128)\n",
      "Test Clinical Embeddings:  (21, 38, 128)\n"
     ]
    }
   ],
   "source": [
    "# Load and normalise Embeddings\n",
    "train_image_embeddings = np.load(\"data/image_data/train_image_embeddings.npy\")             # (n_train, 6, 6, embed_dim)\n",
    "train_clinical_embeddings = np.load(\"data/clinical_data/train_embeddings.npy\")          # (n_train, 38, embed_dim)\n",
    "test_image_embeddings = np.load(\"data/image_data/test_image_embeddings.npy\")               # (n_test, 6, 6, embed_dim)\n",
    "test_clinical_embeddings = np.load(\"data/clinical_data/test_embeddings.npy\")            # (n_test, 38, embed_dim)\n",
    "\n",
    "print(\"Train Image Embeddings: \", train_image_embeddings.shape)\n",
    "print(\"Train Clinical Embeddings: \", train_clinical_embeddings.shape)\n",
    "print(\"Test Image Embeddings: \",test_image_embeddings.shape)\n",
    "print(\"Test Clinical Embeddings: \", test_clinical_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Train Image Embeddings:  torch.Size([84, 36, 128])\n",
      "Combined Train Embeddings:  torch.Size([84, 74, 128])\n",
      "Reshaped Test Image Embeddings:  torch.Size([21, 36, 128])\n",
      "Combined Test Embeddings:  torch.Size([21, 74, 128])\n"
     ]
    }
   ],
   "source": [
    "# Reshape image embeddings to match size of clinical embeddings\n",
    "train_image_features = torch.tensor(train_image_embeddings.reshape(n_train, 36, embed_dim))                             # Shape: [n_train, 36, embed_dim]\n",
    "test_image_features = torch.tensor(test_image_embeddings.reshape(n_test, 36, embed_dim))                                # Shape: [n_test, 36, embed_dim]\n",
    "\n",
    "# Combine clinical and image features\n",
    "train_patient_features = torch.cat([torch.tensor(train_clinical_embeddings), train_image_features], dim=1)              # Shape: [n_train, 74, embed_dim]\n",
    "test_patient_features = torch.cat([torch.tensor(test_clinical_embeddings), test_image_features], dim=1)                 # Shape: [n_test, 74, embed_dim]\n",
    "\n",
    "print('Reshaped Train Image Embeddings: ', train_image_features.shape)\n",
    "print('Combined Train Embeddings: ', train_patient_features.shape)\n",
    "print('Reshaped Test Image Embeddings: ', test_image_features.shape)\n",
    "print('Combined Test Embeddings: ', test_patient_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patient_edges(n_clinical, n_nodes):\n",
    "    \"\"\"\n",
    "    Creates bidirectional edges between clinical nodes and image nodes.\n",
    "    Adds a self-edge to each node.\n",
    "\n",
    "    Total edges = n_nodes (self-edges) + 2 * n_clinical * n_image_nodes (bidirectional edges)\n",
    "\n",
    "    Parameters:\n",
    "    - n_clinical: number of clinical nodes (for a specific patient)\n",
    "    - n_image_nodes: number of image nodes (for a specific patient)\n",
    "    \"\"\"\n",
    "    node_ids = np.expand_dims(np.arange(n_nodes, dtype=int), 0)\n",
    "    # self-edges = preserves some features of each own node during a graph convolution\n",
    "    self_edges = np.concatenate((node_ids, node_ids), 0)\n",
    "\n",
    "    # clinical nodes\n",
    "    c_array_asc = np.expand_dims(np.arange(n_clinical), 0)\n",
    "    all_edges = self_edges[:]\n",
    "\n",
    "    for i in range(n_clinical, n_nodes):\n",
    "        # image nodes\n",
    "        i_array = np.expand_dims(np.array([i]*n_clinical), 0)\n",
    "\n",
    "        # image --> clinical\n",
    "        inter_edges_ic = np.concatenate((i_array, c_array_asc), 0)\n",
    "        # clinical --> image\n",
    "        inter_edges_ci = np.concatenate((c_array_asc, i_array), 0)\n",
    "\n",
    "        # bidirectional edges\n",
    "        inter_edges_i = np.concatenate((inter_edges_ic, inter_edges_ci), 1)\n",
    "        all_edges = np.concatenate((all_edges, inter_edges_i), 1)\n",
    "\n",
    "    return torch.tensor(all_edges, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_list(patient_features, patient_labels):\n",
    "    \"\"\"\n",
    "    Generates a sub-graph for each patient given its embeddings\n",
    "\n",
    "    Parameters:\n",
    "    - patient_features: combined clinical and image embeddings of one patient\n",
    "    - patient_labels: groud truth values\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for i in range(len(patient_labels)):\n",
    "        # Create the graph for each patient\n",
    "        patient_edges = create_patient_edges(n_clinical, n_nodes)   # Shape: [2, num_edges]\n",
    "        patient_y = patient_labels[i]                               # Target label for this patient\n",
    "\n",
    "        data = Data(x=patient_features[i], edge_index=patient_edges, y=patient_y)\n",
    "        data_list.append(data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Patients:  84\n",
      "Test Patients:  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usama.Khatab\\Projects\\miniconda3\\envs\\hackathon\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_data_list = get_data_list(train_patient_features, train_labels)\n",
    "test_data_list = get_data_list(test_patient_features, test_labels)\n",
    "\n",
    "# Batch size 1 for individual patients\n",
    "train_loader = DataLoader(train_data_list, batch_size=1, shuffle=False, num_workers=0)  \n",
    "test_loader = DataLoader(test_data_list, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Train Patients: \", len(train_loader))\n",
    "print(\"Test Patients: \", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "We define the Graph Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)          # Second GCN layer\n",
    "        self.fc = torch.nn.Linear(hidden_channels, 1)                   # Fully connected layer for binary classification\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Apply graph convolution\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Global pooling (mean) across all nodes\n",
    "        x = global_mean_pool(x, batch)  # This will aggregate node features into one scalar per graph\n",
    "        \n",
    "        # Pass the aggregated feature through a fully connected layer to get a single logit\n",
    "        x = self.fc(x)  # Output size is (batch_size, 1)\n",
    "        return x  # Output a single logit for each patient (before applying sigmoid in loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Graph Attention Network\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, heads=2, dropout=0.5):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, dropout=dropout)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, 1)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)          # Aggregate node features\n",
    "        x = self.fc(x)                          # Binary classification output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Model Parameters\n",
    "learning_rate = 0.0001\n",
    "w_decay = 5e-4\n",
    "hidden_channels = 128\n",
    "\n",
    "# Initialize Model\n",
    "model = GCN(in_channels=embed_dim, hidden_channels=hidden_channels)\n",
    "# model = GAT(in_channels=embed_dim, hidden_channels=hidden_channels)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=w_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.6929318720386142\n",
      "Epoch 2/300, Loss: 0.6827408635900134\n",
      "Epoch 3/300, Loss: 0.6750122961543855\n",
      "Epoch 4/300, Loss: 0.6679679268882388\n",
      "Epoch 5/300, Loss: 0.6609917421426091\n",
      "Epoch 6/300, Loss: 0.6536318386594454\n",
      "Epoch 7/300, Loss: 0.6455290665229162\n",
      "Epoch 8/300, Loss: 0.6363561093097642\n",
      "Epoch 9/300, Loss: 0.6258446215873673\n",
      "Epoch 10/300, Loss: 0.6137918475128356\n",
      "Epoch 11/300, Loss: 0.6000146134978249\n",
      "Epoch 12/300, Loss: 0.5843705261747042\n",
      "Epoch 13/300, Loss: 0.5668111427554062\n",
      "Epoch 14/300, Loss: 0.5473760558026177\n",
      "Epoch 15/300, Loss: 0.5261340559948058\n",
      "Epoch 16/300, Loss: 0.5032630511337802\n",
      "Epoch 17/300, Loss: 0.4790101771553357\n",
      "Epoch 18/300, Loss: 0.45368667665336815\n",
      "Epoch 19/300, Loss: 0.42762251996568273\n",
      "Epoch 20/300, Loss: 0.4011286681606656\n",
      "Epoch 21/300, Loss: 0.37451000853131217\n",
      "Epoch 22/300, Loss: 0.3480432416268048\n",
      "Epoch 23/300, Loss: 0.3219765505443017\n",
      "Epoch 24/300, Loss: 0.2965397781781143\n",
      "Epoch 25/300, Loss: 0.2719309533990565\n",
      "Epoch 26/300, Loss: 0.24833966274967506\n",
      "Epoch 27/300, Loss: 0.2259048939726892\n",
      "Epoch 28/300, Loss: 0.20473512678983666\n",
      "Epoch 29/300, Loss: 0.184916075463185\n",
      "Epoch 30/300, Loss: 0.1665109669279662\n",
      "Epoch 31/300, Loss: 0.14955983101390302\n",
      "Epoch 32/300, Loss: 0.13405703534815638\n",
      "Epoch 33/300, Loss: 0.11997249184752859\n",
      "Epoch 34/300, Loss: 0.10725914653656739\n",
      "Epoch 35/300, Loss: 0.09584796865515056\n",
      "Epoch 36/300, Loss: 0.08565687899300385\n",
      "Epoch 37/300, Loss: 0.07658148626242542\n",
      "Epoch 38/300, Loss: 0.06852623551302324\n",
      "Epoch 39/300, Loss: 0.06138776836866912\n",
      "Epoch 40/300, Loss: 0.05508156842690715\n",
      "Epoch 41/300, Loss: 0.04951783124401811\n",
      "Epoch 42/300, Loss: 0.044605261554049015\n",
      "Epoch 43/300, Loss: 0.040267738268344795\n",
      "Epoch 44/300, Loss: 0.0364375434596912\n",
      "Epoch 45/300, Loss: 0.03304794067142731\n",
      "Epoch 46/300, Loss: 0.030045258128319846\n",
      "Epoch 47/300, Loss: 0.02738258081773806\n",
      "Epoch 48/300, Loss: 0.02501550236532542\n",
      "Epoch 49/300, Loss: 0.022908293996893224\n",
      "Epoch 50/300, Loss: 0.02102779856362585\n",
      "Epoch 51/300, Loss: 0.01934666963192978\n",
      "Epoch 52/300, Loss: 0.017840700281903127\n",
      "Epoch 53/300, Loss: 0.016488421553298223\n",
      "Epoch 54/300, Loss: 0.01527220093772658\n",
      "Epoch 55/300, Loss: 0.014176062258446195\n",
      "Epoch 56/300, Loss: 0.013186515577347433\n",
      "Epoch 57/300, Loss: 0.012291158484263829\n",
      "Epoch 58/300, Loss: 0.01147976331066318\n",
      "Epoch 59/300, Loss: 0.010743577567717736\n",
      "Epoch 60/300, Loss: 0.010074320059305007\n",
      "Epoch 61/300, Loss: 0.009464953508237764\n",
      "Epoch 62/300, Loss: 0.008909078098224114\n",
      "Epoch 63/300, Loss: 0.008401502976041576\n",
      "Epoch 64/300, Loss: 0.007937150069838458\n",
      "Epoch 65/300, Loss: 0.007511720765034538\n",
      "Epoch 66/300, Loss: 0.007121636457625192\n",
      "Epoch 67/300, Loss: 0.006763714927945416\n",
      "Epoch 68/300, Loss: 0.006434717095154545\n",
      "Epoch 69/300, Loss: 0.006132057285261301\n",
      "Epoch 70/300, Loss: 0.005853510205616615\n",
      "Epoch 71/300, Loss: 0.0055967136920896466\n",
      "Epoch 72/300, Loss: 0.005360099249910057\n",
      "Epoch 73/300, Loss: 0.005141740869194542\n",
      "Epoch 74/300, Loss: 0.004940082585476159\n",
      "Epoch 75/300, Loss: 0.004753847628502886\n",
      "Epoch 76/300, Loss: 0.0045817250982379066\n",
      "Epoch 77/300, Loss: 0.004422533507511703\n",
      "Epoch 78/300, Loss: 0.004275207004130131\n",
      "Epoch 79/300, Loss: 0.004138995312089156\n",
      "Epoch 80/300, Loss: 0.004012865924388669\n",
      "Epoch 81/300, Loss: 0.00389604850546935\n",
      "Epoch 82/300, Loss: 0.003787789448541021\n",
      "Epoch 83/300, Loss: 0.0036873961714826735\n",
      "Epoch 84/300, Loss: 0.0035943428762428603\n",
      "Epoch 85/300, Loss: 0.003508023621916003\n",
      "Epoch 86/300, Loss: 0.0034280151421193124\n",
      "Epoch 87/300, Loss: 0.00335368793072294\n",
      "Epoch 88/300, Loss: 0.003284634798828414\n",
      "Epoch 89/300, Loss: 0.0032205076754724445\n",
      "Epoch 90/300, Loss: 0.0031608529403196633\n",
      "Epoch 91/300, Loss: 0.0031053896178881196\n",
      "Epoch 92/300, Loss: 0.0030535704405078184\n",
      "Epoch 93/300, Loss: 0.003005219631675226\n",
      "Epoch 94/300, Loss: 0.0029599614911906733\n",
      "Epoch 95/300, Loss: 0.002917596092109826\n",
      "Epoch 96/300, Loss: 0.0028778516150937536\n",
      "Epoch 97/300, Loss: 0.002840381294369027\n",
      "Epoch 98/300, Loss: 0.0028049583195189328\n",
      "Epoch 99/300, Loss: 0.0027714079480271416\n",
      "Epoch 100/300, Loss: 0.0027394596033359234\n",
      "Epoch 101/300, Loss: 0.0027088267295636926\n",
      "Epoch 102/300, Loss: 0.002679411879918397\n",
      "Epoch 103/300, Loss: 0.002651184182468569\n",
      "Epoch 104/300, Loss: 0.0026238095448380115\n",
      "Epoch 105/300, Loss: 0.002597305642774743\n",
      "Epoch 106/300, Loss: 0.0025714682284333754\n",
      "Epoch 107/300, Loss: 0.002546256130504584\n",
      "Epoch 108/300, Loss: 0.0025215089197964907\n",
      "Epoch 109/300, Loss: 0.002497257349616467\n",
      "Epoch 110/300, Loss: 0.0024734393468861526\n",
      "Epoch 111/300, Loss: 0.0024500282413880215\n",
      "Epoch 112/300, Loss: 0.002426844565910696\n",
      "Epoch 113/300, Loss: 0.0024039618088387386\n",
      "Epoch 114/300, Loss: 0.0023812852191223925\n",
      "Epoch 115/300, Loss: 0.002358824909996266\n",
      "Epoch 116/300, Loss: 0.0023366062116890263\n",
      "Epoch 117/300, Loss: 0.0023145911390245254\n",
      "Epoch 118/300, Loss: 0.0022929349764862785\n",
      "Epoch 119/300, Loss: 0.002271328654409271\n",
      "Epoch 120/300, Loss: 0.0022500856196616256\n",
      "Epoch 121/300, Loss: 0.002228944551504918\n",
      "Epoch 122/300, Loss: 0.0022081404040580396\n",
      "Epoch 123/300, Loss: 0.0021875444461632774\n",
      "Epoch 124/300, Loss: 0.002167292326486772\n",
      "Epoch 125/300, Loss: 0.0021473913518439943\n",
      "Epoch 126/300, Loss: 0.002127745419762678\n",
      "Epoch 127/300, Loss: 0.002108439172951919\n",
      "Epoch 128/300, Loss: 0.0020893201510608335\n",
      "Epoch 129/300, Loss: 0.00207061739237837\n",
      "Epoch 130/300, Loss: 0.002052257322551062\n",
      "Epoch 131/300, Loss: 0.0020342344607472895\n",
      "Epoch 132/300, Loss: 0.0020166958282148003\n",
      "Epoch 133/300, Loss: 0.0019993816293728222\n",
      "Epoch 134/300, Loss: 0.0019824654437538163\n",
      "Epoch 135/300, Loss: 0.0019658712793075425\n",
      "Epoch 136/300, Loss: 0.0019496937016984226\n",
      "Epoch 137/300, Loss: 0.0019338891285033675\n",
      "Epoch 138/300, Loss: 0.0019183244107374602\n",
      "Epoch 139/300, Loss: 0.0019032588486749904\n",
      "Epoch 140/300, Loss: 0.001888530603688802\n",
      "Epoch 141/300, Loss: 0.0018741201930164866\n",
      "Epoch 142/300, Loss: 0.001860217362826959\n",
      "Epoch 143/300, Loss: 0.0018466665427955117\n",
      "Epoch 144/300, Loss: 0.0018335479775747185\n",
      "Epoch 145/300, Loss: 0.0018206669617592567\n",
      "Epoch 146/300, Loss: 0.001808289138092992\n",
      "Epoch 147/300, Loss: 0.001796349691695858\n",
      "Epoch 148/300, Loss: 0.00178474808052713\n",
      "Epoch 149/300, Loss: 0.001773647166763632\n",
      "Epoch 150/300, Loss: 0.001762808006051706\n",
      "Epoch 151/300, Loss: 0.0017523478391445852\n",
      "Epoch 152/300, Loss: 0.0017423445398803562\n",
      "Epoch 153/300, Loss: 0.0017327571328375295\n",
      "Epoch 154/300, Loss: 0.0017234794661687505\n",
      "Epoch 155/300, Loss: 0.0017145072979474207\n",
      "Epoch 156/300, Loss: 0.0017059496039586346\n",
      "Epoch 157/300, Loss: 0.0016977190690730432\n",
      "Epoch 158/300, Loss: 0.0016898676041732653\n",
      "Epoch 159/300, Loss: 0.0016823060932438239\n",
      "Epoch 160/300, Loss: 0.0016751775182932587\n",
      "Epoch 161/300, Loss: 0.0016682330668637796\n",
      "Epoch 162/300, Loss: 0.0016616860923802182\n",
      "Epoch 163/300, Loss: 0.0016553458338724643\n",
      "Epoch 164/300, Loss: 0.0016493379625466158\n",
      "Epoch 165/300, Loss: 0.0016436019341199426\n",
      "Epoch 166/300, Loss: 0.0016381051967376454\n",
      "Epoch 167/300, Loss: 0.0016328730146900246\n",
      "Epoch 168/300, Loss: 0.0016280242287480394\n",
      "Epoch 169/300, Loss: 0.0016234614508686632\n",
      "Epoch 170/300, Loss: 0.001619157420377416\n",
      "Epoch 171/300, Loss: 0.0016150420133271837\n",
      "Epoch 172/300, Loss: 0.001611104761693804\n",
      "Epoch 173/300, Loss: 0.001607583525323467\n",
      "Epoch 174/300, Loss: 0.0016041657460543017\n",
      "Epoch 175/300, Loss: 0.0016009915078982538\n",
      "Epoch 176/300, Loss: 0.0015980565445022962\n",
      "Epoch 177/300, Loss: 0.00159535087039417\n",
      "Epoch 178/300, Loss: 0.0015928361690818011\n",
      "Epoch 179/300, Loss: 0.0015906002412780903\n",
      "Epoch 180/300, Loss: 0.00158843947732142\n",
      "Epoch 181/300, Loss: 0.001586510934993428\n",
      "Epoch 182/300, Loss: 0.0015845696811777003\n",
      "Epoch 183/300, Loss: 0.001582927050592497\n",
      "Epoch 184/300, Loss: 0.0015813906688674165\n",
      "Epoch 185/300, Loss: 0.00158002552892694\n",
      "Epoch 186/300, Loss: 0.0015788061925507333\n",
      "Epoch 187/300, Loss: 0.0015777905922992172\n",
      "Epoch 188/300, Loss: 0.0015768812727401382\n",
      "Epoch 189/300, Loss: 0.0015759296081904765\n",
      "Epoch 190/300, Loss: 0.0015752001725645858\n",
      "Epoch 191/300, Loss: 0.0015745543087177793\n",
      "Epoch 192/300, Loss: 0.0015739409052565904\n",
      "Epoch 193/300, Loss: 0.001573435044260155\n",
      "Epoch 194/300, Loss: 0.0015731006080733053\n",
      "Epoch 195/300, Loss: 0.0015728241532997345\n",
      "Epoch 196/300, Loss: 0.001572420515306779\n",
      "Epoch 197/300, Loss: 0.0015721384661879884\n",
      "Epoch 198/300, Loss: 0.0015720289896177877\n",
      "Epoch 199/300, Loss: 0.001571888488061155\n",
      "Epoch 200/300, Loss: 0.0015718243645702994\n",
      "Epoch 201/300, Loss: 0.0015718026074214972\n",
      "Epoch 202/300, Loss: 0.0015717484836949595\n",
      "Epoch 203/300, Loss: 0.001571697064329263\n",
      "Epoch 204/300, Loss: 0.0015717829810455441\n",
      "Epoch 205/300, Loss: 0.0015718602577787049\n",
      "Epoch 206/300, Loss: 0.001571889427569182\n",
      "Epoch 207/300, Loss: 0.0015720261410370725\n",
      "Epoch 208/300, Loss: 0.0015721643369380174\n",
      "Epoch 209/300, Loss: 0.001572304065684009\n",
      "Epoch 210/300, Loss: 0.0015724846639170124\n",
      "Epoch 211/300, Loss: 0.0015725253541036398\n",
      "Epoch 212/300, Loss: 0.0015727613733057904\n",
      "Epoch 213/300, Loss: 0.0015726946250132251\n",
      "Epoch 214/300, Loss: 0.0015729872574887676\n",
      "Epoch 215/300, Loss: 0.0015730223677521628\n",
      "Epoch 216/300, Loss: 0.0015731933660087332\n",
      "Epoch 217/300, Loss: 0.0015732821913323797\n",
      "Epoch 218/300, Loss: 0.00157343477643562\n",
      "Epoch 219/300, Loss: 0.0015735167518349563\n",
      "Epoch 220/300, Loss: 0.0015734896522276575\n",
      "Epoch 221/300, Loss: 0.0015736040667044498\n",
      "Epoch 222/300, Loss: 0.0015736519444977776\n",
      "Epoch 223/300, Loss: 0.0015736376566391776\n",
      "Epoch 224/300, Loss: 0.001573771902474705\n",
      "Epoch 225/300, Loss: 0.0015736303432190436\n",
      "Epoch 226/300, Loss: 0.0015737149732168681\n",
      "Epoch 227/300, Loss: 0.0015736370827913301\n",
      "Epoch 228/300, Loss: 0.0015736947881080034\n",
      "Epoch 229/300, Loss: 0.0015737425431628556\n",
      "Epoch 230/300, Loss: 0.001573710956888265\n",
      "Epoch 231/300, Loss: 0.0015737417271295619\n",
      "Epoch 232/300, Loss: 0.0015737086622764416\n",
      "Epoch 233/300, Loss: 0.0015737227251866855\n",
      "Epoch 234/300, Loss: 0.0015736487545966935\n",
      "Epoch 235/300, Loss: 0.0015737940396190555\n",
      "Epoch 236/300, Loss: 0.001573594288423746\n",
      "Epoch 237/300, Loss: 0.0015735614717760612\n",
      "Epoch 238/300, Loss: 0.0015734847011550474\n",
      "Epoch 239/300, Loss: 0.0015734746171108868\n",
      "Epoch 240/300, Loss: 0.0015735152884579812\n",
      "Epoch 241/300, Loss: 0.0015733805534009228\n",
      "Epoch 242/300, Loss: 0.0015733447642213875\n",
      "Epoch 243/300, Loss: 0.0015733755672911122\n",
      "Epoch 244/300, Loss: 0.0015732522683936708\n",
      "Epoch 245/300, Loss: 0.0015732293473081967\n",
      "Epoch 246/300, Loss: 0.0015730988013404254\n",
      "Epoch 247/300, Loss: 0.0015729840214633094\n",
      "Epoch 248/300, Loss: 0.0015727644812657325\n",
      "Epoch 249/300, Loss: 0.0015727231240730145\n",
      "Epoch 250/300, Loss: 0.0015724385550475702\n",
      "Epoch 251/300, Loss: 0.0015723477888189205\n",
      "Epoch 252/300, Loss: 0.0015721155075966852\n",
      "Epoch 253/300, Loss: 0.0015718506280488025\n",
      "Epoch 254/300, Loss: 0.0015718120709737246\n",
      "Epoch 255/300, Loss: 0.0015716914474199126\n",
      "Epoch 256/300, Loss: 0.001571420737924373\n",
      "Epoch 257/300, Loss: 0.00157122790109757\n",
      "Epoch 258/300, Loss: 0.001570996804643766\n",
      "Epoch 259/300, Loss: 0.0015708873780091554\n",
      "Epoch 260/300, Loss: 0.0015706011983119034\n",
      "Epoch 261/300, Loss: 0.001570300827638892\n",
      "Epoch 262/300, Loss: 0.0015701675171970106\n",
      "Epoch 263/300, Loss: 0.001569807775272888\n",
      "Epoch 264/300, Loss: 0.00156963330850669\n",
      "Epoch 265/300, Loss: 0.0015694788043696289\n",
      "Epoch 266/300, Loss: 0.0015692252292345593\n",
      "Epoch 267/300, Loss: 0.0015689701864348969\n",
      "Epoch 268/300, Loss: 0.0015688737070636957\n",
      "Epoch 269/300, Loss: 0.001568505419180881\n",
      "Epoch 270/300, Loss: 0.0015683806673873083\n",
      "Epoch 271/300, Loss: 0.001568090284828421\n",
      "Epoch 272/300, Loss: 0.0015678821036133393\n",
      "Epoch 273/300, Loss: 0.0015676581166796193\n",
      "Epoch 274/300, Loss: 0.0015674298997293504\n",
      "Epoch 275/300, Loss: 0.0015672145007210063\n",
      "Epoch 276/300, Loss: 0.001567006105644742\n",
      "Epoch 277/300, Loss: 0.0015667114631859086\n",
      "Epoch 278/300, Loss: 0.0015664676763359846\n",
      "Epoch 279/300, Loss: 0.0015662848395766645\n",
      "Epoch 280/300, Loss: 0.0015660566608684825\n",
      "Epoch 281/300, Loss: 0.001565846836672877\n",
      "Epoch 282/300, Loss: 0.001565567669742796\n",
      "Epoch 283/300, Loss: 0.0015653706100887835\n",
      "Epoch 284/300, Loss: 0.00156517633101993\n",
      "Epoch 285/300, Loss: 0.0015649551991200063\n",
      "Epoch 286/300, Loss: 0.0015646888174333505\n",
      "Epoch 287/300, Loss: 0.0015645073233913752\n",
      "Epoch 288/300, Loss: 0.0015642636127224598\n",
      "Epoch 289/300, Loss: 0.001564041118810911\n",
      "Epoch 290/300, Loss: 0.0015638057608114088\n",
      "Epoch 291/300, Loss: 0.0015636893630298367\n",
      "Epoch 292/300, Loss: 0.0015634130841785268\n",
      "Epoch 293/300, Loss: 0.0015631962910014235\n",
      "Epoch 294/300, Loss: 0.001563024760300669\n",
      "Epoch 295/300, Loss: 0.0015627655146826804\n",
      "Epoch 296/300, Loss: 0.0015625698238615033\n",
      "Epoch 297/300, Loss: 0.0015623147986020957\n",
      "Epoch 298/300, Loss: 0.0015621673663493614\n",
      "Epoch 299/300, Loss: 0.0015619024717731615\n",
      "Epoch 300/300, Loss: 0.0015617520661657008\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "train_losses = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for data in train_loader:                                               # Iterate over each batch (here, each batch is one patient)\n",
    "                                                                            # Data object contains 'x' (features), 'edge_index' (graph edges), 'y' (labels)\n",
    "        patient_features = data.x                                           # Shape: (num_nodes, in_channels)\n",
    "        patient_edges = data.edge_index                                     # Shape: (2, num_edges)\n",
    "        patient_label = data.y.float()                                      # Target label\n",
    "        batch = data.batch\n",
    "\n",
    "        # Ensure correct format\n",
    "        patient_features = patient_features.float()\n",
    "        patient_edges = patient_edges.to(torch.long)                 \n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(patient_features, patient_edges, batch)                  # Output shape: (1, 1)\n",
    "        \n",
    "        # Binary Classification Loss\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(output.view(-1), patient_label)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Print loss after each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAIjCAYAAAA0vUuxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABR9UlEQVR4nO3dCXhU5fn+8TsJJICSsMkqsoiCiOyIiAitCK6AS8XlJ4gV61oVbQUVUGyLK1KVgqKIVSuoFdxBobijCIgCYgQVCGhYBBLZAibzv553/hMSskwSkpwzM9/PdR3nzJoncxKcO+/7PicuEAgEBAAAAAAoUnzRdwEAAAAADMEJAAAAAMIgOAEAAABAGAQnAAAAAAiD4AQAAAAAYRCcAAAAACAMghMAAAAAhEFwAgAAAIAwCE4AAAAAEAbBCQAizBVXXKHmzZuX6bl333234uLiyr0moCQ/d1u3bvW6FAAoM4ITAJQT+2BYku39999XrAa+ww8/XJEgEAjoueee06mnnqpatWqpRo0aOuGEEzRu3Djt2rVLfg0mRW3p6elelwgAEa+K1wUAQLSwD9p5/fvf/9Z7771X4PbjjjvukL7O1KlTlZOTU6bn3nXXXRo5cuQhff1ol52drUsvvVQvvfSSevXq5UKJBaePPvpI99xzj15++WXNmzdPDRo0kN9Mnjy50HBq4Q8AcGgITgBQTv7v//4v3/XPPvvMBaeDbz/Y7t273QfzkqpatWqZa6xSpYrbULQHHnjAhabbbrtNDz74YO7tV199tS666CINGjTIjZ698847lVpXSX5OLrzwQtWrV6/SagKAWMJUPQCoRH369FG7du20ZMkSNw3MPgjfcccd7r7XXntNZ599tho3bqykpCQdffTRuvfee90ISHFrnNauXeumYz300EN68skn3fPs+d26ddMXX3wRdo2TXb/hhhs0e/ZsV5s99/jjj9ecOXMK1G/TDLt27apq1aq5r/PEE0+U+7opG9Hp0qWLqlev7kKABc+NGzfme4xNPRs2bJiOPPJIV2+jRo00cOBA916ELF68WP3793evYa/VokULXXnllcV+7T179riwdOyxx2r8+PEF7j/33HM1dOhQ995YMDbnnHOOWrZsWejr9ejRw71feT3//PO531+dOnV08cUXKy0trcQ/J4fCjp8dq5kzZ7rXa9iwoQ477DANGDCgQA0lPRbm22+/daHyiCOOcI9t3bq17rzzzgKP27Fjh/v5tRGwlJQUdwwtEOZlf2w45ZRT3GNs9Mxeqzy+dwA4VPzZEQAq2S+//KIzzzzTfWC2D6KhKV/Tp093HxRHjBjhLv/3v/9pzJgxyszMzDfyUZT//Oc/+vXXX/WnP/3JfTi2kZPzzz9fP/zwQ9hRqo8//livvvqqrrvuOtWsWVOPPvqoLrjgAq1fv15169Z1j/nyyy91xhlnuJBiU9Ys0NmaH/uwXF7sPbAP0xb6LLhs2rRJ//znP/XJJ5+4rx+acma1rVy5UjfeeKMLkZs3b3YfuK3e0PV+/fq52mxqoj3PQpV9j+Heh+3bt+umm24qcmRuyJAheuaZZ/Tmm2/qpJNO0uDBg91tFlKt7pB169a5cJX32P3973/X6NGjXci46qqrtGXLFj322GMuHOX9/or7OSnOtm3bCtxm38fBU/WsDvsZuf322917NXHiRPXt21fLli1zwac0x+Lrr792UxrtZ8xG5ez9//777/XGG2+4r5OXfd8WYO31li5dqqeeekr169fX/fff7+63Y2pBtH379u5ny0LxmjVr3NcEAM8FAAAV4vrrrw8c/M9s79693W1Tpkwp8Pjdu3cXuO1Pf/pToEaNGoG9e/fm3jZ06NBAs2bNcq//+OOP7jXr1q0b2LZtW+7tr732mrv9jTfeyL1t7NixBWqy64mJiYE1a9bk3vbVV1+52x977LHc284991xXy8aNG3NvW716daBKlSoFXrMwVvdhhx1W5P379u0L1K9fP9CuXbvAnj17cm9/88033euPGTPGXd++fbu7/uCDDxb5WrNmzXKP+eKLLwKlMXHiRPc8e35R7D22x5x//vnuekZGRiApKSlw66235nvcAw88EIiLiwusW7fOXV+7dm0gISEh8Pe//z3f45YvX+7ew7y3F/dzUpjQcS1sa926de7jFixY4G5r0qRJIDMzM/f2l156yd3+z3/+s1THwpx66qmBmjVr5n6fITk5OQXqu/LKK/M95rzzznM/tyGPPPKIe9yWLVtK9H0DQGViqh4AVDL7K7r9Jf9gob/0Gxs5stbN9pd8m8pkU6HCsZGP2rVr51635xobcQrHRhts6l2I/cU/OTk597k2umQNEWx9j00lDGnVqpUbFSkPNrXORj9s1MumAobY9MU2bdrorbfeyn2fEhMT3bQzGx0qTGg0xEaF9u/fX+Ia7H03NupWlNB9NhJo7H2y98DWRQVzaJBNh7MRqaOOOspdt9Eua+phoy52bEObTZc75phjtGDBghL9nBTnv//9rxt5y7vZ6NjBbIQs7/doa6NsJPHtt98u1bGwEbMPP/zQTYEMfZ8hhU3fvOaaa/Jdt59RG1kLvZeh42bTVsvaAAUAKgrBCQAqWZMmTdwH/4PZNKXzzjvPrf2wD+M2zSzUWCIjIyPs6x78wTUUoooKF8U9N/T80HPtQ7St/7GgdLDCbisLm9pmbE3LwezDeuh+CxQ2tcuaM9j0NZvmZtMS87bc7t27t5vOZ1MKbW2OrX+yAJGVlVVsDaEwEQpQJQ1XFlptjdDChQvddZuqZuuT7PaQ1atXu2BlIcmObd5t1apV7j0uyc9Jcey9sBCcd7N1VgezGg4OOXYcQ2vESnosQsHa1mOVRLifUXu/evbs6aYx2rG1aYoWSAlRAPyA4AQAlSzvyFLeRfP2Yf+rr75yaztsfYiNFoTWfpTkg2NCQkKht+cdBamI53rh5ptv1nfffefWytiIiK0bsjbvtvYmFAReeeUVF2Ss8YU1NLBREWt0sHPnziJfN9Qq3tbtFCV0X9u2bfM1jbAGDvYh39hlfHy8/vCHP+Q+xo6h1WWNJQ4eFbLNGm2E+zmJdOF+zux7thEsG928/PLL3XttYer0008v0CQFACobwQkAfMCmndmUJVuQb40JbIG8jRbknXrnJVvAbwHFFuofrLDbyqJZs2buMjU1tcB9dlvo/hCbWnjrrbfq3Xff1YoVK7Rv3z49/PDD+R5jU+WsQYFNPXvhhRfcqN6MGTOKrCHUzc0abRT1Qd3Oz2XsGIVYZzq7bl3oLCDZND2bhpZ3WqPVawHBmiMcPCpkm9VaWWz0Ky+ry45jqFtjSY9FqJugvf/lxQLnaaedpgkTJuibb75xx88apRw8lREAKhvBCQB89Jf4vCM8FgT+9a9/yS/12Yd7a1n+008/5d5uH7bL63xG1rbbAtqUKVPyTamz17epbLa+xtiar7179+Z7roUSmzoXep5N/Tp4tKxjx47usrjpejZqZOdvsnBQWDttW9tj4dbanB8cdGxkxN4b6xRnI4d5p+kZ63Bo76NNHzy4NrtuwbmyWPjLOx3RRud+/vnn3PVqJT0WNs3QpgdOmzbNdTQ8+HsqrcK6ApbkuAFAZaAdOQD4wMknn+xGl+wcQX/+85/dlK7nnnvOV1Pl7HxNNrpja1CuvfZaNyLz+OOPu/Ut1sa6JKxRw9/+9rcCt9v5jKwRgU1NtIYINm3xkksuyW2BbSMht9xyi3usTdGzEQlrsmDT5azd9qxZs9xjbU2MefbZZ13otDVjFqosJEydOtWtHTvrrLOKrdHal9uUP6vFpvrZWimbQmatyu0cTDadz17/YPa6Ft4seFlAsuflZXXY9z5q1Ci3lsgabdjjf/zxR1e/tfK25x4KC0DWyv5gNtUtbztze79tdM3ea3vfrB25rXEaPny4u99ai5fkWBhrXW+v1blzZ/c92IiafX8WMkv6cxFi01Rtqp4FMxvVsnVfdhztfF32NQDAU5Xaww8AYkhR7ciPP/74Qh//ySefBE466aRA9erVA40bNw789a9/DcydO9e9hrWRDteOvLD23Ha7tYIO147caj2YfQ37WnnNnz8/0KlTJ9e+/Oijjw489dRTrg13tWrVwr4f9lpFtcy21wqZOXOm+xrW4rtOnTqByy67LLBhw4bc+7du3erqbdOmjWtvnpKSEujevbtrqR2ydOnSwCWXXBI46qij3OtYa+1zzjknsHjx4kBJZGdnB5555plAz549A8nJye77s+N2zz33BHbu3Fnk86xW+3769u1b5GP++9//Bk455RRXu232fdj3k5qaWqKfk9K2I8/78xNqR/7iiy8GRo0a5d4X+3k7++yzC7QTL8mxCFmxYoVrLV6rVi33XlkL9NGjRxeo7+A24/Ye2+32Mxz6+Ro4cKD7+befMbu04/jdd9+V+L0AgIoSZ//xNroBACKZjZzY2qGD183An2vpfve737m1WNaCHABQcqxxAgCUmLUkz8vCkp37p0+fPp7VBABAZWCNEwCgxKyL2hVXXOEu7Vw+kydPduca+utf/+p1aQAAVCiCEwCgxM444wy9+OKL7mSzdiJaO7nqP/7xjwInVAUAINqwxgkAAAAAwmCNEwAAAACEQXACAAAAgDBibo1TTk6OO7O7nXTQTjAJAAAAIDYFAgF3kvTGjRsrPr74MaWYC04Wmpo2bep1GQAAAAB8Ii0tTUceeWSxj4m54GQjTaE3Jzk52etyAAAAAHgkMzPTDaqEMkJxYi44habnWWgiOAEAAACIK8ESHppDAAAAAEAYBCcAAAAACIPgBAAAAABhxNwaJwAAAESP7Oxs7d+/3+sy4GNVq1ZVQkLCIb8OwQkAAAARaefOndqwYYM7Fw9QXOMHazV++OGH61AQnAAAABCRI00WmmrUqKEjjjiiRF3REHsCgYC2bNniflaOOeaYQxp5IjgBAAAg4tj0PPtQbKGpevXqXpcDH7OfkbVr17qfmUMJTjSHAAAAQMRipAmV9TNCcAIAAACAMAhOAAAAABAJwWnSpElq3ry5qlWrpu7du2vRokVFPrZPnz5uuO3g7eyzz67UmgEAAAA/sM/REydOLPHj33//fff5eceOHRVaV7TxPDjNnDlTI0aM0NixY7V06VJ16NBB/fv31+bNmwt9/Kuvvqqff/45d1uxYoVb5PWHP/yh0msHAAAASqqwP/7n3e6+++4yve4XX3yhq6++usSPP/nkk93n6JSUFFWk96MsoHneVW/ChAkaPny4hg0b5q5PmTJFb731lqZNm6aRI0cWeHydOnXyXZ8xY4ZrQ1lUcMrKynJbSGZmZrl/DwAAAEA4FlbyDh6MGTNGqampubflPc+QdQy0lutVqlQpUde40khMTFTDhg1L9Rx4POK0b98+LVmyRH379j1QUHy8u75w4cISvcbTTz+tiy++WIcddlih948fP96l6dDWtGnTcqsfAAAA/mDnwN21y5utpOfftbAS2uxzqY3GhK5/++23qlmzpt555x116dJFSUlJ+vjjj/X9999r4MCBatCggQtW3bp107x584qdqmev+9RTT+m8885zAwx2/qLXX3+9yJGg6dOnq1atWpo7d66OO+4493XOOOOMfEHvt99+05///Gf3uLp16+r222/X0KFDNWjQoDIfs+3bt2vIkCGqXbu2q/PMM8/U6tWrc+9ft26dzj33XHe/fdY//vjj9fbbb+c+97LLLsttR2/f4zPPPKOoDU5bt251Sdp+EPKy6+np6WGfb2uhbKreVVddVeRjRo0apYyMjNwtLS2tXGoHAACAf+zebSM23mz2tcuLzbi67777tGrVKrVv3147d+7UWWedpfnz5+vLL790gcbCxPr164t9nXvuuUcXXXSRvv76a/d8Cxnbtm0r5v3brYceekjPPfecPvzwQ/f6t912W+79999/v1544QUXTj755BM3i2v27NmH9L1eccUVWrx4sQt1Nmhio2xWq51vyVx//fVu5pjVs3z5cldDaFRu9OjR+uabb1zQtPdq8uTJqlevnqJ6qt6hsNGmE044QSeeeGKRj7G0bhsAAADgd+PGjdPpp5+eb5mK9QAIuffeezVr1iwXNm644YZiQ8kll1zi9v/xj3/o0UcfdYMOFrwKY2HFlswcffTR7rq9ttUS8thjj7kBCRvFMo8//nju6E9Z2MiSfQ8WwmzNlbFgZrPDLJDZMhwLbxdccIH7vG9atmyZ+3y7r1OnTuratWvuqFtF8zQ4WSq0xg6bNm3Kd7tdDzfvcteuXW59U94DGmlsudVLL1mnQKlVK6+rAQAAiFw1akg7d3r3tctLKAiE2IiTNY2wHgA2dc6mzO3ZsyfsiJONVoXYNLfk5OQim68ZmyoXCk2mUaNGuY+3WVubNm3KN1hhn+FtSmFOTk6Zvk8bJbL1W9ZRO8SmALZu3drdZ2xq4LXXXqt3333XLeWxEBX6vux2u27N5fr16+emDIYCWFRO1bOFafaG29BjiL35dr1Hjx7FPvfll192Q3f/93//p0hl/TCGD5emTvW6EgAAgMgWF2cBwZvNvnZ5OXjdvk2XsxEmGzX66KOPtGzZMjcCY70CilO1atWD3p+4YkNOYY+3qXNeuuqqq/TDDz/o8ssvd1P1LFTayJex9VC2BuqWW27RTz/9pNNOOy3f1MKobEdurcinTp2qZ5991qVLS482mhTqsmcLxmxYsLBpepYsLZlGqlDm+/e/bcGd19UAAADAb2wqm027sylyFphsVtbatWsrtQZrZNGgQQPX9jzE+hTYaE9ZWRMKGz37/PPPc2/75ZdfXJfBtm3b5t5mU/euueYad0qiW2+91eWGEGsMYQ0qnn/+edcc48knn1RUr3EaPHiwtmzZ4toxWkOIjh07as6cObkNI2wY0jrt5WVvqHUZsWG7SGbn7LU1bNYHY84c6ZxzvK4IAAAAfmLd4iw0WEMIGwWypghlnR53KG688UbXrbpVq1Zq06aNG/mxznZWUzg2WmQdA0PsObZuy7oF2mmJnnjiCXe/NcZo0qSJu93cfPPNbmTp2GOPdV9rwYIFLnAZyw42c8067dkstDfffDP3vqgNTqHFZ0UtbrN2iQezuY9eDx2Wh8TE4KiTdY+07okEJwAAABx8ztMrr7zSrd+x/gDWBtyL85LefvvtbpDDZoPZ+iY74W7//v3dfjinnnpqvuv2HBttsg59N910k8455xw39dAeZw0nQtMGbVTLOutt2LDBrdGyxhaPPPJI7pIfm5Vmo2/WjrxXr16u/0FFigtEQwIpBftBs+FGW+RmB8Bry5fb4j3Jzm3200825Oh1RQAAAP63d+9e/fjjj2rRooWqVavmdTkxJycnx43wWMtz6/QXqT8rpckGnq9xinXWXbFLl+Aapxde8LoaAAAAoKB169a59UXfffedm3pnfQksjFx66aWKFQQnH7jyyuClrXXzYMoqAAAAUKz4+HhNnz5d3bp1U8+ePV14mjdvXoWvK/ITX6xxinUW1EeOlL75RrITMJ9/vtcVAQAAAMrX3e6TTz5RLGPEyQdq1ZJuuim4f/fdjDoBAAAAfkNw8olbbpFsPZo1i5g1y+tqAAAAIkOM9TmDhz8jBCefqFPnwKjTPfcw6gQAAFCcUBtsa2MNFCf0M1KS1unFYY2Tz0ad/vnP4KjTSy9JF1/sdUUAAAD+VKVKFdWoUUNbtmxx5/2x5gVAYW3T7WfEflbsZ+ZQcB4nn7E2+GPGSC1aSKtWSUlJXlcEAADg35EEa4ltH46BoliotnM42UlzDyUbEJx8ZtcuqVUrKT1dmjjxwPQ9AAAAFGShiel6KI4FpqJGJAlOERyczJNPSn/6k1S3rvT991JKitcVAQAAALGdDZgM6tMT4rZpI/3yi/TAA15XAwAAAIDg5EO2bu2++4L7jzwibdzodUUAAABAbCM4+dSAAVLPntKePdLYsV5XAwAAAMQ2gpNPxcVJDz4Y3H/mGWnlSq8rAgAAAGIXwcnHevSQzj8/eDLckSO9rgYAAACIXQQnn/vHP+wsx9Kbb0offOB1NQAAAEBsIjj5XOvW0tVXB/f/+lcptprHAwAAAP5AcIoA1hzisMOkRYukV17xuhoAAAAg9hCcIkCDBtJttwX377hD4uTYAAAAQOUiOEWIW28NBqg1a6Rnn/W6GgAAACC2EJwiRM2aBzrr2clxf/vN64oAAACA2EFwiiDDh0v16kk//CC99JLX1QAAAACxg+AUQaxBxM03H2hTbud3AgAAAFDxCE4R5vrrpeRkaeVK6Y03vK4GAAAAiA0EpwhTq1YwPJkHHvC6GgAAACA2EJwi0A03SFWqSJ9+Kn35pdfVAAAAANGP4BSBGjeWLrwwuD9pktfVAAAAANGP4BTBo07mhRekX37xuhoAAAAguhGcItTJJ0sdO0p790rTpnldDQAAABDdCE4RKi5OuvHG4P6//kVrcgAAAKAiEZwi2CWXBLvsrV0rLVjgdTUAAABA9CI4RbDq1aXBg4P7//6319UAAAAA0YvgFOGGDAle/ve/0s6dXlcDAAAARCeCU4Tr0UNq1UratUt69VWvqwEAAACiE8EpCppEhEadmK4HAAAAVAyCUxS4/PLg5f/+J6WleV0NAAAAEH0ITlGgeXOpd28pEJBmzPC6GgAAACD6EJyiRKi7HuucAAAAgPJHcIoSgwYF1zt99pm0YYPX1QAAAADRheAUJRo1CnbYM7Nne10NAAAAEF0ITlHkgguCl0zXAwAAAMoXwSmKnHde8PKDD6QtW7yuBgAAAIgeBKco0qKF1KmTlJMjvf6619UAAAAA0YPgFGWYrgcAAACUP4JTlBkwIHi5YIG0Z4/X1QAAAADRgeAUZdq1kxo3Doamjz7yuhoAAAAgOhCcooydy+mMM4L7c+Z4XQ0AAAAQHQhOUejMM4OXBCcAAAAgSoLTpEmT1Lx5c1WrVk3du3fXokWLin38jh07dP3116tRo0ZKSkrSscceq7fffrvS6o0EfftKCQnSqlXSunVeVwMAAABEPk+D08yZMzVixAiNHTtWS5cuVYcOHdS/f39t3ry50Mfv27dPp59+utauXatXXnlFqampmjp1qpo0aVLptftZrVrSSScF9+fO9boaAAAAIPJ5GpwmTJig4cOHa9iwYWrbtq2mTJmiGjVqaNq0aYU+3m7ftm2bZs+erZ49e7qRqt69e7vAhfxY5wQAAABEQXCy0aMlS5aor80rCxUTH++uL1y4sNDnvP766+rRo4ebqtegQQO1a9dO//jHP5SdnV3k18nKylJmZma+LZaC07x50v79XlcDAAAARDbPgtPWrVtd4LEAlJddT09PL/Q5P/zwg5uiZ8+zdU2jR4/Www8/rL/97W9Ffp3x48crJSUld2vatKliQefOUt260q+/SosXe10NAAAAENk8bw5RGjk5Oapfv76efPJJdenSRYMHD9add97ppvgVZdSoUcrIyMjd0tLSFAvi46XevYP7H3zgdTUAAABAZPMsONWrV08JCQnatGlTvtvtesOGDQt9jnXSsy569ryQ4447zo1Q2dS/wljnveTk5HxbrAgFp/ff97oSAAAAILJ5FpwSExPdqNH8+fPzjSjZdVvHVBhrCLFmzRr3uJDvvvvOBSp7PeTXp0/w8pNPWOcEAAAAROxUPWtFbu3En332Wa1atUrXXnutdu3a5brsmSFDhripdiF2v3XVu+mmm1xgeuutt1xzCGsWgYLatZPq1JF27pSWLvW6GgAAACByVfHyi9sapS1btmjMmDFuul3Hjh01Z86c3IYR69evd532Qqyxw9y5c3XLLbeoffv27vxNFqJuv/12D78L/7K37tRTpdmzg+ucunf3uiIAAAAgMsUFAoGAYoi1I7fuetYoIhbWO02cKN1yi3TmmdLbb3tdDQAAABCZ2SCiuuqh7A0iPv5Y+u03r6sBAAAAIhPBKcq1by/VqhU8n9OXX3pdDQAAABCZCE5Rzjq39+oV3P/wQ6+rAQAAACITwSkGnHJK8HLhQq8rAQAAACITwSkGnHTSgeAUW61AAAAAgPJBcIoBXbsGp+z99JO0YYPX1QAAAACRh+AUA2rUkDp0CO4zXQ8AAAAoPYJTjOjRI3j52WdeVwIAAABEHoJTDK5zAgAAAFA6BKcYG3FaulTKyvK6GgAAACCyEJxiRMuWUr160r59nAgXAAAAKC2CU4yIizsw6sR0PQAAAKB0CE4xhAYRAAAAQNkQnGKwQcTnn3tdCQAAABBZCE4xpHPn4OW6ddIvv3hdDQAAABA5CE4xJCVFatUquL9kidfVAAAAAJGD4BRjunQJXhKcAAAAgJIjOMWYrl2DlwQnAAAAoOQITjE64rR4sdeVAAAAAJGD4BRjaBABAAAAlB7BKcbQIAIAAAAoPYJTDKJBBAAAAFA6BKcYRHACAAAASofgFIMITgAAAEDpEJxiuEHE2rU0iAAAAABKguAUg2rVklq2DO4vW+Z1NQAAAID/EZxiVIcOwcuvv/a6EgAAAMD/CE4xHpy++srrSgAAAAD/IzjFKIITAAAAUHIEpxgVCk7ffCPt3+91NQAAAIC/EZxiVLNmUs2a0r59Umqq19UAAAAA/kZwilHx8VL79sF9pusBAAAAxSM4xTDWOQEAAAAlQ3CKYQQnAAAAoGQITjGMczkBAAAAJUNwimHt2klxcVJ6urR5s9fVAAAAAP5FcIphhx0mtWoV3Ge6HgAAAFA0glOMY50TAAAAEB7BKcaFWpKvWOF1JQAAAIB/EZxi3PHHBy8JTgAAAEDRCE4xzhpEmFWrpJwcr6sBAAAA/IngFOOOPlpKSpJ275bWrvW6GgAAAMCfCE4xLiFBatMmuM90PQAAAKBwBCfkTtdbudLrSgAAAAB/IjiBBhEAAABAGAQn5AYnRpwAAACAwhGckK+z3m+/eV0NAAAA4D8EJ6h5c6lGDWnfPun7772uBgAAAPAfghMUHy8dd1xwn+l6AAAAQEEEJ+SbrkeDCAAAAMCnwWnSpElq3ry5qlWrpu7du2vRokVFPnb69OmKi4vLt9nzcGhoEAEAAAD4ODjNnDlTI0aM0NixY7V06VJ16NBB/fv31+bNm4t8TnJysn7++efcbd26dZVaczRixAkAAADwcXCaMGGChg8frmHDhqlt27aaMmWKatSooWnTphX5HBtlatiwYe7WoEGDSq05GrVtG7xcvZrOegAAAICvgtO+ffu0ZMkS9e3b90BB8fHu+sKFC4t83s6dO9WsWTM1bdpUAwcO1Mpi5pdlZWUpMzMz34aCmjaVqleX9u+XfvjB62oAAAAAf/E0OG3dulXZ2dkFRozsenp6eqHPad26tRuNeu211/T8888rJydHJ598sjZs2FDo48ePH6+UlJTczcIWCu+s17p1cP/bb72uBgAAAPAXz6fqlVaPHj00ZMgQdezYUb1799arr76qI444Qk888UShjx81apQyMjJyt7S0tEqvOVK0aRO8JDgBAAAA+VWRh+rVq6eEhARt2rQp3+123dYulUTVqlXVqVMnrVmzptD7k5KS3IbwCE4AAACAD0ecEhMT1aVLF82fPz/3Npt6Z9dtZKkkbKrf8uXL1ahRowqsNDYQnAAAAAAfjjgZa0U+dOhQde3aVSeeeKImTpyoXbt2uS57xqblNWnSxK1VMuPGjdNJJ52kVq1aaceOHXrwwQddO/KrrrrK4+8kuoJTIGDdC72uCAAAAPAHz4PT4MGDtWXLFo0ZM8Y1hLC1S3PmzMltGLF+/XrXaS9k+/btrn25PbZ27dpuxOrTTz91rcxxaI45JhiWtm+XtmyR6tf3uiIAAADAH+ICARtbiB3Wjty661mjCDuRLvJr0UJau1b64APp1FO9rgYAAADwRzaIuK56qFiscwIAAAAKIjghn+OOC14SnAAAAIADCE7IhxEnAAAAoCCCE/IhOAEAAAAFEZxQaHCyBhF79nhdDQAAAOAPBCfkc8QRUu3awfM4rV7tdTUAAACAPxCckI+dx4npegAAAEB+BCcUQHACAAAA8iM4ocjgtGqV15UAAAAA/kBwQgGMOAEAAAD5EZxQZHBKTZVycryuBgAAAPAewQkFtGghVa0abEeeluZ1NQAAAID3CE4owEJTq1bBfabrAQAAAAQnFIF1TgAAAMABBCcUiuAEAAAAHEBwQqEITgAAAMABBCcUiuAEAAAAHEBwQrHBKT1d2rHD62oAAAAAbxGcUKjkZKlx4wPncwIAAABiGcEJRWK6HgAAABBEcELY4LRqldeVAAAAAN4iOKFIrVsHL7/7zutKAAAAAG8RnFCkY48NXhKcAAAAEOsITijSMccEL9eskXJyvK4GAAAA8A7BCUVq1kyqWlXKypLS0ryuBgAAAPAOwQlFqlJFatkyuM90PQAAAMQyghNKtM5p9WqvKwEAAAC8Q3BCsWgQAQAAABCcUMIGEYw4AQAAIJYRnFAsRpwAAAAAghNKGJx+/FHav9/ragAAAABvEJxQrMaNpRo1pOzsYHgCAAAAYhHBCcWKizuwzonpegAAAIhVBCeExTonAAAAxDqCE8Kisx4AAABiHcEJYTHiBAAAgFhHcEJYjDgBAAAg1hGcUOIRp7Q0afdur6sBAAAAKh/BCWHVrSvVrh3cX7PG62oAAACAykdwQqlakjNdDwAAALGI4IQSoUEEAAAAYhnBCSVCcAIAAEAsIzihRJiqBwAAgFhGcEKJMOIEAACAWEZwQqlGnLZskXbs8LoaAAAAoHIRnFAiNWtKDRsG95muBwAAgFhDcEKJMV0PAAAAsYrghBKjQQQAAABiFcEJJcaIEwAAAGIVwQklRnACAABArPJFcJo0aZKaN2+uatWqqXv37lq0aFGJnjdjxgzFxcVp0KBBFV4j8k/VCwS8rgYAAACIoeA0c+ZMjRgxQmPHjtXSpUvVoUMH9e/fX5s3by72eWvXrtVtt92mXr16VVqtse7oo6W4OCkzUwpzeAAAAICo4nlwmjBhgoYPH65hw4apbdu2mjJlimrUqKFp06YV+Zzs7Gxddtlluueee9SyZctKrTeWVasmNWsW3Ge6HgAAAGKJp8Fp3759WrJkifr27XugoPh4d33hwoVFPm/cuHGqX7++/vjHP4b9GllZWcrMzMy3oezorAcAAIBY5Glw2rp1qxs9atCgQb7b7Xp6enqhz/n444/19NNPa+rUqSX6GuPHj1dKSkru1rRp03KpPVbRIAIAAACxyPOpeqXx66+/6vLLL3ehqV69eiV6zqhRo5SRkZG7paWlVXid0YwRJwAAAMSiKl5+cQs/CQkJ2rRpU77b7XrDhg0LPP777793TSHOPffc3NtycnLcZZUqVZSamqqjrYNBHklJSW5D+WDECQAAALHI0xGnxMREdenSRfPnz88XhOx6jx49Cjy+TZs2Wr58uZYtW5a7DRgwQL/73e/cPtPwKi84rVljx8rragAAAIAYGHEy1op86NCh6tq1q0488URNnDhRu3btcl32zJAhQ9SkSRO3VsnO89SuXbt8z69Vq5a7PPh2VAzrqlelirR3r7Rhg3TUUV5XBAAAAMRAcBo8eLC2bNmiMWPGuIYQHTt21Jw5c3IbRqxfv9512oM/WGiy2ZCpqcHpegQnAAAAxIK4QCAQUAyxduTWXc8aRSQnJ3tdTkQaMEB64w1p0iTpuuu8rgYAAACo+GzAUA5Kjc56AAAAiDUEJ5QanfUAAAAQawhOKHNwYsQJAAAAsYLghDJP1fvhB2n/fq+rAQAAACoewQml1rixVKOGlJ0t/fij19UAAAAAFY/ghFKz7vCtWgX3ma4HAACAWEBwQpnQIAIAAACxhOCEMiE4AQAAIJYQnHBIDSIITgAAAIgFBCeUCS3JAQAAEEsITjik4JSWJu3e7XU1AAAAQMUiOKFM6taVatcO7q9Z43U1AAAAQMUiOKFM4uKYrgcAAIDYUabglJaWpg0bNuReX7RokW6++WY9+eST5VkbfI7OegAAAIgVZQpOl156qRYsWOD209PTdfrpp7vwdOedd2rcuHHlXSN8is56AAAAiBVlCk4rVqzQiSee6PZfeukltWvXTp9++qleeOEFTZ8+vbxrhE8x4gQAAIBYUabgtH//fiUlJbn9efPmacCAAW6/TZs2+vnnn8u3QvgWwQkAAACxokzB6fjjj9eUKVP00Ucf6b333tMZZ5zhbv/pp59U19qtIaam6m3dKm3f7nU1AAAAgM+C0/33368nnnhCffr00SWXXKIOHTq4219//fXcKXyIfocfLjVuHNynsx4AAACiWZWyPMkC09atW5WZmanaoZP5SLr66qtVo0aN8qwPETDq9NNPwel6ZGYAAABEqzKNOO3Zs0dZWVm5oWndunWaOHGiUlNTVb9+/fKuET7GOicAAADEgjIFp4EDB+rf//6329+xY4e6d++uhx9+WIMGDdLkyZPLu0b4GMEJAAAAsaBMwWnp0qXq1auX23/llVfUoEEDN+pkYerRRx8t7xoRAcGJNU4AAACIZmUKTrt371bNmjXd/rvvvqvzzz9f8fHxOumkk1yAQmyOOAUCXlcDAAAA+Cg4tWrVSrNnz1ZaWprmzp2rfv36uds3b96s5OTk8q4RPtaypRQfL+3cKaWne10NAAAA4KPgNGbMGN12221q3ry5az/eo0eP3NGnTp06lXeN8LHERKl58+A+65wAAAAQrcoUnC688EKtX79eixcvdiNOIaeddpoeeeSR8qwPEYB1TgAAAIh2ZTqPk2nYsKHbNmzY4K4feeSRnPw2hoPTnDmMOAEAACB6lWnEKScnR+PGjVNKSoqaNWvmtlq1aunee+919yG20JIcAAAA0a5MI0533nmnnn76ad13333q2bOnu+3jjz/W3Xffrb179+rvf/97edcJHyM4AQAAINrFBQKlbyLduHFjTZkyRQMGDMh3+2uvvabrrrtOGzdulF9lZma6kbKMjAw6AJaTtWulFi2CjSJ275YSEryuCAAAACjfbFCmqXrbtm1TmzZtCtxut9l9iC1Nm0pJSdK+fdL69V5XAwAAAJS/MgWnDh066PHHHy9wu93Wvn378qgLEcRGmFq1Cu4zXQ8AAADRqExrnB544AGdffbZmjdvXu45nBYuXOhOiPv222+Xd42IkHVOK1cGg1P//l5XAwAAAPhgxKl379767rvvdN5552nHjh1uO//887Vy5Uo999xz5VwiIgENIgAAABDNytQcoihfffWVOnfurOzsbPkVzSEqxrRp0h//KPXrJ+U5JzIAAAAQu80hgIMdc0zwcvVqrysBAAAAyh/BCeU6Vc9ak2dleV0NAAAAUL4ITigX9etLNrppEz+//97ragAAAAAPu+pZA4jiWJMIxKa4uOCo0+LFUmqq1Lat1xUBAAAAHgUnWzgV7v4hQ4Ycak2IUKHgRGc9AAAAxHRweuaZZyquEkS8Nm2ClzbiBAAAAEQT1jih3LRuHbz89luvKwEAAADKF8EJ5T7iZMGp/M4OBgAAAHiP4IRyPZeTNYnYvl3autXragAAAIDyQ3BCualeXWrWLLjPdD0AAABEE4ITKmy6HgAAABAtCE4oVwQnAAAARCOCEyqksx4tyQEAABBNCE4oV4w4AQAAIBr5IjhNmjRJzZs3V7Vq1dS9e3ctWrSoyMe++uqr6tq1q2rVqqXDDjtMHTt21HPPPVep9SJ8cPrxRykry+tqAAAAgCgJTjNnztSIESM0duxYLV26VB06dFD//v21efPmQh9fp04d3XnnnVq4cKG+/vprDRs2zG1z586t9NpRUIMGUnKylJMjrVnjdTUAAABA+YgLBLw9VamNMHXr1k2PP/64u56Tk6OmTZvqxhtv1MiRI0v0Gp07d9bZZ5+te++9t8B9WVlZbgvJzMx0r5+RkaFk+4SPcte9u2SDhq+8Il1wgdfVAAAAAIWzbJCSklKibODpiNO+ffu0ZMkS9e3b90BB8fHuuo0ohWOZb/78+UpNTdWpp55a6GPGjx/v3ozQZqEJFYt1TgAAAIg2nganrVu3Kjs7Ww1sflcedj09Pb3I51kiPPzww5WYmOhGmh577DGdfvrphT521KhR7vGhLS0trdy/D+RHZz0AAABEmyqKQDVr1tSyZcu0c+dON+Jka6RatmypPn36FHhsUlKS21B5GHECAABAtPE0ONWrV08JCQnatGlTvtvtesOGDYt8nk3na9Wqldu3rnqrVq1yU/IKC07wNjjZCrq4OK8rAgAAACJ4qp5NtevSpYsbNQqx5hB2vUePHiV+HXtO3gYQ8NbRR1u4lX79VSpmxiUAAAAQMTyfqmfT7IYOHerOzXTiiSdq4sSJ2rVrl2sxboYMGaImTZq4ESVjl/bYo48+2oWlt99+253HafLkyR5/JwixmZEtWwbbkduoU6NGXlcEAAAARHhwGjx4sLZs2aIxY8a4hhA29W7OnDm5DSPWr1/vpuaFWKi67rrrtGHDBlWvXl1t2rTR888/714H/pquFwpOv/ud19UAAAAAEX4eJz/3akfZ3Xab9PDD0k03SRMnel0NAAAAEMHncUL0orMeAAAAognBCRV6LieCEwAAAKIBwQkVOuK0fr20e7fX1QAAAACHhuCEClGvnlSnTvA8TqtXe10NAAAAcGgITqgQdtJbpusBAAAgWhCcUOHT9VJTva4EAAAAODQEJ1QYOusBAAAgWhCcUGGYqgcAAIBoQXBCpUzVy8nxuhoAAACg7AhOqDAtW0pVqgTbkW/Y4HU1AAAAQNkRnFBhqlaVjj02uP/NN15XAwAAAJQdwQkVqm3b4CXBCQAAAJGM4IQKRXACAABANCA4oUIRnAAAABANCE6otOAUCHhdDQAAAFA2BCdUKGsOER8vZWRIP//sdTUAAABA2RCcUKGSkqRWrYL7TNcDAABApCI4ocKxzgkAAACRjuCECnf88cFLghMAAAAiFcEJlTbitHKl15UAAAAAZUNwQqUGJzrrAQAAIBIRnFDhWreW4uKk7dulzZu9rgYAAAAoPYITKlz16lLLlsF91jkBAAAgEhGcUClY5wQAAIBIRnBCpXbWIzgBAAAgEhGcUClOOCF4uWKF15UAAAAApUdwQqVo1y54uXw5nfUAAAAQeQhOqLTOegkJUkaGtHGj19UAAAAApUNwQqVISgqGJ8N0PQAAAEQaghMqfboewQkAAACRhuAET9Y5AQAAAJGE4IRKw4gTAAAAIhXBCZXekvybb6TsbK+rAQAAAEqO4IRK06KFVL26tHev9MMPXlcDAAAAlBzBCZXG2pG3bRvcZ50TAAAAIgnBCZ5M12OdEwAAACIJwQmVigYRAAAAiEQEJ3gSnL7+2utKAAAAgJIjOKFStW8fvFy9Wtqzx+tqAAAAgJIhOKFSNWwoHXGElJPDdD0AAABEDoITKlVcnNShQ3D/q6+8rgYAAAAoGYITKh3BCQAAAJGG4IRKR3ACAABApCE4wbPgZJ31AgGvqwEAAADCIzih0rVpI1WtKmVkSOvXe10NAAAAEB7BCZUuMVE67rjgPtP1AAAAEAkITvAE65wAAAAQSQhO8ATBCQAAAJGE4ARPEJwAAAAQSXwRnCZNmqTmzZurWrVq6t69uxYtWlTkY6dOnapevXqpdu3abuvbt2+xj4e/g9P330s7d3pdDQAAAODz4DRz5kyNGDFCY8eO1dKlS9WhQwf1799fmzdvLvTx77//vi655BItWLBACxcuVNOmTdWvXz9t3Lix0mtH2R1xhNSoUbAdubUlBwAAAPwsLhDw9kw6NsLUrVs3Pf744+56Tk6OC0M33nijRo4cGfb52dnZbuTJnj9kyJCwj8/MzFRKSooyMjKUnJxcLt8Dyuacc6S33pIee0y64QavqwEAAECsySxFNvB0xGnfvn1asmSJm26XW1B8vLtuo0klsXv3bu3fv1916tQp9P6srCz3huTd4A+dOwcvly71uhIAAABA/g1OW7dudSNGDRo0yHe7XU9PTy/Ra9x+++1q3LhxvvCV1/jx412KDG02mgV/IDgBAAAgUni+xulQ3HfffZoxY4ZmzZrlGksUZtSoUW7oLbSlpaVVep0oPjitXCnt3et1NQAAAEDRqshD9erVU0JCgjZt2pTvdrvesGHDYp/70EMPueA0b948tW/fvsjHJSUluQ3+Y4N/detKv/wirVghde3qdUUAAACAD0ecEhMT1aVLF82fPz/3NmsOYdd79OhR5PMeeOAB3XvvvZozZ4668mk7YsXFSV26BPeZrgcAAAA/83yqnrUit3MzPfvss1q1apWuvfZa7dq1S8OGDXP3W6c8m24Xcv/992v06NGaNm2aO/eTrYWybScnA4pIrHMCAABAJPB0qp4ZPHiwtmzZojFjxrgA1LFjRzeSFGoYsX79etdpL2Ty5MmuG9+FF16Y73XsPFB33313pdeP8glOS5Z4XQkAAADg4/M4VTbO4+Qv338vtWpl0zYlGzSsWtXrigAAABArMiPlPE5Ay5ZSSoqd00v65huvqwEAAAAKR3CC5w0iOnUK7rPOCQAAAH5FcIJv1jktXux1JQAAAEDhCE7w3IknBi+/+MLrSgAAAIDCEZzgm+C0bJmUleV1NQAAAEBBBCd4rnlzqV49af9+6auvvK4GAAAAKIjgBF80iAiNOi1a5HU1AAAAQEEEJ/gCwQkAAAB+RnCCLxCcAAAA4GcEJ/hCt27By9RUaccOr6sBAAAA8iM4wResOUTLlsF9zucEAAAAvyE4wTeYrgcAAAC/IjjBd9P1CE4AAADwG4ITfDfi9NlnUiDgdTUAAADAAQQn+EaXLlLVqtKmTdLatV5XAwAAABxAcIJvVK8ude4c3P/kE6+rAQAAAA4gOMFXevYMXhKcAAAA4CcEJ/gyOH36qdeVAAAAAAcQnOArJ58cvFy+XMrI8LoaAAAAIIjgBF9p2DB4Ilzrqvf5515XAwAAAAQRnODbUSfWOQEAAMAvCE7wHdY5AQAAwG8ITvBtcLIT4f72m9fVAAAAAAQn+FDbtlJysrRzZ7BJBAAAAOA1ghN8JyFBOuWU4P4HH3hdDQAAAEBwgk/16RO8fP99rysBAAAACE7weXD68EMpJ8fragAAABDrCE7wpU6dpJo1pe3bpa+/9roaAAAAxDqCE3ypShWpV6/gPtP1AAAA4DWCE3yLdU4AAADwC4ITfIt1TgAAAPALghN8i3VOAAAA8AuCEyJindOCBV5XAwAAgFhGcEJETNebP9/rSgAAABDLCE7wtX79Dow4ZWV5XQ0AAABiFcEJvta+vdSggbR7t/Tpp15XAwAAgFhFcIKvxcUdGHWaO9fragAAABCrCE7wvf79g5cEJwAAAHiF4ATfO/304OWyZdKmTV5XAwAAgFhEcILv1a8fPKeTee89r6sBAABALCI4ISKE1jm9+67XlQAAACAWEZwQEfI2iMjJ8boaAAAAxBqCEyLCKadINWtKmzdLX3zhdTUAAACINQQnRITEROnMM4P7r7/udTUAAACINQQnRIwBA4KXBCcAAABUNoITIoaNOCUkSCtWSD/84HU1AAAAiCUEJ0SMOnWkXr2C+2+84XU1AAAAiCUEJ0QUpusBAADACwQnRGRw+uADaft2r6sBAABArCA4IaIcfbTUtq2UnS299ZbX1QAAACBWeB6cJk2apObNm6tatWrq3r27Fi1aVORjV65cqQsuuMA9Pi4uThMnTqzUWuEPF14YvJw50+tKAAAAECs8DU4zZ87UiBEjNHbsWC1dulQdOnRQ//79tdnOclqI3bt3q2XLlrrvvvvUsGHDSq8X/jB4cPBy7lym6wEAACAGgtOECRM0fPhwDRs2TG3bttWUKVNUo0YNTZs2rdDHd+vWTQ8++KAuvvhiJSUlVXq98AebqteunbR/vzR7ttfVAAAAIBZ4Fpz27dunJUuWqG/fvgeKiY931xcuXFhuXycrK0uZmZn5NkTPqBPT9QAAABDVwWnr1q3Kzs5WgwYN8t1u19PT08vt64wfP14pKSm5W9OmTcvtteF9cJo3z36WvK4GAAAA0c7z5hAVbdSoUcrIyMjd0tLSvC4J5eCYY6ROnYLd9f77X6+rAQAAQLTzLDjVq1dPCQkJ2rRpU77b7Xp5Nn6wtVDJycn5NkSHiy8OXr7wgteVAAAAINp5FpwSExPVpUsXzZ8/P/e2nJwcd71Hjx5elYUIcumlti5O+ugjac0ar6sBAABANPN0qp61Ip86daqeffZZrVq1Stdee6127drluuyZIUOGuKl2eRtKLFu2zG22v3HjRre/hk/NMenII6V+/YL706d7XQ0AAACiWVwgEAh4WcDjjz/uWoxbQ4iOHTvq0UcfdSfCNX369HEnu53+/z8Vr127Vi1atCjwGr1799b7779foq9nXfWsSYStd2LaXuR7+WXpooukJk2kdeukhASvKwIAAECkKE028Dw4VTaCU3TJypIaN5a2bZPmzJH69/e6IgAAAERjNoj6rnqIbnYeZFvrZJ55xutqAAAAEK0IToh4V14ZvJw1S9qyxetqAAAAEI0IToh4dj6nbt2seYg0darX1QAAACAaEZwQFW68MXg5ebK0f7/X1QAAACDaEJwQFayzXv360oYN0uzZXlcDAACAaENwQtQ0ifjTn4L7jz3mdTUAAACINgQnRI1rrpGqVJE++kj68kuvqwEAAEA0ITghatj5nP7wh+D+/fd7XQ0AAACiCcEJUWXkyODlSy9JqaleVwMAAIBoQXBCVGnfXjr3XCkQkO67z+tqAAAAEC0ITog6d94ZvHz+eWntWq+rAQAAQDQgOCHqdO8u9e0r/fYbo04AAAAoHwQnRKXRo4OXTz8trV7tdTUAAACIdAQnRKVTT5XOOis46nTHHV5XAwAAgEhHcELUsml6cXHSK69In3/udTUAAACIZAQnRK0TTpCGDg3u/+UvwU57AAAAQFkQnBDVxo2TqlWTPvpImjnT62oAAAAQqQhOiGpNm0qjRgX3b7lFysjwuiIAAABEIoITot7tt0vHHiulp0t33eV1NQAAAIhEBCdEvaQk6V//Cu7b5RdfeF0RAAAAIg3BCTHhtNOkSy+VcnKkIUOk3bu9rggAAACRhOCEmPHoo1KjRtK330ojR3pdDQAAACIJwQkxo25d6ZlngvuPPSa9+67XFQEAACBSEJwQU/r3l66/Prh/+eXSxo1eVwQAAIBIQHBCzHngAalDB2nzZumCC6SsLK8rAgAAgN8RnBBzatSQXn1VqlVL+vxz6c9/9roiAAAA+B3BCTGpZUvpP/+R4uKkJ5+UJkzwuiIAAAD4GcEJMevMM6X77w/u33qr9OKLXlcEAAAAvyI4IabddtuBqXpDh0pz53pdEQAAAPyI4ISYZlP1HnlEuugiaf9+aeBAac4cr6sCAACA3xCcEPPi46XnnpMGDQp22LPw9NZbXlcFAAAAPyE4AZISE6WXXpLOP1/aty8Ynp5+2uuqAAAA4BcEJ+D/q1pVmjEjeGLc7Gzpqquku+6ScnK8rgwAAABeIzgBB4WnZ5+VRo8OXv/736UBA6Tt272uDAAAAF4iOAGFNIwYN06aPl1KSgqud+rSRfriC68rAwAAgFcITkARrD35woVSixbSjz9KPXpIY8cGu+8BAAAgthCcgGJ06iQtWSINHhxc92QjUd26SZ995nVlAAAAqEwEJyCM2rWDTSNsq1NH+uor6eSTpauvljZt8ro6AAAAVAaCE1BCNuq0alVwCl8gIE2dKrVqFRyF+vVXr6sDAABARSI4AaVQv36wacSHH0pdu0o7dwbXPR11lHTHHVJ6utcVAgAAoCIQnIAy6NVL+vxzaeZMqXVraccOafx4qVmz4Pmfvv3W6woBAABQnghOQBnFx0sXXSR98400e3Zw3dO+fdLTT0vHHRcMV089JWVkeF0pAAAADhXBCSiHADVwoPTJJ9LHHwf37VxQtj98uNSwoXTxxdLrr0u7d3tdLQAAAMoiLhCwZe6xIzMzUykpKcrIyFBycrLX5SBKbdwovfCC9OyzwRGpkOrVpdNOk845J7g1aeJllQAAALEtsxTZgOAEVCD77Vq6VPr3v6VZs6S0tPz3t20rnXqq1Lt38LJxY68qBQAAiD2ZBKeiEZzgFftNW75cevPN4GYn0T34t69lS+nEE6UuXYKbnYC3Vi2vKgYAAIhumQSnohGc4BdbtwbXQX3wQbC9+bJlUk5OwcdZmLJmE23aBLfQft26XlQNAAAQPQhOxSA4wa+s+56NQi1ZcmBbt67oxx9xhHTsscEW6Hk3O6eUXR52WGVWDwAAEHkITsUgOCHSRqVWrAieF2rVquClbevXh39unTpSgwbBzU7cG9o/+LqNXB1+eLATIAAAQCzJJDgVjeCEaLBrl5SaKq1ZExyVOnjLzCx9S3VbS1W7dvAy775d2q+KhSsbxQpd5t3Pe5t1DiSEAQCAaMsGVeQDkyZN0oMPPqj09HR16NBBjz32mE60FfJFePnllzV69GitXbtWxxxzjO6//36dddZZlVoz4CULKJ07B7eipv1ZB7/Nm6VNmw5shV3Pygqurdq2LbgdKgtNFp6qVZOSkkp3mZgoVakiVa1ausuibrNAaFtCQuH7xd1Xkn3bAABAbPA8OM2cOVMjRozQlClT1L17d02cOFH9+/dXamqq6tt8ooN8+umnuuSSSzR+/Hidc845+s9//qNBgwZp6dKlateunSffA+A3KSnBLRwbb967V9q+XdqxI7jl3Q9dtyBmo1y27dxZcN8u9+w58Jp2ot9YOdlvYYHKwuPBlyXZj/XHhjaT97Ko/XD3l+dr+elrlYdofZ3yfK1ofR2Ex3tdec4+O/iH00jh+VQ9C0vdunXT448/7q7n5OSoadOmuvHGGzVy5MgCjx88eLB27dqlN62f8/930kknqWPHji58hcNUPaBiZGcHw5KFKAtjttloVmku9+2TfvtN2r//0C+tHhtJC10Wt1/YdQAAULF+/llq2NDbGiJmqt6+ffu0ZMkSjRo1Kve2+Ph49e3bVwsXLiz0OXa7jVDlZSNUs2fPLvTxWVlZbsv75gAofzbiUrNmcIt09uck20oavA7ez/v8g/cLu620++XxGn59vdD7n/c4FLYf7v7yfC0/fa1DEcnPj+Ta/fB8lBzvdeWqWlURxdPgtHXrVmVnZ6uBtfbKw65/a63DCmHroAp7vN1eGJvSd88995Rj1QCiXd5pZLZWCgAAIOqXNttolg29hbY0WzEPAAAAAKXg6d9S69Wrp4SEBG2y9l552PWGRUx4tNtL8/ikpCS3AQAAAEBEjjglJiaqS5cumj9/fu5t1hzCrvfo0aPQ59jteR9v3nvvvSIfDwAAAACHyvPZ+9boYejQoeratas7d5O1I7euecOGDXP3DxkyRE2aNHFrlcxNN92k3r176+GHH9bZZ5+tGTNmaPHixXryySc9/k4AAAAARCvPg5O1F9+yZYvGjBnjGjxYW/E5c+bkNoBYv36967QXcvLJJ7tzN911112644473AlwraMe53ACAAAAELXncapsnMcJAAAAQGmzQdR31QMAAACAQ0VwAgAAAIAwCE4AAAAAEAbBCQAAAADCIDgBAAAAQBgEJwAAAAAIg+AEAAAAAGEQnAAAAAAgDIITAAAAAIRBcAIAAACAMAhOAAAAABAGwQkAAAAAwqiiGBMIBNxlZmam16UAAAAA8FAoE4QyQnFiLjj9+uuv7rJp06ZelwIAAADAJxkhJSWl2MfEBUoSr6JITk6OfvrpJ9WsWVNxcXGeJVsLbmlpaUpOTvakBpQ/jmt04rhGJ45r9OGYRieOa3TK9NFxtShkoalx48aKjy9+FVPMjTjZG3LkkUfKD+wHxesfFpQ/jmt04rhGJ45r9OGYRieOa3RK9slxDTfSFEJzCAAAAAAIg+AEAAAAAGEQnDyQlJSksWPHuktED45rdOK4RieOa/ThmEYnjmt0SorQ4xpzzSEAAAAAoLQYcQIAAACAMAhOAAAAABAGwQkAAAAAwiA4AQAAAEAYBCcPTJo0Sc2bN1e1atXUvXt3LVq0yOuSUEJ333234uLi8m1t2rTJvX/v3r26/vrrVbduXR1++OG64IILtGnTJk9rRkEffvihzj33XHeWcDuGs2fPzne/9cwZM2aMGjVqpOrVq6tv375avXp1vsds27ZNl112mTtxX61atfTHP/5RO3furOTvBKU5rldccUWB398zzjgj32M4rv4yfvx4devWTTVr1lT9+vU1aNAgpaam5ntMSf7dXb9+vc4++2zVqFHDvc5f/vIX/fbbb5X83aA0x7VPnz4Ffl+vueaafI/huPrL5MmT1b59+9yT2vbo0UPvvPNOVP2uEpwq2cyZMzVixAjXgnHp0qXq0KGD+vfvr82bN3tdGkro+OOP188//5y7ffzxx7n33XLLLXrjjTf08ssv64MPPtBPP/2k888/39N6UdCuXbvc7579EaMwDzzwgB599FFNmTJFn3/+uQ477DD3e2r/6IfYh+uVK1fqvffe05tvvuk+tF999dWV+F2gtMfVWFDK+/v74osv5ruf4+ov9u+ofdD67LPP3DHZv3+/+vXr5451Sf/dzc7Odh/E9u3bp08//VTPPvuspk+f7v44Av8eVzN8+PB8v6/2b3MIx9V/jjzySN13331asmSJFi9erN///vcaOHCg+zc1an5XrR05Ks+JJ54YuP7663OvZ2dnBxo3bhwYP368p3WhZMaOHRvo0KFDofft2LEjULVq1cDLL7+ce9uqVaus3X9g4cKFlVglSsOOz6xZs3Kv5+TkBBo2bBh48MEH8x3bpKSkwIsvvuiuf/PNN+55X3zxRe5j3nnnnUBcXFxg48aNlfwdoCTH1QwdOjQwcODAIp/DcfW/zZs3u2P0wQcflPjf3bfffjsQHx8fSE9Pz33M5MmTA8nJyYGsrCwPvguEO66md+/egZtuuqnI53BcI0Pt2rUDTz31VNT8rjLiVIksQVsKt2k/IfHx8e76woULPa0NJWdTtmwqUMuWLd1fp21Y2dixtb+a5T2+No3vqKOO4vhGkB9//FHp6en5jmNKSoqbVhs6jnZp07i6du2a+xh7vP0+2wgV/Ov999930z9at26ta6+9Vr/88kvufRxX/8vIyHCXderUKfG/u3Z5wgknqEGDBrmPsRHkzMzM3L+Ew1/HNeSFF15QvXr11K5dO40aNUq7d+/OvY/j6m/Z2dmaMWOGG0W0KXvR8rtaxesCYsnWrVvdD1LeHwhj17/99lvP6kLJ2YdnGza2D102beCee+5Rr169tGLFCvdhOzEx0X3wOvj42n2IDKFjVdjvaeg+u7QP33lVqVLF/U+fY+1fNk3PpoW0aNFC33//ve644w6deeaZ7n/WCQkJHFefy8nJ0c0336yePXu6D9KmJP/u2mVhv8+h++C/42ouvfRSNWvWzP2h8uuvv9btt9/u1kG9+uqr7n6Oqz8tX77cBSWb2m7rmGbNmqW2bdtq2bJlUfG7SnACSsE+ZIXYAkgLUvYP+0svveSaCADwr4svvjh33/6qab/DRx99tBuFOu200zytDeHZmhj7I1XedaWI3uOad22h/b5asx77PbU/etjvLfypdevWLiTZKOIrr7yioUOHuvVM0YKpepXIhpvtr5oHdxCx6w0bNvSsLpSd/eXk2GOP1Zo1a9wxtOmYO3bsyPcYjm9kCR2r4n5P7fLghi7W9cc6snGsI4dNt7V/l+3313Bc/euGG25wzToWLFjgFqCHlOTfXbss7Pc5dB/8d1wLY3+oNHl/Xzmu/pOYmKhWrVqpS5curnuiNez55z//GTW/qwSnSv5hsh+k+fPn5xuitus2rInIY22K7a9f9pcwO7ZVq1bNd3xtWoGtgeL4Rg6bxmX/QOc9jja/2ta4hI6jXdo//jZnO+R///uf+30O/c8d/rdhwwa3xsl+fw3H1X+sz4d9uLbpPnYs7Pczr5L8u2uXNn0obyi2Tm7WLtmmEMF/x7UwNoph8v6+clz9LycnR1lZWdHzu+p1d4pYM2PGDNeda/r06a6D09VXXx2oVatWvg4i8K9bb7018P777wd+/PHHwCeffBLo27dvoF69eq4jkLnmmmsCRx11VOB///tfYPHixYEePXq4Df7y66+/Br788ku32T+DEyZMcPvr1q1z9993333u9/K1114LfP31164TW4sWLQJ79uzJfY0zzjgj0KlTp8Dnn38e+PjjjwPHHHNM4JJLLvHwu0Jxx9Xuu+2221z3Jvv9nTdvXqBz587uuO3duzf3NTiu/nLttdcGUlJS3L+7P//8c+62e/fu3MeE+3f3t99+C7Rr1y7Qr1+/wLJlywJz5swJHHHEEYFRo0Z59F0h3HFds2ZNYNy4ce542u+r/VvcsmXLwKmnnpr7GhxX/xk5cqTrjGjHzP7fadetK+m7774bNb+rBCcPPPbYY+4HJzEx0bUn/+yzz7wuCSU0ePDgQKNGjdyxa9Kkibtu/8CH2Afr6667zrXfrFGjRuC8885z/zOAvyxYsMB9sD54s3bVoZbko0ePDjRo0MD9oeO0004LpKam5nuNX375xX2gPvzww12r1GHDhrkP5/DncbUPZPY/Y/ufsLXEbdasWWD48OEF/mjFcfWXwo6nbc8880yp/t1du3Zt4MwzzwxUr17d/bHL/gi2f/9+D74jlOS4rl+/3oWkOnXquH+DW7VqFfjLX/4SyMjIyPc6HFd/ufLKK92/rfYZyf6ttf93hkJTtPyuxtl/vB71AgAAAAA/Y40TAAAAAIRBcAIAAACAMAhOAAAAABAGwQkAAAAAwiA4AQAAAEAYBCcAAAAACIPgBAAAAABhEJwAAAAAIAyCEwAAxYiLi9Ps2bO9LgMA4DGCEwDAt6644goXXA7ezjjjDK9LAwDEmCpeFwAAQHEsJD3zzDP5bktKSvKsHgBAbGLECQDgaxaSGjZsmG+rXbu2u89GnyZPnqwzzzxT1atXV8uWLfXKK6/ke/7y5cv1+9//3t1ft25dXX311dq5c2e+x0ybNk3HH3+8+1qNGjXSDTfckO/+rVu36rzzzlONGjV0zDHH6PXXX8+9b/v27brssst0xBFHuK9h9x8c9AAAkY/gBACIaKNHj9YFF1ygr776ygWYiy++WKtWrXL37dq1S/3793dB64svvtDLL7+sefPm5QtGFryuv/56F6gsZFkoatWqVb6vcc899+iiiy7S119/rbPOOst9nW3btuV+/W+++UbvvPOO+7r2evXq1avkdwEAUNHiAoFAoMK/CgAAZVzj9Pzzz6tatWr5br/jjjvcZiNO11xzjQsrISeddJI6d+6sf/3rX5o6dapuv/12paWl6bDDDnP3v/322zr33HP1008/qUGDBmrSpImGDRumv/3tb4XWYF/jrrvu0r333psbxg4//HAXlGwa4YABA1xQslErAED0Yo0TAMDXfve73+ULRqZOnTq5+z169Mh3n11ftmyZ27cRoA4dOuSGJtOzZ0/l5OQoNTXVhSILUKeddlqxNbRv3z53314rOTlZmzdvdtevvfZaN+K1dOlS9evXT4MGDdLJJ598iN81AMBvCE4AAF+zoHLw1LnyYmuSSqJq1ar5rlvgsvBlbH3VunXr3EjWe++950KYTf176KGHKqRmAIA3WOMEAIhon332WYHrxx13nNu3S1v7ZNPrQj755BPFx8erdevWqlmzppo3b6758+cfUg3WGGLo0KFuWuHEiRP15JNPHtLrAQD8hxEnAICvZWVlKT09Pd9tVapUyW3AYA0funbtqlNOOUUvvPCCFi1apKefftrdZ00cxo4d60LN3XffrS1btujGG2/U5Zdf7tY3Gbvd1knVr1/fjR79+uuvLlzZ40pizJgx6tKli+vKZ7W++eabucENABA9CE4AAF+bM2eOaxGel40Wffvtt7kd72bMmKHrrrvOPe7FF19U27Zt3X3WPnzu3Lm66aab1K1bN3fd1iNNmDAh97UsVO3du1ePPPKIbrvtNhfILrzwwhLXl5iYqFGjRmnt2rVu6l+vXr1cPQCA6EJXPQBAxLK1RrNmzXINGQAAqEiscQIAAACAMAhOAAAAABAGa5wAABGL2eYAgMrCiBMAAAAAhEFwAgAAAIAwCE4AAAAAEAbBCQAAAADCIDgBAAAAQBgEJwAAAAAIg+AEAAAAAGEQnAAAAABAxft/xClxyOssWBUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 76.19047619047619%\n",
      "Precision: 0.75\n",
      "Recall: 0.6666666666666666\n",
      "F1-Score: 0.7058823529411765\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "model.eval() \n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:                            # Iterate over each batch (i.e. one patient)\n",
    "        patient_features = data.x                       # Get features (shape: [num_nodes, in_channels])\n",
    "        patient_edges = data.edge_index                 # Get edges (shape: [2, num_edges])\n",
    "        patient_label = data.y.float()                  # Get label (shape: [1])\n",
    "\n",
    "        # Ensure correct format\n",
    "        patient_features = patient_features.float()    \n",
    "        patient_edges = patient_edges.to(torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(patient_features, patient_edges, data.batch)  # Use the batch info to aggregate across nodes\n",
    "\n",
    "        # Apply sigmoid to the output logits and get the predicted class (0 or 1)\n",
    "        pred = torch.sigmoid(output.squeeze())\n",
    "        predicted_class = (pred >= 0.5).float()                     # Threshold at 0.5 to classify as 0 or 1\n",
    "        \n",
    "        # Collect the labels and predictions for metrics\n",
    "        all_labels.append(patient_label.cpu().numpy())\n",
    "        all_predictions.append(predicted_class.cpu().numpy())\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct += (predicted_class == patient_label).sum().item()\n",
    "        total += patient_label.size(0)  # Increment by the number of samples in this batch\n",
    "\n",
    "# Accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy}%\")\n",
    "\n",
    "# Calculate Metrics\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "Test classification with clinical and image embeddings only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)             # Binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Clinical-Only Model\n",
      "Train Features:  torch.Size([84, 4864])\n",
      "Test Features:  torch.Size([21, 4864])\n",
      "Train Labels:  torch.Size([84, 1])\n",
      "Test Labels:  torch.Size([21, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usama.Khatab\\Projects\\miniconda3\\envs\\hackathon\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.8324291137534947\n",
      "Epoch 2/100, Loss: 0.8150841612368822\n",
      "Epoch 3/100, Loss: 0.6844312957532349\n",
      "Epoch 4/100, Loss: 0.5987186691989856\n",
      "Epoch 5/100, Loss: 0.6697295542274203\n",
      "Epoch 6/100, Loss: 0.5706211835723869\n",
      "Epoch 7/100, Loss: 0.5171767361905604\n",
      "Epoch 8/100, Loss: 0.5198103309098986\n",
      "Epoch 9/100, Loss: 0.5284805203163636\n",
      "Epoch 10/100, Loss: 0.472991892947383\n",
      "Epoch 11/100, Loss: 0.523902631300979\n",
      "Epoch 12/100, Loss: 0.4128730779872664\n",
      "Epoch 13/100, Loss: 0.45822909278337776\n",
      "Epoch 14/100, Loss: 0.4099242388462396\n",
      "Epoch 15/100, Loss: 0.36476224027492643\n",
      "Epoch 16/100, Loss: 0.3394659299201088\n",
      "Epoch 17/100, Loss: 0.3553986298867953\n",
      "Epoch 18/100, Loss: 0.3671609139265846\n",
      "Epoch 19/100, Loss: 0.3684795521641956\n",
      "Epoch 20/100, Loss: 0.2962379426147284\n",
      "Epoch 21/100, Loss: 0.3239615947576177\n",
      "Epoch 22/100, Loss: 0.2845884482223558\n",
      "Epoch 23/100, Loss: 0.2659846457037626\n",
      "Epoch 24/100, Loss: 0.271445686582562\n",
      "Epoch 25/100, Loss: 0.26963922978342497\n",
      "Epoch 26/100, Loss: 0.3079948402931092\n",
      "Epoch 27/100, Loss: 0.25882682442607724\n",
      "Epoch 28/100, Loss: 0.24613486402354906\n",
      "Epoch 29/100, Loss: 0.168239441503006\n",
      "Epoch 30/100, Loss: 0.21341477722478971\n",
      "Epoch 31/100, Loss: 0.24452450912487958\n",
      "Epoch 32/100, Loss: 0.24621218445692994\n",
      "Epoch 33/100, Loss: 0.15745303551189171\n",
      "Epoch 34/100, Loss: 0.16681804903278344\n",
      "Epoch 35/100, Loss: 0.18260570180736732\n",
      "Epoch 36/100, Loss: 0.2359363869652227\n",
      "Epoch 37/100, Loss: 0.19260044941264182\n",
      "Epoch 38/100, Loss: 0.1880057189653894\n",
      "Epoch 39/100, Loss: 0.18973875413029787\n",
      "Epoch 40/100, Loss: 0.18570396152624943\n",
      "Epoch 41/100, Loss: 0.1393723277536927\n",
      "Epoch 42/100, Loss: 0.16299638568951633\n",
      "Epoch 43/100, Loss: 0.1478895648420137\n",
      "Epoch 44/100, Loss: 0.1482875728204234\n",
      "Epoch 45/100, Loss: 0.13298784176897915\n",
      "Epoch 46/100, Loss: 0.1697449593158893\n",
      "Epoch 47/100, Loss: 0.1325133950278224\n",
      "Epoch 48/100, Loss: 0.11847193780485825\n",
      "Epoch 49/100, Loss: 0.10454267314821664\n",
      "Epoch 50/100, Loss: 0.14407941572035488\n",
      "Epoch 51/100, Loss: 0.07444551180742294\n",
      "Epoch 52/100, Loss: 0.12740005330823004\n",
      "Epoch 53/100, Loss: 0.11952933047802801\n",
      "Epoch 54/100, Loss: 0.09101517168466115\n",
      "Epoch 55/100, Loss: 0.09811676020515289\n",
      "Epoch 56/100, Loss: 0.07763554713388768\n",
      "Epoch 57/100, Loss: 0.1302483307892491\n",
      "Epoch 58/100, Loss: 0.06497959892885638\n",
      "Epoch 59/100, Loss: 0.0665670637952054\n",
      "Epoch 60/100, Loss: 0.10636449529054914\n",
      "Epoch 61/100, Loss: 0.09772636571362658\n",
      "Epoch 62/100, Loss: 0.22264105869786968\n",
      "Epoch 63/100, Loss: 0.10725938379224924\n",
      "Epoch 64/100, Loss: 0.10421169102890053\n",
      "Epoch 65/100, Loss: 0.06775118631596114\n",
      "Epoch 66/100, Loss: 0.07296365683822259\n",
      "Epoch 67/100, Loss: 0.11578316195344687\n",
      "Epoch 68/100, Loss: 0.159163604066786\n",
      "Epoch 69/100, Loss: 0.10280467947615216\n",
      "Epoch 70/100, Loss: 0.08520318514345232\n",
      "Epoch 71/100, Loss: 0.09241072484197418\n",
      "Epoch 72/100, Loss: 0.08903779640151122\n",
      "Epoch 73/100, Loss: 0.12370623835177394\n",
      "Epoch 74/100, Loss: 0.08312860932083554\n",
      "Epoch 75/100, Loss: 0.0761464949841118\n",
      "Epoch 76/100, Loss: 0.06250824401014181\n",
      "Epoch 77/100, Loss: 0.07035505625666684\n",
      "Epoch 78/100, Loss: 0.08619852325486409\n",
      "Epoch 79/100, Loss: 0.06456022621509593\n",
      "Epoch 80/100, Loss: 0.0937989494721645\n",
      "Epoch 81/100, Loss: 0.03173980253909343\n",
      "Epoch 82/100, Loss: 0.08397842836168765\n",
      "Epoch 83/100, Loss: 0.07709941011074821\n",
      "Epoch 84/100, Loss: 0.05926211814453148\n",
      "Epoch 85/100, Loss: 0.11062083582842436\n",
      "Epoch 86/100, Loss: 0.06898454442849006\n",
      "Epoch 87/100, Loss: 0.04582658077941001\n",
      "Epoch 88/100, Loss: 0.04988084136315126\n",
      "Epoch 89/100, Loss: 0.08521938121695806\n",
      "Epoch 90/100, Loss: 0.06403529630533929\n",
      "Epoch 91/100, Loss: 0.05449111960205635\n",
      "Epoch 92/100, Loss: 0.09694908446121497\n",
      "Epoch 93/100, Loss: 0.04004523981912412\n",
      "Epoch 94/100, Loss: 0.1168845991270473\n",
      "Epoch 95/100, Loss: 0.0638522092578292\n",
      "Epoch 96/100, Loss: 0.04088739488100165\n",
      "Epoch 97/100, Loss: 0.022954846292354652\n",
      "Epoch 98/100, Loss: 0.047937692831745995\n",
      "Epoch 99/100, Loss: 0.03111265535068893\n",
      "Epoch 100/100, Loss: 0.05029224450176274\n",
      "Clinical-Only Model - Precision: 0.7000, Recall: 0.7778, F1-Score: 0.7368\n",
      "Test Accuracy: 76.19047619047619%\n",
      "Precision: 0.7\n",
      "Recall: 0.7777777777777778\n",
      "F1-Score: 0.7368421052631579\n",
      "\n",
      "Training Image-Only Model\n",
      "Train Features:  torch.Size([84, 4608])\n",
      "Test Features:  torch.Size([21, 4608])\n",
      "Train Labels:  torch.Size([84, 1])\n",
      "Test Labels:  torch.Size([21, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usama.Khatab\\AppData\\Local\\Temp\\ipykernel_64256\\1274329738.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_features = torch.tensor(feature_set.reshape(len(feature_set), -1))\n",
      "C:\\Users\\Usama.Khatab\\AppData\\Local\\Temp\\ipykernel_64256\\1274329738.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_features = torch.tensor((test_clinical_embeddings if modality == 'Clinical' else test_image_features).reshape(len(test_labels), -1))\n",
      "c:\\Users\\Usama.Khatab\\Projects\\miniconda3\\envs\\hackathon\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.7048673292710668\n",
      "Epoch 2/100, Loss: 0.32130189674595994\n",
      "Epoch 3/100, Loss: 0.1934162536635995\n",
      "Epoch 4/100, Loss: 0.118618916343188\n",
      "Epoch 5/100, Loss: 0.08713047540125747\n",
      "Epoch 6/100, Loss: 0.06337674709391736\n",
      "Epoch 7/100, Loss: 0.051583659657765\n",
      "Epoch 8/100, Loss: 0.038345083877045126\n",
      "Epoch 9/100, Loss: 0.029274747244060217\n",
      "Epoch 10/100, Loss: 0.029932498258338974\n",
      "Epoch 11/100, Loss: 0.020258782112727032\n",
      "Epoch 12/100, Loss: 0.02002864698408216\n",
      "Epoch 13/100, Loss: 0.016516920807286322\n",
      "Epoch 14/100, Loss: 0.012359488063209158\n",
      "Epoch 15/100, Loss: 0.012862704352647554\n",
      "Epoch 16/100, Loss: 0.009352197594125755\n",
      "Epoch 17/100, Loss: 0.009865118717286913\n",
      "Epoch 18/100, Loss: 0.009268157845196714\n",
      "Epoch 19/100, Loss: 0.009574065606145831\n",
      "Epoch 20/100, Loss: 0.0067476154458528895\n",
      "Epoch 21/100, Loss: 0.004398772868322946\n",
      "Epoch 22/100, Loss: 0.0045602859621188445\n",
      "Epoch 23/100, Loss: 0.00393754162996629\n",
      "Epoch 24/100, Loss: 0.00340441452239507\n",
      "Epoch 25/100, Loss: 0.004256981832094468\n",
      "Epoch 26/100, Loss: 0.0038556375408613044\n",
      "Epoch 27/100, Loss: 0.004009861654464787\n",
      "Epoch 28/100, Loss: 0.0030871618233677942\n",
      "Epoch 29/100, Loss: 0.0022386149371630467\n",
      "Epoch 30/100, Loss: 0.0025630989108495193\n",
      "Epoch 31/100, Loss: 0.002597435998549848\n",
      "Epoch 32/100, Loss: 0.0023932482234245286\n",
      "Epoch 33/100, Loss: 0.004515484390789685\n",
      "Epoch 34/100, Loss: 0.0019162891399208178\n",
      "Epoch 35/100, Loss: 0.00226097614135835\n",
      "Epoch 36/100, Loss: 0.002116930757741405\n",
      "Epoch 37/100, Loss: 0.0021992188810112828\n",
      "Epoch 38/100, Loss: 0.001228031919708883\n",
      "Epoch 39/100, Loss: 0.0016099607362862333\n",
      "Epoch 40/100, Loss: 0.0011750764235676755\n",
      "Epoch 41/100, Loss: 0.001210037364866626\n",
      "Epoch 42/100, Loss: 0.0011383166434673815\n",
      "Epoch 43/100, Loss: 0.0010829589837807621\n",
      "Epoch 44/100, Loss: 0.0013463855846220256\n",
      "Epoch 45/100, Loss: 0.0012783989858516655\n",
      "Epoch 46/100, Loss: 0.0010811305919127161\n",
      "Epoch 47/100, Loss: 0.0009792160033807054\n",
      "Epoch 48/100, Loss: 0.0009916284101607396\n",
      "Epoch 49/100, Loss: 0.0010446273149032745\n",
      "Epoch 50/100, Loss: 0.0010128026147174837\n",
      "Epoch 51/100, Loss: 0.0013763634382095396\n",
      "Epoch 52/100, Loss: 0.0007776811757615914\n",
      "Epoch 53/100, Loss: 0.001236385435342007\n",
      "Epoch 54/100, Loss: 0.0004853317253421318\n",
      "Epoch 55/100, Loss: 0.0006248967618679065\n",
      "Epoch 56/100, Loss: 0.0008464854814638741\n",
      "Epoch 57/100, Loss: 0.0007662756141880458\n",
      "Epoch 58/100, Loss: 0.0007192759493418201\n",
      "Epoch 59/100, Loss: 0.0007348722890908123\n",
      "Epoch 60/100, Loss: 0.0009511651435440442\n",
      "Epoch 61/100, Loss: 0.000822035759656934\n",
      "Epoch 62/100, Loss: 0.0005701499088618654\n",
      "Epoch 63/100, Loss: 0.0004897990409062913\n",
      "Epoch 64/100, Loss: 0.0007801384186011871\n",
      "Epoch 65/100, Loss: 0.0006023335948189665\n",
      "Epoch 66/100, Loss: 0.0005839384580754302\n",
      "Epoch 67/100, Loss: 0.0005253610817405951\n",
      "Epoch 68/100, Loss: 0.0005317838937716466\n",
      "Epoch 69/100, Loss: 0.0010801454032557125\n",
      "Epoch 70/100, Loss: 0.0005180393331091268\n",
      "Epoch 71/100, Loss: 0.000787017582425503\n",
      "Epoch 72/100, Loss: 0.0013818475333593483\n",
      "Epoch 73/100, Loss: 0.0005481113013112397\n",
      "Epoch 74/100, Loss: 0.0004722265059880572\n",
      "Epoch 75/100, Loss: 0.0006164867022585933\n",
      "Epoch 76/100, Loss: 0.0005827351144861701\n",
      "Epoch 77/100, Loss: 0.0005140202177629016\n",
      "Epoch 78/100, Loss: 0.0003891367831340089\n",
      "Epoch 79/100, Loss: 0.0005257377061600348\n",
      "Epoch 80/100, Loss: 0.0003172795021176474\n",
      "Epoch 81/100, Loss: 0.0003661461926131302\n",
      "Epoch 82/100, Loss: 0.000260848473479043\n",
      "Epoch 83/100, Loss: 0.0003303735229713939\n",
      "Epoch 84/100, Loss: 0.0006184273207547145\n",
      "Epoch 85/100, Loss: 0.000330192866467796\n",
      "Epoch 86/100, Loss: 0.00037467090648338593\n",
      "Epoch 87/100, Loss: 0.0003281756210017626\n",
      "Epoch 88/100, Loss: 0.00037302015593625236\n",
      "Epoch 89/100, Loss: 0.00024845659816903555\n",
      "Epoch 90/100, Loss: 0.0012507755435752273\n",
      "Epoch 91/100, Loss: 0.0004405836543498306\n",
      "Epoch 92/100, Loss: 0.0006597143329759634\n",
      "Epoch 93/100, Loss: 0.00048156365944388807\n",
      "Epoch 94/100, Loss: 0.00035854056166734997\n",
      "Epoch 95/100, Loss: 0.0003506928395256332\n",
      "Epoch 96/100, Loss: 0.0006249817908334469\n",
      "Epoch 97/100, Loss: 0.00048715476281845413\n",
      "Epoch 98/100, Loss: 0.000427351225940107\n",
      "Epoch 99/100, Loss: 0.0002581955662604414\n",
      "Epoch 100/100, Loss: 0.00046754520091672546\n",
      "Image-Only Model - Precision: 0.4167, Recall: 0.5556, F1-Score: 0.4762\n",
      "Test Accuracy: 47.61904761904762%\n",
      "Precision: 0.4166666666666667\n",
      "Recall: 0.5555555555555556\n",
      "F1-Score: 0.47619047619047616\n"
     ]
    }
   ],
   "source": [
    "# Experiment: Train Clinical-only and Image-only Models\n",
    "for modality, feature_set in [('Clinical', train_clinical_embeddings), ('Image', train_image_features)]:\n",
    "    print(f\"\\nTraining {modality}-Only Model\")\n",
    "    \n",
    "    train_labels = train_labels.clone().detach().float().view(-1, 1)\n",
    "    test_labels = test_labels.clone().detach().float().view(-1, 1)\n",
    "    train_features = torch.tensor(feature_set.reshape(len(feature_set), -1))\n",
    "    test_features = torch.tensor((test_clinical_embeddings if modality == 'Clinical' else test_image_features).reshape(len(test_labels), -1))\n",
    "\n",
    "    print(\"Train Features: \", train_features.shape)\n",
    "    print(\"Test Features: \", test_features.shape)\n",
    "    print(\"Train Labels: \", train_labels.shape)\n",
    "    print(\"Test Labels: \", test_labels.shape)\n",
    "    \n",
    "    train_dataset = TensorDataset(train_features, train_labels)\n",
    "    test_dataset = TensorDataset(test_features, test_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    model = MLP(input_dim=train_features.shape[1])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=w_decay)\n",
    "    \n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(features.float())\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(output.view(-1), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_labels, all_predictions = [], []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            output = model(features.float())\n",
    "\n",
    "            pred = torch.sigmoid(output.squeeze()) >= 0.5\n",
    "            \n",
    "            all_labels.append(labels.cpu().numpy().flatten())\n",
    "            all_predictions.append(pred.cpu().numpy().flatten())\n",
    "\n",
    "            # Count correct predictions\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)  # Increment by the number of samples in this batch\n",
    "    \n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    print(f\"{modality}-Only Model - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy}%\")\n",
    "\n",
    "    # Calculate Metrics\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
