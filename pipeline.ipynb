{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, GATConv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "n_clinical = 38 \n",
    "n_image_nodes = 6*6\n",
    "n_nodes = n_clinical + n_image_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples:  84\n",
      "Test Samples:  21\n",
      "Train labels shape: torch.Size([84])\n",
      "Test labels shape: torch.Size([21])\n"
     ]
    }
   ],
   "source": [
    "# Load Ground-Truth Values\n",
    "train_labels = pd.read_csv(\"data/labels/train_labels.csv\")\n",
    "train_labels = train_labels.iloc[:, 1].tolist()                 # (n_train,)\n",
    "test_labels = pd.read_csv(\"data/labels/test_labels.csv\")\n",
    "test_labels = test_labels.iloc[:, 1].tolist()                   # (n_test,)\n",
    "\n",
    "n_train = len(train_labels) # 84\n",
    "n_test = len(test_labels)   # 21\n",
    "\n",
    "print('Training Samples: ', n_train)\n",
    "print('Test Samples: ', n_test)\n",
    "\n",
    "# Convert to tensors\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "print(\"Train labels shape:\", train_labels.shape)                # Should be (n_train,)\n",
    "print(\"Test labels shape:\", test_labels.shape)                  # Should be (n_test,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "    return (embeddings - embeddings.mean()) / (embeddings.std() + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Image Embeddings:  (84, 6, 6, 128)\n",
      "Train Clinical Embeddings:  (84, 38, 128)\n",
      "Test Image Embeddings:  (21, 6, 6, 128)\n",
      "Test Clinical Embeddings:  (21, 38, 128)\n"
     ]
    }
   ],
   "source": [
    "# Load and normalise Embeddings\n",
    "train_image_embeddings = np.load(\"data/image_data/train_image_embeddings.npy\")             # (n_train, 6, 6, embed_dim)\n",
    "train_clinical_embeddings = np.load(\"data/clinical_data/train_embeddings.npy\")          # (n_train, 38, embed_dim)\n",
    "test_image_embeddings = np.load(\"data/image_data/test_image_embeddings.npy\")               # (n_test, 6, 6, embed_dim)\n",
    "test_clinical_embeddings = np.load(\"data/clinical_data/test_embeddings.npy\")            # (n_test, 38, embed_dim)\n",
    "\n",
    "print(\"Train Image Embeddings: \", train_image_embeddings.shape)\n",
    "print(\"Train Clinical Embeddings: \", train_clinical_embeddings.shape)\n",
    "print(\"Test Image Embeddings: \",test_image_embeddings.shape)\n",
    "print(\"Test Clinical Embeddings: \", test_clinical_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Train Image Embeddings:  torch.Size([84, 36, 128])\n",
      "Combined Train Embeddings:  torch.Size([84, 74, 128])\n",
      "Reshaped Test Image Embeddings:  torch.Size([21, 36, 128])\n",
      "Combined Test Embeddings:  torch.Size([21, 74, 128])\n"
     ]
    }
   ],
   "source": [
    "# Reshape image embeddings to match size of clinical embeddings\n",
    "train_image_features = torch.tensor(train_image_embeddings.reshape(n_train, 36, embed_dim))                             # Shape: [n_train, 36, embed_dim]\n",
    "test_image_features = torch.tensor(test_image_embeddings.reshape(n_test, 36, embed_dim))                                # Shape: [n_test, 36, embed_dim]\n",
    "\n",
    "# Combine clinical and image features\n",
    "train_patient_features = torch.cat([torch.tensor(train_clinical_embeddings), train_image_features], dim=1)              # Shape: [n_train, 74, embed_dim]\n",
    "test_patient_features = torch.cat([torch.tensor(test_clinical_embeddings), test_image_features], dim=1)                 # Shape: [n_test, 74, embed_dim]\n",
    "\n",
    "print('Reshaped Train Image Embeddings: ', train_image_features.shape)\n",
    "print('Combined Train Embeddings: ', train_patient_features.shape)\n",
    "print('Reshaped Test Image Embeddings: ', test_image_features.shape)\n",
    "print('Combined Test Embeddings: ', test_patient_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patient_edges(n_clinical, n_nodes):\n",
    "    \"\"\"\n",
    "    Creates bidirectional edges between clinical nodes and image nodes.\n",
    "    Adds a self-edge to each node.\n",
    "\n",
    "    Total edges = n_nodes (self-edges) + 2 * n_clinical * n_image_nodes (bidirectional edges)\n",
    "\n",
    "    Parameters:\n",
    "    - n_clinical: number of clinical nodes (for a specific patient)\n",
    "    - n_image_nodes: number of image nodes (for a specific patient)\n",
    "    \"\"\"\n",
    "    node_ids = np.expand_dims(np.arange(n_nodes, dtype=int), 0)\n",
    "    # self-edges = preserves some features of each own node during a graph convolution\n",
    "    self_edges = np.concatenate((node_ids, node_ids), 0)\n",
    "\n",
    "    # clinical nodes\n",
    "    c_array_asc = np.expand_dims(np.arange(n_clinical), 0)\n",
    "    all_edges = self_edges[:]\n",
    "\n",
    "    for i in range(n_clinical, n_nodes):\n",
    "        # image nodes\n",
    "        i_array = np.expand_dims(np.array([i]*n_clinical), 0)\n",
    "\n",
    "        # image --> clinical\n",
    "        inter_edges_ic = np.concatenate((i_array, c_array_asc), 0)\n",
    "        # clinical --> image\n",
    "        inter_edges_ci = np.concatenate((c_array_asc, i_array), 0)\n",
    "\n",
    "        # bidirectional edges\n",
    "        inter_edges_i = np.concatenate((inter_edges_ic, inter_edges_ci), 1)\n",
    "        all_edges = np.concatenate((all_edges, inter_edges_i), 1)\n",
    "\n",
    "    return torch.tensor(all_edges, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_list(patient_features, patient_labels):\n",
    "    \"\"\"\n",
    "    Generates a sub-graph for each patient given its embeddings\n",
    "\n",
    "    Parameters:\n",
    "    - patient_features: combined clinical and image embeddings of one patient\n",
    "    - patient_labels: groud truth values\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for i in range(len(patient_labels)):\n",
    "        # Create the graph for each patient\n",
    "        patient_edges = create_patient_edges(n_clinical, n_nodes)   # Shape: [2, num_edges]\n",
    "        patient_y = patient_labels[i]                               # Target label for this patient\n",
    "\n",
    "        data = Data(x=patient_features[i], edge_index=patient_edges, y=patient_y)\n",
    "        data_list.append(data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Patients:  84\n",
      "Test Patients:  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usama.Khatab\\Projects\\miniconda3\\envs\\hackathon\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_data_list = get_data_list(train_patient_features, train_labels)\n",
    "test_data_list = get_data_list(test_patient_features, test_labels)\n",
    "\n",
    "# Batch size 1 for individual patients\n",
    "train_loader = DataLoader(train_data_list, batch_size=1, shuffle=False, num_workers=0)  \n",
    "test_loader = DataLoader(test_data_list, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Train Patients: \", len(train_loader))\n",
    "print(\"Test Patients: \", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "We define the Graph Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)          # Second GCN layer\n",
    "        self.fc = torch.nn.Linear(hidden_channels, 1)                   # Fully connected layer for binary classification\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Apply graph convolution\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Global pooling (mean) across all nodes\n",
    "        x = global_mean_pool(x, batch)  # This will aggregate node features into one scalar per graph\n",
    "        \n",
    "        # Pass the aggregated feature through a fully connected layer to get a single logit\n",
    "        x = self.fc(x)  # Output size is (batch_size, 1)\n",
    "        return x  # Output a single logit for each patient (before applying sigmoid in loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Graph Attention Network\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, heads=2, dropout=0.5):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, dropout=dropout)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, 1)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)          # Aggregate node features\n",
    "        x = self.fc(x)                          # Binary classification output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Model Parameters\n",
    "learning_rate = 0.0001\n",
    "w_decay = 5e-4\n",
    "hidden_channels = 128\n",
    "\n",
    "# Initialize Model\n",
    "model = GCN(in_channels=embed_dim, hidden_channels=hidden_channels)\n",
    "# model = GAT(in_channels=embed_dim, hidden_channels=hidden_channels)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=w_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.6890569570518675\n",
      "Epoch 2/300, Loss: 0.6819770279384795\n",
      "Epoch 3/300, Loss: 0.677782355319886\n",
      "Epoch 4/300, Loss: 0.6744857231775919\n",
      "Epoch 5/300, Loss: 0.6714065564530236\n",
      "Epoch 6/300, Loss: 0.6682449238640922\n",
      "Epoch 7/300, Loss: 0.6647981206576029\n",
      "Epoch 8/300, Loss: 0.6609586502114931\n",
      "Epoch 9/300, Loss: 0.6566638460471517\n",
      "Epoch 10/300, Loss: 0.6518827705156236\n",
      "Epoch 11/300, Loss: 0.646445666040693\n",
      "Epoch 12/300, Loss: 0.6404123583010265\n",
      "Epoch 13/300, Loss: 0.6335544830986432\n",
      "Epoch 14/300, Loss: 0.6260371846812112\n",
      "Epoch 15/300, Loss: 0.6177300385066441\n",
      "Epoch 16/300, Loss: 0.6088211855718068\n",
      "Epoch 17/300, Loss: 0.5992431048126448\n",
      "Epoch 18/300, Loss: 0.5889384663175969\n",
      "Epoch 19/300, Loss: 0.5782203871224608\n",
      "Epoch 20/300, Loss: 0.5670125658313433\n",
      "Epoch 21/300, Loss: 0.5555436788570314\n",
      "Epoch 22/300, Loss: 0.5438530160380262\n",
      "Epoch 23/300, Loss: 0.5321409585220473\n",
      "Epoch 24/300, Loss: 0.5204329759414706\n",
      "Epoch 25/300, Loss: 0.5089312120385113\n",
      "Epoch 26/300, Loss: 0.497426173694077\n",
      "Epoch 27/300, Loss: 0.48624950412306045\n",
      "Epoch 28/300, Loss: 0.4751471510334384\n",
      "Epoch 29/300, Loss: 0.4643745961199914\n",
      "Epoch 30/300, Loss: 0.45370910808976206\n",
      "Epoch 31/300, Loss: 0.4432505841721736\n",
      "Epoch 32/300, Loss: 0.4328580243246896\n",
      "Epoch 33/300, Loss: 0.42264968642432776\n",
      "Epoch 34/300, Loss: 0.4124833531677723\n",
      "Epoch 35/300, Loss: 0.40258792882031275\n",
      "Epoch 36/300, Loss: 0.39256466653508443\n",
      "Epoch 37/300, Loss: 0.3829711003095976\n",
      "Epoch 38/300, Loss: 0.3733918404627946\n",
      "Epoch 39/300, Loss: 0.36395591547313544\n",
      "Epoch 40/300, Loss: 0.3547026587129083\n",
      "Epoch 41/300, Loss: 0.34553843310901095\n",
      "Epoch 42/300, Loss: 0.33650121273517253\n",
      "Epoch 43/300, Loss: 0.32759559039203895\n",
      "Epoch 44/300, Loss: 0.31892707781489227\n",
      "Epoch 45/300, Loss: 0.31027167435968295\n",
      "Epoch 46/300, Loss: 0.30189238325138357\n",
      "Epoch 47/300, Loss: 0.2935214987457065\n",
      "Epoch 48/300, Loss: 0.2856449776196054\n",
      "Epoch 49/300, Loss: 0.2772057910277952\n",
      "Epoch 50/300, Loss: 0.2694614384381566\n",
      "Epoch 51/300, Loss: 0.2614636063157204\n",
      "Epoch 52/300, Loss: 0.25387871718876776\n",
      "Epoch 53/300, Loss: 0.24627533633056806\n",
      "Epoch 54/300, Loss: 0.23901800998248204\n",
      "Epoch 55/300, Loss: 0.2315408692943276\n",
      "Epoch 56/300, Loss: 0.22429948320773074\n",
      "Epoch 57/300, Loss: 0.2170478312895977\n",
      "Epoch 58/300, Loss: 0.20987900224697245\n",
      "Epoch 59/300, Loss: 0.2030153618072204\n",
      "Epoch 60/300, Loss: 0.19613397919082282\n",
      "Epoch 61/300, Loss: 0.18933465674455788\n",
      "Epoch 62/300, Loss: 0.18269265448959363\n",
      "Epoch 63/300, Loss: 0.1760482515793618\n",
      "Epoch 64/300, Loss: 0.16967605504812258\n",
      "Epoch 65/300, Loss: 0.1635101039091318\n",
      "Epoch 66/300, Loss: 0.15725308180134606\n",
      "Epoch 67/300, Loss: 0.1515628663709365\n",
      "Epoch 68/300, Loss: 0.14526196830359236\n",
      "Epoch 69/300, Loss: 0.13957779143194265\n",
      "Epoch 70/300, Loss: 0.13393182075560617\n",
      "Epoch 71/300, Loss: 0.1284144386600452\n",
      "Epoch 72/300, Loss: 0.12288184705455153\n",
      "Epoch 73/300, Loss: 0.11765127554350502\n",
      "Epoch 74/300, Loss: 0.11252265394945685\n",
      "Epoch 75/300, Loss: 0.10757658112037288\n",
      "Epoch 76/300, Loss: 0.10269306980019012\n",
      "Epoch 77/300, Loss: 0.09795828343192711\n",
      "Epoch 78/300, Loss: 0.09354736437236559\n",
      "Epoch 79/300, Loss: 0.089108732663606\n",
      "Epoch 80/300, Loss: 0.08491752035670938\n",
      "Epoch 81/300, Loss: 0.08094674436775524\n",
      "Epoch 82/300, Loss: 0.0769155223006562\n",
      "Epoch 83/300, Loss: 0.07326850793540036\n",
      "Epoch 84/300, Loss: 0.0695812746498659\n",
      "Epoch 85/300, Loss: 0.06625686516233709\n",
      "Epoch 86/300, Loss: 0.06292358410305524\n",
      "Epoch 87/300, Loss: 0.05980970641210314\n",
      "Epoch 88/300, Loss: 0.0567392011758619\n",
      "Epoch 89/300, Loss: 0.05398449321029104\n",
      "Epoch 90/300, Loss: 0.0512384486704767\n",
      "Epoch 91/300, Loss: 0.0485548780645143\n",
      "Epoch 92/300, Loss: 0.046118289174294044\n",
      "Epoch 93/300, Loss: 0.043785407932931876\n",
      "Epoch 94/300, Loss: 0.04151885271564297\n",
      "Epoch 95/300, Loss: 0.03939885200145928\n",
      "Epoch 96/300, Loss: 0.037457141745799206\n",
      "Epoch 97/300, Loss: 0.035514447534101545\n",
      "Epoch 98/300, Loss: 0.03374375110320994\n",
      "Epoch 99/300, Loss: 0.031982065034710364\n",
      "Epoch 100/300, Loss: 0.03043887557797315\n",
      "Epoch 101/300, Loss: 0.02884063414749884\n",
      "Epoch 102/300, Loss: 0.027474087053610995\n",
      "Epoch 103/300, Loss: 0.026029640217128586\n",
      "Epoch 104/300, Loss: 0.024813118787873394\n",
      "Epoch 105/300, Loss: 0.023572498644695693\n",
      "Epoch 106/300, Loss: 0.02250619147621926\n",
      "Epoch 107/300, Loss: 0.021365369473170876\n",
      "Epoch 108/300, Loss: 0.020403484009188063\n",
      "Epoch 109/300, Loss: 0.019432150140571074\n",
      "Epoch 110/300, Loss: 0.01859627861997001\n",
      "Epoch 111/300, Loss: 0.017665615357723548\n",
      "Epoch 112/300, Loss: 0.016944825012209074\n",
      "Epoch 113/300, Loss: 0.016159281673560785\n",
      "Epoch 114/300, Loss: 0.015500995899739729\n",
      "Epoch 115/300, Loss: 0.014795480355477932\n",
      "Epoch 116/300, Loss: 0.014215268887792206\n",
      "Epoch 117/300, Loss: 0.013591492870327486\n",
      "Epoch 118/300, Loss: 0.013081042617209332\n",
      "Epoch 119/300, Loss: 0.012526881979523199\n",
      "Epoch 120/300, Loss: 0.012074267749743703\n",
      "Epoch 121/300, Loss: 0.011594886098957804\n",
      "Epoch 122/300, Loss: 0.01119274031281052\n",
      "Epoch 123/300, Loss: 0.010762055499896965\n",
      "Epoch 124/300, Loss: 0.010403134174236158\n",
      "Epoch 125/300, Loss: 0.010034414979055703\n",
      "Epoch 126/300, Loss: 0.00971223459279544\n",
      "Epoch 127/300, Loss: 0.009367063854084009\n",
      "Epoch 128/300, Loss: 0.009085423380154807\n",
      "Epoch 129/300, Loss: 0.008792229830076775\n",
      "Epoch 130/300, Loss: 0.008542273816140065\n",
      "Epoch 131/300, Loss: 0.008273125303954151\n",
      "Epoch 132/300, Loss: 0.008051523981446792\n",
      "Epoch 133/300, Loss: 0.007812781700872915\n",
      "Epoch 134/300, Loss: 0.007618026183406118\n",
      "Epoch 135/300, Loss: 0.007407436034054001\n",
      "Epoch 136/300, Loss: 0.00723141534517754\n",
      "Epoch 137/300, Loss: 0.0070505354715539766\n",
      "Epoch 138/300, Loss: 0.006882065506618132\n",
      "Epoch 139/300, Loss: 0.006719969572294392\n",
      "Epoch 140/300, Loss: 0.0065875217576240415\n",
      "Epoch 141/300, Loss: 0.006435364629684838\n",
      "Epoch 142/300, Loss: 0.006308456292397698\n",
      "Epoch 143/300, Loss: 0.006180305110663433\n",
      "Epoch 144/300, Loss: 0.006061216484467684\n",
      "Epoch 145/300, Loss: 0.005954647513349324\n",
      "Epoch 146/300, Loss: 0.0058417268411565275\n",
      "Epoch 147/300, Loss: 0.005744571296552047\n",
      "Epoch 148/300, Loss: 0.005647348522078911\n",
      "Epoch 149/300, Loss: 0.005558271924576004\n",
      "Epoch 150/300, Loss: 0.0054652557572080155\n",
      "Epoch 151/300, Loss: 0.005386269269985448\n",
      "Epoch 152/300, Loss: 0.005306224235631535\n",
      "Epoch 153/300, Loss: 0.005227354907452493\n",
      "Epoch 154/300, Loss: 0.005151544192136455\n",
      "Epoch 155/300, Loss: 0.005092626613989502\n",
      "Epoch 156/300, Loss: 0.0050140514261307716\n",
      "Epoch 157/300, Loss: 0.0049584520033850565\n",
      "Epoch 158/300, Loss: 0.0048926684773987076\n",
      "Epoch 159/300, Loss: 0.004830072394673564\n",
      "Epoch 160/300, Loss: 0.004775115935672147\n",
      "Epoch 161/300, Loss: 0.004730951592559255\n",
      "Epoch 162/300, Loss: 0.00466041824279273\n",
      "Epoch 163/300, Loss: 0.00462453974768637\n",
      "Epoch 164/300, Loss: 0.004558596013821227\n",
      "Epoch 165/300, Loss: 0.004527462364300001\n",
      "Epoch 166/300, Loss: 0.004465327778055845\n",
      "Epoch 167/300, Loss: 0.004438824928516148\n",
      "Epoch 168/300, Loss: 0.0043799406978992225\n",
      "Epoch 169/300, Loss: 0.004341220585767284\n",
      "Epoch 170/300, Loss: 0.0042918324343353175\n",
      "Epoch 171/300, Loss: 0.004262015171927184\n",
      "Epoch 172/300, Loss: 0.0042215686086570885\n",
      "Epoch 173/300, Loss: 0.0041809785737075885\n",
      "Epoch 174/300, Loss: 0.004143266368000046\n",
      "Epoch 175/300, Loss: 0.004106172043510101\n",
      "Epoch 176/300, Loss: 0.004075140778046001\n",
      "Epoch 177/300, Loss: 0.004041008646736156\n",
      "Epoch 178/300, Loss: 0.004006536904880488\n",
      "Epoch 179/300, Loss: 0.003972881751668042\n",
      "Epoch 180/300, Loss: 0.00394314050909846\n",
      "Epoch 181/300, Loss: 0.003909682883707839\n",
      "Epoch 182/300, Loss: 0.003876133497350392\n",
      "Epoch 183/300, Loss: 0.003852981019898986\n",
      "Epoch 184/300, Loss: 0.003823658539455924\n",
      "Epoch 185/300, Loss: 0.0037909916086418674\n",
      "Epoch 186/300, Loss: 0.0037609378251745726\n",
      "Epoch 187/300, Loss: 0.003733940936293883\n",
      "Epoch 188/300, Loss: 0.0037128889890574272\n",
      "Epoch 189/300, Loss: 0.0036876947792978043\n",
      "Epoch 190/300, Loss: 0.003663185626077761\n",
      "Epoch 191/300, Loss: 0.0036325524527028178\n",
      "Epoch 192/300, Loss: 0.0036112547968433057\n",
      "Epoch 193/300, Loss: 0.003592619371886485\n",
      "Epoch 194/300, Loss: 0.0035707768816856905\n",
      "Epoch 195/300, Loss: 0.0035451619189968498\n",
      "Epoch 196/300, Loss: 0.0035287252770948335\n",
      "Epoch 197/300, Loss: 0.0035066392181956224\n",
      "Epoch 198/300, Loss: 0.0034862752667907315\n",
      "Epoch 199/300, Loss: 0.0034650972251624593\n",
      "Epoch 200/300, Loss: 0.0034574198309209286\n",
      "Epoch 201/300, Loss: 0.003435398604049548\n",
      "Epoch 202/300, Loss: 0.0034136096831543583\n",
      "Epoch 203/300, Loss: 0.003397109054586302\n",
      "Epoch 204/300, Loss: 0.0033825318822786293\n",
      "Epoch 205/300, Loss: 0.0033781199524975406\n",
      "Epoch 206/300, Loss: 0.0033609935972481794\n",
      "Epoch 207/300, Loss: 0.00334365268892882\n",
      "Epoch 208/300, Loss: 0.003327904283289789\n",
      "Epoch 209/300, Loss: 0.0033145352349560567\n",
      "Epoch 210/300, Loss: 0.003301330070112975\n",
      "Epoch 211/300, Loss: 0.0032964532510539983\n",
      "Epoch 212/300, Loss: 0.003284342996400775\n",
      "Epoch 213/300, Loss: 0.0032643371978351645\n",
      "Epoch 214/300, Loss: 0.0032567024043268183\n",
      "Epoch 215/300, Loss: 0.003242858867558934\n",
      "Epoch 216/300, Loss: 0.0032327321721679495\n",
      "Epoch 217/300, Loss: 0.0032270235523601427\n",
      "Epoch 218/300, Loss: 0.0032227644386289846\n",
      "Epoch 219/300, Loss: 0.0032059334333387163\n",
      "Epoch 220/300, Loss: 0.0031960820171966936\n",
      "Epoch 221/300, Loss: 0.0031923642479240654\n",
      "Epoch 222/300, Loss: 0.0031851160174035612\n",
      "Epoch 223/300, Loss: 0.0031773042077463437\n",
      "Epoch 224/300, Loss: 0.003162008431851369\n",
      "Epoch 225/300, Loss: 0.003158342801135493\n",
      "Epoch 226/300, Loss: 0.0031555093570702127\n",
      "Epoch 227/300, Loss: 0.0031487339143992984\n",
      "Epoch 228/300, Loss: 0.0031434856639406526\n",
      "Epoch 229/300, Loss: 0.0031307033751393105\n",
      "Epoch 230/300, Loss: 0.003127685055335862\n",
      "Epoch 231/300, Loss: 0.0031210678095453894\n",
      "Epoch 232/300, Loss: 0.0031185866627191096\n",
      "Epoch 233/300, Loss: 0.0031133102259152693\n",
      "Epoch 234/300, Loss: 0.0031200021034713137\n",
      "Epoch 235/300, Loss: 0.0031007644695228245\n",
      "Epoch 236/300, Loss: 0.0030885826261596145\n",
      "Epoch 237/300, Loss: 0.0031018032768986054\n",
      "Epoch 238/300, Loss: 0.0030892744658893314\n",
      "Epoch 239/300, Loss: 0.003081649367248032\n",
      "Epoch 240/300, Loss: 0.0030814466766091557\n",
      "Epoch 241/300, Loss: 0.0030761619837967828\n",
      "Epoch 242/300, Loss: 0.0030787623174973583\n",
      "Epoch 243/300, Loss: 0.0030686840042821843\n",
      "Epoch 244/300, Loss: 0.0030691853007845673\n",
      "Epoch 245/300, Loss: 0.0030635583994601694\n",
      "Epoch 246/300, Loss: 0.003063033134134416\n",
      "Epoch 247/300, Loss: 0.0030547045899776072\n",
      "Epoch 248/300, Loss: 0.0030566934929082954\n",
      "Epoch 249/300, Loss: 0.003050192300439104\n",
      "Epoch 250/300, Loss: 0.0030516783471733213\n",
      "Epoch 251/300, Loss: 0.003043535715816757\n",
      "Epoch 252/300, Loss: 0.0030438964299474066\n",
      "Epoch 253/300, Loss: 0.0030409898245935436\n",
      "Epoch 254/300, Loss: 0.0030434710646430304\n",
      "Epoch 255/300, Loss: 0.0030392564101040945\n",
      "Epoch 256/300, Loss: 0.003034629444116429\n",
      "Epoch 257/300, Loss: 0.0030326803899111184\n",
      "Epoch 258/300, Loss: 0.0030346345940137844\n",
      "Epoch 259/300, Loss: 0.0030240720280076856\n",
      "Epoch 260/300, Loss: 0.0030271510301963034\n",
      "Epoch 261/300, Loss: 0.00302200104286617\n",
      "Epoch 262/300, Loss: 0.0030173388109182788\n",
      "Epoch 263/300, Loss: 0.0030181248761803454\n",
      "Epoch 264/300, Loss: 0.003021054187586443\n",
      "Epoch 265/300, Loss: 0.0030172033493860865\n",
      "Epoch 266/300, Loss: 0.0030216490466617955\n",
      "Epoch 267/300, Loss: 0.003009709547124185\n",
      "Epoch 268/300, Loss: 0.003003936414128782\n",
      "Epoch 269/300, Loss: 0.003009005266860363\n",
      "Epoch 270/300, Loss: 0.003010217597398882\n",
      "Epoch 271/300, Loss: 0.003012569298410333\n",
      "Epoch 272/300, Loss: 0.003002285246403737\n",
      "Epoch 273/300, Loss: 0.0029976563927300434\n",
      "Epoch 274/300, Loss: 0.0029978767217193826\n",
      "Epoch 275/300, Loss: 0.002998253154471019\n",
      "Epoch 276/300, Loss: 0.0030012315322040756\n",
      "Epoch 277/300, Loss: 0.0029993978327523237\n",
      "Epoch 278/300, Loss: 0.0029872548579417653\n",
      "Epoch 279/300, Loss: 0.0029894591860450113\n",
      "Epoch 280/300, Loss: 0.002996909635106264\n",
      "Epoch 281/300, Loss: 0.0029924766839954826\n",
      "Epoch 282/300, Loss: 0.0029854032898587405\n",
      "Epoch 283/300, Loss: 0.002987908820649413\n",
      "Epoch 284/300, Loss: 0.002989343855772018\n",
      "Epoch 285/300, Loss: 0.0029846639453665717\n",
      "Epoch 286/300, Loss: 0.0029816048079552055\n",
      "Epoch 287/300, Loss: 0.0029781528030605317\n",
      "Epoch 288/300, Loss: 0.0029908047612900576\n",
      "Epoch 289/300, Loss: 0.0029839367078811022\n",
      "Epoch 290/300, Loss: 0.002981847997218388\n",
      "Epoch 291/300, Loss: 0.0029752136173002326\n",
      "Epoch 292/300, Loss: 0.002975222229392182\n",
      "Epoch 293/300, Loss: 0.0029741896765816903\n",
      "Epoch 294/300, Loss: 0.002977074294151346\n",
      "Epoch 295/300, Loss: 0.002972476690723726\n",
      "Epoch 296/300, Loss: 0.0029727381010988077\n",
      "Epoch 297/300, Loss: 0.0029667634492374717\n",
      "Epoch 298/300, Loss: 0.002972207493230646\n",
      "Epoch 299/300, Loss: 0.0029653661545422003\n",
      "Epoch 300/300, Loss: 0.0029644343606219556\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "train_losses = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for data in train_loader:                                               # Iterate over each batch (here, each batch is one patient)\n",
    "                                                                            # Data object contains 'x' (features), 'edge_index' (graph edges), 'y' (labels)\n",
    "        patient_features = data.x                                           # Shape: (num_nodes, in_channels)\n",
    "        patient_edges = data.edge_index                                     # Shape: (2, num_edges)\n",
    "        patient_label = data.y.float()                                      # Target label\n",
    "        batch = data.batch\n",
    "\n",
    "        # Ensure correct format\n",
    "        patient_features = patient_features.float()\n",
    "        patient_edges = patient_edges.to(torch.long)                 \n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(patient_features, patient_edges, batch)                  # Output shape: (1, 1)\n",
    "        \n",
    "        # Binary Classification Loss\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(output.view(-1), patient_label)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Print loss after each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 76.19047619047619%\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.8888888888888888\n",
      "F1-Score: 0.7619047619047619\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "model.eval() \n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:                            # Iterate over each batch (i.e. one patient)\n",
    "        patient_features = data.x                       # Get features (shape: [num_nodes, in_channels])\n",
    "        patient_edges = data.edge_index                 # Get edges (shape: [2, num_edges])\n",
    "        patient_label = data.y.float()                  # Get label (shape: [1])\n",
    "\n",
    "        # Ensure correct format\n",
    "        patient_features = patient_features.float()    \n",
    "        patient_edges = patient_edges.to(torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(patient_features, patient_edges, data.batch)  # Use the batch info to aggregate across nodes\n",
    "\n",
    "        # Apply sigmoid to the output logits and get the predicted class (0 or 1)\n",
    "        pred = torch.sigmoid(output.squeeze())\n",
    "        predicted_class = (pred >= 0.5).float()                     # Threshold at 0.5 to classify as 0 or 1\n",
    "        \n",
    "        # Collect the labels and predictions for metrics\n",
    "        all_labels.append(patient_label.cpu().numpy())\n",
    "        all_predictions.append(predicted_class.cpu().numpy())\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct += (predicted_class == patient_label).sum().item()\n",
    "        total += patient_label.size(0)  # Increment by the number of samples in this batch\n",
    "\n",
    "# Accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy}%\")\n",
    "\n",
    "# Calculate Metrics\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "Test classification with clinical and image embeddings only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)             # Binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Clinical-Only Model\n",
      "Train Features:  torch.Size([84, 4864])\n",
      "Test Features:  torch.Size([21, 4864])\n",
      "Train Labels:  torch.Size([84, 1])\n",
      "Test Labels:  torch.Size([21, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usama.Khatab\\Projects\\miniconda3\\envs\\hackathon\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.7910627390568455\n",
      "Epoch 2/100, Loss: 0.7529586512329323\n",
      "Epoch 3/100, Loss: 0.7197760065041837\n",
      "Epoch 4/100, Loss: 0.6130474964433926\n",
      "Epoch 5/100, Loss: 0.6297825403723566\n",
      "Epoch 6/100, Loss: 0.5476658287280727\n",
      "Epoch 7/100, Loss: 0.5549302352245875\n",
      "Epoch 8/100, Loss: 0.5387516185167313\n",
      "Epoch 9/100, Loss: 0.48829233026065466\n",
      "Epoch 10/100, Loss: 0.4998331777799952\n",
      "Epoch 11/100, Loss: 0.4973229716547177\n",
      "Epoch 12/100, Loss: 0.4614590999037665\n",
      "Epoch 13/100, Loss: 0.474405270560445\n",
      "Epoch 14/100, Loss: 0.4574277028683961\n",
      "Epoch 15/100, Loss: 0.42421762190393303\n",
      "Epoch 16/100, Loss: 0.3948086916529324\n",
      "Epoch 17/100, Loss: 0.3850413062876517\n",
      "Epoch 18/100, Loss: 0.4029987124550167\n",
      "Epoch 19/100, Loss: 0.3269506291010094\n",
      "Epoch 20/100, Loss: 0.32763435646692024\n",
      "Epoch 21/100, Loss: 0.3142760954066207\n",
      "Epoch 22/100, Loss: 0.3104773474710999\n",
      "Epoch 23/100, Loss: 0.29906982385355585\n",
      "Epoch 24/100, Loss: 0.24632630353549498\n",
      "Epoch 25/100, Loss: 0.35854381778725175\n",
      "Epoch 26/100, Loss: 0.2861301110935005\n",
      "Epoch 27/100, Loss: 0.26020564158330034\n",
      "Epoch 28/100, Loss: 0.2414607038250927\n",
      "Epoch 29/100, Loss: 0.2604599223472519\n",
      "Epoch 30/100, Loss: 0.27030233468282117\n",
      "Epoch 31/100, Loss: 0.26086629521038657\n",
      "Epoch 32/100, Loss: 0.22838575069770395\n",
      "Epoch 33/100, Loss: 0.20174809303629868\n",
      "Epoch 34/100, Loss: 0.2177057676104769\n",
      "Epoch 35/100, Loss: 0.20929263098934467\n",
      "Epoch 36/100, Loss: 0.1998989800919514\n",
      "Epoch 37/100, Loss: 0.19770175839038356\n",
      "Epoch 38/100, Loss: 0.1813207714464213\n",
      "Epoch 39/100, Loss: 0.15256253465501382\n",
      "Epoch 40/100, Loss: 0.16955811127986853\n",
      "Epoch 41/100, Loss: 0.17935227896949782\n",
      "Epoch 42/100, Loss: 0.1493005351853888\n",
      "Epoch 43/100, Loss: 0.19569246369903767\n",
      "Epoch 44/100, Loss: 0.15395310974699056\n",
      "Epoch 45/100, Loss: 0.1353656245910904\n",
      "Epoch 46/100, Loss: 0.11673624800177591\n",
      "Epoch 47/100, Loss: 0.12778237598759223\n",
      "Epoch 48/100, Loss: 0.1451845099519405\n",
      "Epoch 49/100, Loss: 0.1781472602334476\n",
      "Epoch 50/100, Loss: 0.1505486837183642\n",
      "Epoch 51/100, Loss: 0.1136006041191476\n",
      "Epoch 52/100, Loss: 0.09925164611358292\n",
      "Epoch 53/100, Loss: 0.09407937581578164\n",
      "Epoch 54/100, Loss: 0.13747719779247997\n",
      "Epoch 55/100, Loss: 0.13941296062964847\n",
      "Epoch 56/100, Loss: 0.07171169772618119\n",
      "Epoch 57/100, Loss: 0.08875278361762726\n",
      "Epoch 58/100, Loss: 0.07477088763316862\n",
      "Epoch 59/100, Loss: 0.06961209721277076\n",
      "Epoch 60/100, Loss: 0.14712291296810268\n",
      "Epoch 61/100, Loss: 0.16176665317657127\n",
      "Epoch 62/100, Loss: 0.07698830530809277\n",
      "Epoch 63/100, Loss: 0.09678674066332706\n",
      "Epoch 64/100, Loss: 0.09280217577039206\n",
      "Epoch 65/100, Loss: 0.12468127099102892\n",
      "Epoch 66/100, Loss: 0.07075853549526928\n",
      "Epoch 67/100, Loss: 0.09754859744870435\n",
      "Epoch 68/100, Loss: 0.07627081601038893\n",
      "Epoch 69/100, Loss: 0.041690500752347405\n",
      "Epoch 70/100, Loss: 0.08396534904999249\n",
      "Epoch 71/100, Loss: 0.08463173739480248\n",
      "Epoch 72/100, Loss: 0.08457812694939955\n",
      "Epoch 73/100, Loss: 0.0536411013870785\n",
      "Epoch 74/100, Loss: 0.06514980336746727\n",
      "Epoch 75/100, Loss: 0.0695759737808349\n",
      "Epoch 76/100, Loss: 0.07286239768248078\n",
      "Epoch 77/100, Loss: 0.04783140542039285\n",
      "Epoch 78/100, Loss: 0.051854761183676094\n",
      "Epoch 79/100, Loss: 0.03512859773165695\n",
      "Epoch 80/100, Loss: 0.07377806049855755\n",
      "Epoch 81/100, Loss: 0.05623874313072101\n",
      "Epoch 82/100, Loss: 0.03765639825250505\n",
      "Epoch 83/100, Loss: 0.05076160363716399\n",
      "Epoch 84/100, Loss: 0.09936702735147189\n",
      "Epoch 85/100, Loss: 0.05594170603190461\n",
      "Epoch 86/100, Loss: 0.07959140108816835\n",
      "Epoch 87/100, Loss: 0.07399402566923023\n",
      "Epoch 88/100, Loss: 0.04446763062942084\n",
      "Epoch 89/100, Loss: 0.07419057186667652\n",
      "Epoch 90/100, Loss: 0.051014355077332194\n",
      "Epoch 91/100, Loss: 0.05021093270983059\n",
      "Epoch 92/100, Loss: 0.060259732113868904\n",
      "Epoch 93/100, Loss: 0.0366695215253027\n",
      "Epoch 94/100, Loss: 0.05049784857768957\n",
      "Epoch 95/100, Loss: 0.04152478263572818\n",
      "Epoch 96/100, Loss: 0.04469867364125115\n",
      "Epoch 97/100, Loss: 0.07527610239540912\n",
      "Epoch 98/100, Loss: 0.023583127703723675\n",
      "Epoch 99/100, Loss: 0.029342897523424057\n",
      "Epoch 100/100, Loss: 0.03753187210149143\n",
      "Clinical-Only Model - Precision: 0.7000, Recall: 0.7778, F1-Score: 0.7368\n",
      "Test Accuracy: 76.19047619047619%\n",
      "Precision: 0.7\n",
      "Recall: 0.7777777777777778\n",
      "F1-Score: 0.7368421052631579\n",
      "\n",
      "Training Image-Only Model\n",
      "Train Features:  torch.Size([84, 4608])\n",
      "Test Features:  torch.Size([21, 4608])\n",
      "Train Labels:  torch.Size([84, 1])\n",
      "Test Labels:  torch.Size([21, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usama.Khatab\\AppData\\Local\\Temp\\ipykernel_40204\\1274329738.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_features = torch.tensor(feature_set.reshape(len(feature_set), -1))\n",
      "C:\\Users\\Usama.Khatab\\AppData\\Local\\Temp\\ipykernel_40204\\1274329738.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_features = torch.tensor((test_clinical_embeddings if modality == 'Clinical' else test_image_features).reshape(len(test_labels), -1))\n",
      "c:\\Users\\Usama.Khatab\\Projects\\miniconda3\\envs\\hackathon\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.6844181631292615\n",
      "Epoch 2/100, Loss: 0.6978751811243239\n",
      "Epoch 3/100, Loss: 0.6658809972660882\n",
      "Epoch 4/100, Loss: 0.6978658402249927\n",
      "Epoch 5/100, Loss: 0.683133316891534\n",
      "Epoch 6/100, Loss: 0.6964780308661007\n",
      "Epoch 7/100, Loss: 0.688498202533949\n",
      "Epoch 8/100, Loss: 0.6976524861085982\n",
      "Epoch 9/100, Loss: 0.6654295885846728\n",
      "Epoch 10/100, Loss: 0.660698455004465\n",
      "Epoch 11/100, Loss: 0.6758858941140629\n",
      "Epoch 12/100, Loss: 0.6890053376555443\n",
      "Epoch 13/100, Loss: 0.6803330016278085\n",
      "Epoch 14/100, Loss: 0.6930841062040556\n",
      "Epoch 15/100, Loss: 0.6732852895345006\n",
      "Epoch 16/100, Loss: 0.6874918717713583\n",
      "Epoch 17/100, Loss: 0.6814372025075413\n",
      "Epoch 18/100, Loss: 0.6793521848462877\n",
      "Epoch 19/100, Loss: 0.6745474388202032\n",
      "Epoch 20/100, Loss: 0.6676472758962995\n",
      "Epoch 21/100, Loss: 0.6588651580469949\n",
      "Epoch 22/100, Loss: 0.6690923670927683\n",
      "Epoch 23/100, Loss: 0.6630231901293709\n",
      "Epoch 24/100, Loss: 0.6473863426418531\n",
      "Epoch 25/100, Loss: 0.6651099292295319\n",
      "Epoch 26/100, Loss: 0.6480672121757552\n",
      "Epoch 27/100, Loss: 0.6484574570897079\n",
      "Epoch 28/100, Loss: 0.6426028495743161\n",
      "Epoch 29/100, Loss: 0.6693051223243985\n",
      "Epoch 30/100, Loss: 0.6636747221151987\n",
      "Epoch 31/100, Loss: 0.6336027131903739\n",
      "Epoch 32/100, Loss: 0.6581196196022487\n",
      "Epoch 33/100, Loss: 0.6661404438671612\n",
      "Epoch 34/100, Loss: 0.6572289349777358\n",
      "Epoch 35/100, Loss: 0.6510839919958796\n",
      "Epoch 36/100, Loss: 0.6480705028488523\n",
      "Epoch 37/100, Loss: 0.616195077697436\n",
      "Epoch 38/100, Loss: 0.6433440058359078\n",
      "Epoch 39/100, Loss: 0.6186332500406674\n",
      "Epoch 40/100, Loss: 0.645516309000197\n",
      "Epoch 41/100, Loss: 0.6578625859249205\n",
      "Epoch 42/100, Loss: 0.666003310254642\n",
      "Epoch 43/100, Loss: 0.6311576059531598\n",
      "Epoch 44/100, Loss: 0.6427690303396612\n",
      "Epoch 45/100, Loss: 0.5965377943856376\n",
      "Epoch 46/100, Loss: 0.6385253772494339\n",
      "Epoch 47/100, Loss: 0.6179368534968013\n",
      "Epoch 48/100, Loss: 0.6208359409301054\n",
      "Epoch 49/100, Loss: 0.6509029904291743\n",
      "Epoch 50/100, Loss: 0.6273836699270067\n",
      "Epoch 51/100, Loss: 0.6298184380644843\n",
      "Epoch 52/100, Loss: 0.626353600195476\n",
      "Epoch 53/100, Loss: 0.6208141454983325\n",
      "Epoch 54/100, Loss: 0.6042062590519587\n",
      "Epoch 55/100, Loss: 0.6049533225595951\n",
      "Epoch 56/100, Loss: 0.6209512344073682\n",
      "Epoch 57/100, Loss: 0.6273316964507103\n",
      "Epoch 58/100, Loss: 0.6114026840244021\n",
      "Epoch 59/100, Loss: 0.6265677286400682\n",
      "Epoch 60/100, Loss: 0.627491681614802\n",
      "Epoch 61/100, Loss: 0.6014401799156552\n",
      "Epoch 62/100, Loss: 0.5979837816031206\n",
      "Epoch 63/100, Loss: 0.6232086877737727\n",
      "Epoch 64/100, Loss: 0.6180968416766042\n",
      "Epoch 65/100, Loss: 0.6019202687201046\n",
      "Epoch 66/100, Loss: 0.5845517798193863\n",
      "Epoch 67/100, Loss: 0.5988488833286932\n",
      "Epoch 68/100, Loss: 0.608516540821819\n",
      "Epoch 69/100, Loss: 0.5918393229090032\n",
      "Epoch 70/100, Loss: 0.569862255028316\n",
      "Epoch 71/100, Loss: 0.6121663403298173\n",
      "Epoch 72/100, Loss: 0.5935823018440888\n",
      "Epoch 73/100, Loss: 0.602714770606586\n",
      "Epoch 74/100, Loss: 0.5750998270121359\n",
      "Epoch 75/100, Loss: 0.5710282596271663\n",
      "Epoch 76/100, Loss: 0.5816518601828388\n",
      "Epoch 77/100, Loss: 0.5787903016344422\n",
      "Epoch 78/100, Loss: 0.5654676289608082\n",
      "Epoch 79/100, Loss: 0.5852860210552102\n",
      "Epoch 80/100, Loss: 0.56942502604354\n",
      "Epoch 81/100, Loss: 0.5751182080379554\n",
      "Epoch 82/100, Loss: 0.5800186734469164\n",
      "Epoch 83/100, Loss: 0.596296543166751\n",
      "Epoch 84/100, Loss: 0.5985792514291548\n",
      "Epoch 85/100, Loss: 0.5898191556334496\n",
      "Epoch 86/100, Loss: 0.563375268929771\n",
      "Epoch 87/100, Loss: 0.5952552635932252\n",
      "Epoch 88/100, Loss: 0.5632174105516502\n",
      "Epoch 89/100, Loss: 0.5580218099501162\n",
      "Epoch 90/100, Loss: 0.5552069068487201\n",
      "Epoch 91/100, Loss: 0.5802752841707497\n",
      "Epoch 92/100, Loss: 0.572829021140933\n",
      "Epoch 93/100, Loss: 0.570113086718179\n",
      "Epoch 94/100, Loss: 0.5558334359278282\n",
      "Epoch 95/100, Loss: 0.5404054773971438\n",
      "Epoch 96/100, Loss: 0.554457463900603\n",
      "Epoch 97/100, Loss: 0.567115155536504\n",
      "Epoch 98/100, Loss: 0.5542708467470393\n",
      "Epoch 99/100, Loss: 0.576179957239046\n",
      "Epoch 100/100, Loss: 0.5524785657901139\n",
      "Image-Only Model - Precision: 0.6000, Recall: 0.3333, F1-Score: 0.4286\n",
      "Test Accuracy: 61.904761904761905%\n",
      "Precision: 0.6\n",
      "Recall: 0.3333333333333333\n",
      "F1-Score: 0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "# Experiment: Train Clinical-only and Image-only Models\n",
    "for modality, feature_set in [('Clinical', train_clinical_embeddings), ('Image', train_image_features)]:\n",
    "    print(f\"\\nTraining {modality}-Only Model\")\n",
    "    \n",
    "    train_labels = train_labels.clone().detach().float().view(-1, 1)\n",
    "    test_labels = test_labels.clone().detach().float().view(-1, 1)\n",
    "    train_features = torch.tensor(feature_set.reshape(len(feature_set), -1))\n",
    "    test_features = torch.tensor((test_clinical_embeddings if modality == 'Clinical' else test_image_features).reshape(len(test_labels), -1))\n",
    "\n",
    "    print(\"Train Features: \", train_features.shape)\n",
    "    print(\"Test Features: \", test_features.shape)\n",
    "    print(\"Train Labels: \", train_labels.shape)\n",
    "    print(\"Test Labels: \", test_labels.shape)\n",
    "    \n",
    "    train_dataset = TensorDataset(train_features, train_labels)\n",
    "    test_dataset = TensorDataset(test_features, test_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    model = MLP(input_dim=train_features.shape[1])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=w_decay)\n",
    "    \n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(features.float())\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(output.view(-1), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_labels, all_predictions = [], []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            output = model(features.float())\n",
    "\n",
    "            pred = torch.sigmoid(output.squeeze()) >= 0.5\n",
    "            \n",
    "            all_labels.append(labels.cpu().numpy().flatten())\n",
    "            all_predictions.append(pred.cpu().numpy().flatten())\n",
    "\n",
    "            # Count correct predictions\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)  # Increment by the number of samples in this batch\n",
    "    \n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    print(f\"{modality}-Only Model - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy}%\")\n",
    "\n",
    "    # Calculate Metrics\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
