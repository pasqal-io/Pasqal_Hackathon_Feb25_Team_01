{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, GATConv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "n_clinical = 38 \n",
    "n_image_nodes = 6*6\n",
    "n_nodes = n_clinical + n_image_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples:  84\n",
      "Test Samples:  21\n",
      "Train labels shape: torch.Size([84])\n",
      "Test labels shape: torch.Size([21])\n"
     ]
    }
   ],
   "source": [
    "# Load Ground-Truth Values\n",
    "train_labels = pd.read_csv(\"data/labels/train_labels.csv\")\n",
    "train_labels = train_labels.iloc[:, 1].tolist()                 # (n_train,)\n",
    "test_labels = pd.read_csv(\"data/labels/test_labels.csv\")\n",
    "test_labels = test_labels.iloc[:, 1].tolist()                   # (n_test,)\n",
    "\n",
    "n_train = len(train_labels) # 84\n",
    "n_test = len(test_labels)   # 21\n",
    "\n",
    "print('Training Samples: ', n_train)\n",
    "print('Test Samples: ', n_test)\n",
    "\n",
    "# Convert to tensors\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "print(\"Train labels shape:\", train_labels.shape)                # Should be (n_train,)\n",
    "print(\"Test labels shape:\", test_labels.shape)                  # Should be (n_test,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "    return (embeddings - embeddings.mean()) / (embeddings.std() + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Image Embeddings:  (84, 6, 6, 128)\n",
      "Train Clinical Embeddings:  (84, 38, 128)\n",
      "Test Image Embeddings:  (21, 6, 6, 128)\n",
      "Test Clinical Embeddings:  (21, 38, 128)\n"
     ]
    }
   ],
   "source": [
    "# Load and normalise Embeddings\n",
    "train_image_embeddings = np.load(\"data/image_data/train_image_embeddings.npy\")             # (n_train, 6, 6, embed_dim)\n",
    "train_clinical_embeddings = np.load(\"data/clinical_data/train_embeddings.npy\")          # (n_train, 38, embed_dim)\n",
    "test_image_embeddings = np.load(\"data/image_data/test_image_embeddings.npy\")               # (n_test, 6, 6, embed_dim)\n",
    "test_clinical_embeddings = np.load(\"data/clinical_data/test_embeddings.npy\")            # (n_test, 38, embed_dim)\n",
    "\n",
    "print(\"Train Image Embeddings: \", train_image_embeddings.shape)\n",
    "print(\"Train Clinical Embeddings: \", train_clinical_embeddings.shape)\n",
    "print(\"Test Image Embeddings: \",test_image_embeddings.shape)\n",
    "print(\"Test Clinical Embeddings: \", test_clinical_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Train Image Embeddings:  torch.Size([84, 36, 128])\n",
      "Combined Train Embeddings:  torch.Size([84, 74, 128])\n",
      "Reshaped Test Image Embeddings:  torch.Size([21, 36, 128])\n",
      "Combined Test Embeddings:  torch.Size([21, 74, 128])\n"
     ]
    }
   ],
   "source": [
    "# Reshape image embeddings to match size of clinical embeddings\n",
    "train_image_features = torch.tensor(train_image_embeddings.reshape(n_train, 36, embed_dim))                             # Shape: [n_train, 36, embed_dim]\n",
    "test_image_features = torch.tensor(test_image_embeddings.reshape(n_test, 36, embed_dim))                                # Shape: [n_test, 36, embed_dim]\n",
    "\n",
    "# Combine clinical and image features\n",
    "train_patient_features = torch.cat([torch.tensor(train_clinical_embeddings), train_image_features], dim=1)              # Shape: [n_train, 74, embed_dim]\n",
    "test_patient_features = torch.cat([torch.tensor(test_clinical_embeddings), test_image_features], dim=1)                 # Shape: [n_test, 74, embed_dim]\n",
    "\n",
    "print('Reshaped Train Image Embeddings: ', train_image_features.shape)\n",
    "print('Combined Train Embeddings: ', train_patient_features.shape)\n",
    "print('Reshaped Test Image Embeddings: ', test_image_features.shape)\n",
    "print('Combined Test Embeddings: ', test_patient_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patient_edges(n_clinical, n_nodes):\n",
    "    \"\"\"\n",
    "    Creates bidirectional edges between clinical nodes and image nodes.\n",
    "    Adds a self-edge to each node.\n",
    "\n",
    "    Total edges = n_nodes (self-edges) + 2 * n_clinical * n_image_nodes (bidirectional edges)\n",
    "\n",
    "    Parameters:\n",
    "    - n_clinical: number of clinical nodes (for a specific patient)\n",
    "    - n_image_nodes: number of image nodes (for a specific patient)\n",
    "    \"\"\"\n",
    "    node_ids = np.expand_dims(np.arange(n_nodes, dtype=int), 0)\n",
    "    # self-edges = preserves some features of each own node during a graph convolution\n",
    "    self_edges = np.concatenate((node_ids, node_ids), 0)\n",
    "\n",
    "    # clinical nodes\n",
    "    c_array_asc = np.expand_dims(np.arange(n_clinical), 0)\n",
    "    all_edges = self_edges[:]\n",
    "\n",
    "    for i in range(n_clinical, n_nodes):\n",
    "        # image nodes\n",
    "        i_array = np.expand_dims(np.array([i]*n_clinical), 0)\n",
    "\n",
    "        # image --> clinical\n",
    "        inter_edges_ic = np.concatenate((i_array, c_array_asc), 0)\n",
    "        # clinical --> image\n",
    "        inter_edges_ci = np.concatenate((c_array_asc, i_array), 0)\n",
    "\n",
    "        # bidirectional edges\n",
    "        inter_edges_i = np.concatenate((inter_edges_ic, inter_edges_ci), 1)\n",
    "        all_edges = np.concatenate((all_edges, inter_edges_i), 1)\n",
    "\n",
    "    return torch.tensor(all_edges, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_list(patient_features, patient_labels):\n",
    "    \"\"\"\n",
    "    Generates a sub-graph for each patient given its embeddings\n",
    "\n",
    "    Parameters:\n",
    "    - patient_features: combined clinical and image embeddings of one patient\n",
    "    - patient_labels: groud truth values\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for i in range(len(patient_labels)):\n",
    "        # Create the graph for each patient\n",
    "        patient_edges = create_patient_edges(n_clinical, n_nodes)   # Shape: [2, num_edges]\n",
    "        patient_y = patient_labels[i]                               # Target label for this patient\n",
    "\n",
    "        data = Data(x=patient_features[i], edge_index=patient_edges, y=patient_y)\n",
    "        data_list.append(data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Patients:  84\n",
      "Test Patients:  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pascal/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_data_list = get_data_list(train_patient_features, train_labels)\n",
    "test_data_list = get_data_list(test_patient_features, test_labels)\n",
    "\n",
    "# Batch size 1 for individual patients\n",
    "train_loader = DataLoader(train_data_list, batch_size=1, shuffle=False, num_workers=0)  \n",
    "test_loader = DataLoader(test_data_list, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Train Patients: \", len(train_loader))\n",
    "print(\"Test Patients: \", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "We define the Graph Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)          # Second GCN layer\n",
    "        self.fc = torch.nn.Linear(hidden_channels, 1)                   # Fully connected layer for binary classification\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Apply graph convolution\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Global pooling (mean) across all nodes\n",
    "        x = global_mean_pool(x, batch)  # This will aggregate node features into one scalar per graph\n",
    "        \n",
    "        # Pass the aggregated feature through a fully connected layer to get a single logit\n",
    "        x = self.fc(x)  # Output size is (batch_size, 1)\n",
    "        return x  # Output a single logit for each patient (before applying sigmoid in loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Graph Attention Network\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, heads=2, dropout=0.5):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, dropout=dropout)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, 1)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)          # Aggregate node features\n",
    "        x = self.fc(x)                          # Binary classification output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Model Parameters\n",
    "learning_rate = 0.0001\n",
    "w_decay = 5e-4\n",
    "hidden_channels = 128\n",
    "\n",
    "# Initialize Model\n",
    "model = GCN(in_channels=embed_dim, hidden_channels=hidden_channels)\n",
    "# model = GAT(in_channels=embed_dim, hidden_channels=hidden_channels)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=w_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.6930353854383741\n",
      "Epoch 2/300, Loss: 0.6861713180939356\n",
      "Epoch 3/300, Loss: 0.6818450355813617\n",
      "Epoch 4/300, Loss: 0.6782309271040416\n",
      "Epoch 5/300, Loss: 0.674760531101908\n",
      "Epoch 6/300, Loss: 0.6712288962943214\n",
      "Epoch 7/300, Loss: 0.6674811492363611\n",
      "Epoch 8/300, Loss: 0.6633494648905027\n",
      "Epoch 9/300, Loss: 0.6587262994476727\n",
      "Epoch 10/300, Loss: 0.6536107269071397\n",
      "Epoch 11/300, Loss: 0.6478904031571888\n",
      "Epoch 12/300, Loss: 0.6413966129933085\n",
      "Epoch 13/300, Loss: 0.6342282210077558\n",
      "Epoch 14/300, Loss: 0.6263201967591331\n",
      "Epoch 15/300, Loss: 0.6177267041944322\n",
      "Epoch 16/300, Loss: 0.6083979196846485\n",
      "Epoch 17/300, Loss: 0.5984566637447902\n",
      "Epoch 18/300, Loss: 0.5879514447989918\n",
      "Epoch 19/300, Loss: 0.5769249277100676\n",
      "Epoch 20/300, Loss: 0.5655155109152907\n",
      "Epoch 21/300, Loss: 0.5538169867580846\n",
      "Epoch 22/300, Loss: 0.541913155909805\n",
      "Epoch 23/300, Loss: 0.5300238834073147\n",
      "Epoch 24/300, Loss: 0.5180742667899245\n",
      "Epoch 25/300, Loss: 0.5061394403732958\n",
      "Epoch 26/300, Loss: 0.4942933901967037\n",
      "Epoch 27/300, Loss: 0.4825601740518496\n",
      "Epoch 28/300, Loss: 0.4710981902622041\n",
      "Epoch 29/300, Loss: 0.4595728786662221\n",
      "Epoch 30/300, Loss: 0.44833663673627944\n",
      "Epoch 31/300, Loss: 0.4370698688019599\n",
      "Epoch 32/300, Loss: 0.4259217171708033\n",
      "Epoch 33/300, Loss: 0.4148399652247982\n",
      "Epoch 34/300, Loss: 0.4040118972327383\n",
      "Epoch 35/300, Loss: 0.3931567911945638\n",
      "Epoch 36/300, Loss: 0.38223144719155416\n",
      "Epoch 37/300, Loss: 0.37173164851542206\n",
      "Epoch 38/300, Loss: 0.36113655352632384\n",
      "Epoch 39/300, Loss: 0.3504969564299764\n",
      "Epoch 40/300, Loss: 0.3402530237550049\n",
      "Epoch 41/300, Loss: 0.3295213379093357\n",
      "Epoch 42/300, Loss: 0.31945557154193965\n",
      "Epoch 43/300, Loss: 0.30940905540704816\n",
      "Epoch 44/300, Loss: 0.2993864786154258\n",
      "Epoch 45/300, Loss: 0.28969962383104886\n",
      "Epoch 46/300, Loss: 0.2796206024830185\n",
      "Epoch 47/300, Loss: 0.2699551378685837\n",
      "Epoch 48/300, Loss: 0.26045841122824986\n",
      "Epoch 49/300, Loss: 0.25097777025837875\n",
      "Epoch 50/300, Loss: 0.24176680224038483\n",
      "Epoch 51/300, Loss: 0.23284168760403104\n",
      "Epoch 52/300, Loss: 0.22391457532441045\n",
      "Epoch 53/300, Loss: 0.21512342170223878\n",
      "Epoch 54/300, Loss: 0.2065990380809429\n",
      "Epoch 55/300, Loss: 0.19847374892993858\n",
      "Epoch 56/300, Loss: 0.19034398007663964\n",
      "Epoch 57/300, Loss: 0.18255890289583476\n",
      "Epoch 58/300, Loss: 0.17469850551346358\n",
      "Epoch 59/300, Loss: 0.16727790374378465\n",
      "Epoch 60/300, Loss: 0.16012871066782064\n",
      "Epoch 61/300, Loss: 0.15306375829411645\n",
      "Epoch 62/300, Loss: 0.14605662393274715\n",
      "Epoch 63/300, Loss: 0.13937056117737529\n",
      "Epoch 64/300, Loss: 0.13286372913093406\n",
      "Epoch 65/300, Loss: 0.12669196179086567\n",
      "Epoch 66/300, Loss: 0.12062969397374261\n",
      "Epoch 67/300, Loss: 0.11505610703920865\n",
      "Epoch 68/300, Loss: 0.10932843393500964\n",
      "Epoch 69/300, Loss: 0.10392199686314252\n",
      "Epoch 70/300, Loss: 0.09873699079394967\n",
      "Epoch 71/300, Loss: 0.09398063324248965\n",
      "Epoch 72/300, Loss: 0.08914144225198448\n",
      "Epoch 73/300, Loss: 0.08456470364282745\n",
      "Epoch 74/300, Loss: 0.0804504687627465\n",
      "Epoch 75/300, Loss: 0.07626328387466888\n",
      "Epoch 76/300, Loss: 0.07240236729942462\n",
      "Epoch 77/300, Loss: 0.06857178241989188\n",
      "Epoch 78/300, Loss: 0.06506893936816568\n",
      "Epoch 79/300, Loss: 0.06155203846074777\n",
      "Epoch 80/300, Loss: 0.05863362017089863\n",
      "Epoch 81/300, Loss: 0.055289200545970404\n",
      "Epoch 82/300, Loss: 0.05251198981678483\n",
      "Epoch 83/300, Loss: 0.04961387223551467\n",
      "Epoch 84/300, Loss: 0.04727619180130611\n",
      "Epoch 85/300, Loss: 0.04453482560035601\n",
      "Epoch 86/300, Loss: 0.04242091061491324\n",
      "Epoch 87/300, Loss: 0.040074150721670164\n",
      "Epoch 88/300, Loss: 0.038116127885710546\n",
      "Epoch 89/300, Loss: 0.03605151228067442\n",
      "Epoch 90/300, Loss: 0.034290966270485716\n",
      "Epoch 91/300, Loss: 0.032463126114567\n",
      "Epoch 92/300, Loss: 0.03098584073805032\n",
      "Epoch 93/300, Loss: 0.02929721527531861\n",
      "Epoch 94/300, Loss: 0.027906388348642657\n",
      "Epoch 95/300, Loss: 0.026501071842781124\n",
      "Epoch 96/300, Loss: 0.025279884909997025\n",
      "Epoch 97/300, Loss: 0.023967459762469934\n",
      "Epoch 98/300, Loss: 0.02295402747963256\n",
      "Epoch 99/300, Loss: 0.021755897021597276\n",
      "Epoch 100/300, Loss: 0.020809946218089896\n",
      "Epoch 101/300, Loss: 0.019806210847915076\n",
      "Epoch 102/300, Loss: 0.018963171955886895\n",
      "Epoch 103/300, Loss: 0.018064495882829386\n",
      "Epoch 104/300, Loss: 0.01730706085352619\n",
      "Epoch 105/300, Loss: 0.016530390441650816\n",
      "Epoch 106/300, Loss: 0.015825864507030746\n",
      "Epoch 107/300, Loss: 0.015159899671706696\n",
      "Epoch 108/300, Loss: 0.014546027844161136\n",
      "Epoch 109/300, Loss: 0.013924105327902902\n",
      "Epoch 110/300, Loss: 0.013386591357210199\n",
      "Epoch 111/300, Loss: 0.012846505177454861\n",
      "Epoch 112/300, Loss: 0.012359727933934937\n",
      "Epoch 113/300, Loss: 0.011878611781053141\n",
      "Epoch 114/300, Loss: 0.011468518600810404\n",
      "Epoch 115/300, Loss: 0.011040235260543324\n",
      "Epoch 116/300, Loss: 0.010662568529817543\n",
      "Epoch 117/300, Loss: 0.010284000554405603\n",
      "Epoch 118/300, Loss: 0.009936045655296663\n",
      "Epoch 119/300, Loss: 0.009619257557443174\n",
      "Epoch 120/300, Loss: 0.009322828337151009\n",
      "Epoch 121/300, Loss: 0.009017610743906803\n",
      "Epoch 122/300, Loss: 0.008762532511862986\n",
      "Epoch 123/300, Loss: 0.008499190050742353\n",
      "Epoch 124/300, Loss: 0.008254484398715789\n",
      "Epoch 125/300, Loss: 0.008029045791653052\n",
      "Epoch 126/300, Loss: 0.00781349234505083\n",
      "Epoch 127/300, Loss: 0.007613347759685817\n",
      "Epoch 128/300, Loss: 0.007411156452370364\n",
      "Epoch 129/300, Loss: 0.007237255085290479\n",
      "Epoch 130/300, Loss: 0.007062324480741077\n",
      "Epoch 131/300, Loss: 0.00691080306122678\n",
      "Epoch 132/300, Loss: 0.006748660586058804\n",
      "Epoch 133/300, Loss: 0.006608399717968809\n",
      "Epoch 134/300, Loss: 0.00646315941784885\n",
      "Epoch 135/300, Loss: 0.006345283013707308\n",
      "Epoch 136/300, Loss: 0.0062133739730171385\n",
      "Epoch 137/300, Loss: 0.0061033159778328\n",
      "Epoch 138/300, Loss: 0.005982201248835919\n",
      "Epoch 139/300, Loss: 0.005888296835932056\n",
      "Epoch 140/300, Loss: 0.005786528615630597\n",
      "Epoch 141/300, Loss: 0.005683557921569377\n",
      "Epoch 142/300, Loss: 0.005596312125578386\n",
      "Epoch 143/300, Loss: 0.005512142773989989\n",
      "Epoch 144/300, Loss: 0.005428462759633321\n",
      "Epoch 145/300, Loss: 0.005347940238276412\n",
      "Epoch 146/300, Loss: 0.005275877848312659\n",
      "Epoch 147/300, Loss: 0.005205743272290741\n",
      "Epoch 148/300, Loss: 0.005128871455591097\n",
      "Epoch 149/300, Loss: 0.005059441769286484\n",
      "Epoch 150/300, Loss: 0.0050026172655821865\n",
      "Epoch 151/300, Loss: 0.004944968156384667\n",
      "Epoch 152/300, Loss: 0.004881338978898462\n",
      "Epoch 153/300, Loss: 0.004822106140618928\n",
      "Epoch 154/300, Loss: 0.0047624060625861345\n",
      "Epoch 155/300, Loss: 0.004717130697311352\n",
      "Epoch 156/300, Loss: 0.004660246123988526\n",
      "Epoch 157/300, Loss: 0.004615563523894092\n",
      "Epoch 158/300, Loss: 0.004557649289260978\n",
      "Epoch 159/300, Loss: 0.0045230734507987575\n",
      "Epoch 160/300, Loss: 0.004465491164321463\n",
      "Epoch 161/300, Loss: 0.004425723158471736\n",
      "Epoch 162/300, Loss: 0.004376059506596739\n",
      "Epoch 163/300, Loss: 0.004331618421715974\n",
      "Epoch 164/300, Loss: 0.004288940760654537\n",
      "Epoch 165/300, Loss: 0.004259520258423201\n",
      "Epoch 166/300, Loss: 0.004210870918646475\n",
      "Epoch 167/300, Loss: 0.004177498718022373\n",
      "Epoch 168/300, Loss: 0.004137261213645734\n",
      "Epoch 169/300, Loss: 0.0041077049841195985\n",
      "Epoch 170/300, Loss: 0.004059483274205398\n",
      "Epoch 171/300, Loss: 0.004029627085835467\n",
      "Epoch 172/300, Loss: 0.004002824974453796\n",
      "Epoch 173/300, Loss: 0.003968837552963696\n",
      "Epoch 174/300, Loss: 0.003932720026037897\n",
      "Epoch 175/300, Loss: 0.003898031569638522\n",
      "Epoch 176/300, Loss: 0.0038810762264492077\n",
      "Epoch 177/300, Loss: 0.0038375454121828837\n",
      "Epoch 178/300, Loss: 0.0038129736115872994\n",
      "Epoch 179/300, Loss: 0.0037947350392461793\n",
      "Epoch 180/300, Loss: 0.003756176437791158\n",
      "Epoch 181/300, Loss: 0.003730517469807502\n",
      "Epoch 182/300, Loss: 0.0037055742474972845\n",
      "Epoch 183/300, Loss: 0.0036803042283848767\n",
      "Epoch 184/300, Loss: 0.0036562005079210686\n",
      "Epoch 185/300, Loss: 0.003635061924785397\n",
      "Epoch 186/300, Loss: 0.0036102259501486094\n",
      "Epoch 187/300, Loss: 0.003581556013466119\n",
      "Epoch 188/300, Loss: 0.0035668176395646576\n",
      "Epoch 189/300, Loss: 0.003546572247014099\n",
      "Epoch 190/300, Loss: 0.0035266460626818315\n",
      "Epoch 191/300, Loss: 0.0035031904538809855\n",
      "Epoch 192/300, Loss: 0.003482250853133383\n",
      "Epoch 193/300, Loss: 0.0034576484234251767\n",
      "Epoch 194/300, Loss: 0.003448596177324451\n",
      "Epoch 195/300, Loss: 0.0034319903830415284\n",
      "Epoch 196/300, Loss: 0.0034149536899456784\n",
      "Epoch 197/300, Loss: 0.0033985215313328304\n",
      "Epoch 198/300, Loss: 0.003374833000600507\n",
      "Epoch 199/300, Loss: 0.0033643750460825572\n",
      "Epoch 200/300, Loss: 0.003350860502288743\n",
      "Epoch 201/300, Loss: 0.0033286048224486615\n",
      "Epoch 202/300, Loss: 0.0033248591580694493\n",
      "Epoch 203/300, Loss: 0.0033122129305788966\n",
      "Epoch 204/300, Loss: 0.0032935658097475846\n",
      "Epoch 205/300, Loss: 0.003281973161034692\n",
      "Epoch 206/300, Loss: 0.003271530534750663\n",
      "Epoch 207/300, Loss: 0.0032657833640326673\n",
      "Epoch 208/300, Loss: 0.003253397354922207\n",
      "Epoch 209/300, Loss: 0.0032390206173370678\n",
      "Epoch 210/300, Loss: 0.0032233678926140946\n",
      "Epoch 211/300, Loss: 0.003225379364750546\n",
      "Epoch 212/300, Loss: 0.003210060192795963\n",
      "Epoch 213/300, Loss: 0.003201459301552388\n",
      "Epoch 214/300, Loss: 0.003197713203838199\n",
      "Epoch 215/300, Loss: 0.0031887169226277614\n",
      "Epoch 216/300, Loss: 0.003173017377769887\n",
      "Epoch 217/300, Loss: 0.003169262222426385\n",
      "Epoch 218/300, Loss: 0.0031609154926239094\n",
      "Epoch 219/300, Loss: 0.0031574478489786663\n",
      "Epoch 220/300, Loss: 0.0031505819800406428\n",
      "Epoch 221/300, Loss: 0.0031450300571116314\n",
      "Epoch 222/300, Loss: 0.0031323561764245206\n",
      "Epoch 223/300, Loss: 0.0031362417066343965\n",
      "Epoch 224/300, Loss: 0.003124416250718317\n",
      "Epoch 225/300, Loss: 0.0031146557721689035\n",
      "Epoch 226/300, Loss: 0.003109849904773011\n",
      "Epoch 227/300, Loss: 0.003107940353579525\n",
      "Epoch 228/300, Loss: 0.0031026417184190926\n",
      "Epoch 229/300, Loss: 0.0031000163783403423\n",
      "Epoch 230/300, Loss: 0.0030968942416683106\n",
      "Epoch 231/300, Loss: 0.003085685985961986\n",
      "Epoch 232/300, Loss: 0.003083337038903199\n",
      "Epoch 233/300, Loss: 0.0030854128845955336\n",
      "Epoch 234/300, Loss: 0.003081552358595662\n",
      "Epoch 235/300, Loss: 0.003078374701711287\n",
      "Epoch 236/300, Loss: 0.0030697589022200384\n",
      "Epoch 237/300, Loss: 0.003062446106475764\n",
      "Epoch 238/300, Loss: 0.0030603559835680827\n",
      "Epoch 239/300, Loss: 0.0030589399583599666\n",
      "Epoch 240/300, Loss: 0.0030552349691410946\n",
      "Epoch 241/300, Loss: 0.003048861984415415\n",
      "Epoch 242/300, Loss: 0.0030509042603118325\n",
      "Epoch 243/300, Loss: 0.003050880990646273\n",
      "Epoch 244/300, Loss: 0.0030349113662235063\n",
      "Epoch 245/300, Loss: 0.0030392370524061733\n",
      "Epoch 246/300, Loss: 0.0030383435467674267\n",
      "Epoch 247/300, Loss: 0.0030328766182255626\n",
      "Epoch 248/300, Loss: 0.0030262140590162105\n",
      "Epoch 249/300, Loss: 0.0030291823705025102\n",
      "Epoch 250/300, Loss: 0.003030228697187995\n",
      "Epoch 251/300, Loss: 0.003026284987568385\n",
      "Epoch 252/300, Loss: 0.003018366194627306\n",
      "Epoch 253/300, Loss: 0.0030183298220928614\n",
      "Epoch 254/300, Loss: 0.0030211493601140795\n",
      "Epoch 255/300, Loss: 0.0030165558337075923\n",
      "Epoch 256/300, Loss: 0.003005018062651743\n",
      "Epoch 257/300, Loss: 0.0030106648981727416\n",
      "Epoch 258/300, Loss: 0.003006195460722254\n",
      "Epoch 259/300, Loss: 0.0030123172220658575\n",
      "Epoch 260/300, Loss: 0.0029963260451955605\n",
      "Epoch 261/300, Loss: 0.0029957286499076134\n",
      "Epoch 262/300, Loss: 0.0030015515162441866\n",
      "Epoch 263/300, Loss: 0.002996577417556117\n",
      "Epoch 264/300, Loss: 0.002991582864445891\n",
      "Epoch 265/300, Loss: 0.0029960715680691203\n",
      "Epoch 266/300, Loss: 0.002991769565971665\n",
      "Epoch 267/300, Loss: 0.0029925151992163785\n",
      "Epoch 268/300, Loss: 0.0029838875633851373\n",
      "Epoch 269/300, Loss: 0.0029846004661676162\n",
      "Epoch 270/300, Loss: 0.0029800965459996645\n",
      "Epoch 271/300, Loss: 0.002993045798441634\n",
      "Epoch 272/300, Loss: 0.0029851642914593413\n",
      "Epoch 273/300, Loss: 0.0029723902287560663\n",
      "Epoch 274/300, Loss: 0.002971716753110275\n",
      "Epoch 275/300, Loss: 0.00297199794765052\n",
      "Epoch 276/300, Loss: 0.002978015700138247\n",
      "Epoch 277/300, Loss: 0.0029771861192928314\n",
      "Epoch 278/300, Loss: 0.002967753321498668\n",
      "Epoch 279/300, Loss: 0.002963237596759427\n",
      "Epoch 280/300, Loss: 0.002962463836200056\n",
      "Epoch 281/300, Loss: 0.002962992576656342\n",
      "Epoch 282/300, Loss: 0.002965847031240238\n",
      "Epoch 283/300, Loss: 0.0029605828411723408\n",
      "Epoch 284/300, Loss: 0.0029549780368283577\n",
      "Epoch 285/300, Loss: 0.002956748652000637\n",
      "Epoch 286/300, Loss: 0.002959646692050098\n",
      "Epoch 287/300, Loss: 0.0029510746820733402\n",
      "Epoch 288/300, Loss: 0.002946996785528286\n",
      "Epoch 289/300, Loss: 0.0029574860422390734\n",
      "Epoch 290/300, Loss: 0.00295823612454239\n",
      "Epoch 291/300, Loss: 0.0029421232891403784\n",
      "Epoch 292/300, Loss: 0.0029541054914789455\n",
      "Epoch 293/300, Loss: 0.0029467773505018613\n",
      "Epoch 294/300, Loss: 0.002948375996266707\n",
      "Epoch 295/300, Loss: 0.002942747407391131\n",
      "Epoch 296/300, Loss: 0.0029398567149761285\n",
      "Epoch 297/300, Loss: 0.002943549939284854\n",
      "Epoch 298/300, Loss: 0.0029414676572771655\n",
      "Epoch 299/300, Loss: 0.002941758664462668\n",
      "Epoch 300/300, Loss: 0.0029350733499828017\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "train_losses = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for data in train_loader:                                               # Iterate over each batch (here, each batch is one patient)\n",
    "                                                                            # Data object contains 'x' (features), 'edge_index' (graph edges), 'y' (labels)\n",
    "        patient_features = data.x                                           # Shape: (num_nodes, in_channels)\n",
    "        patient_edges = data.edge_index                                     # Shape: (2, num_edges)\n",
    "        patient_label = data.y.float()                                      # Target label\n",
    "        batch = data.batch\n",
    "\n",
    "        # Ensure correct format\n",
    "        patient_features = patient_features.float()\n",
    "        patient_edges = patient_edges.to(torch.long)                 \n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(patient_features, patient_edges, batch)                  # Output shape: (1, 1)\n",
    "        \n",
    "        # Binary Classification Loss\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(output.view(-1), patient_label)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Print loss after each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAIjCAYAAAA0vUuxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABY0ElEQVR4nO3dCZyN5f//8fdYZuyDJmtKKJKikCRpkSXZ2lCW9E2l8lVa1RftWv2URJRoJRVJUYhKKWVplRbZ9xaTnZnzf3zu+3/GzJjdzFxneT0fj7tzn3U+M/cZnfdc1/W5YwKBQEAAAAAAgEwVyfwuAAAAAIAhOAEAAABANghOAAAAAJANghMAAAAAZIPgBAAAAADZIDgBAAAAQDYITgAAAACQDYITAAAAAGSD4AQAAAAA2SA4AUCYufrqq1WzZs08Pfe+++5TTExMvtcE5OR9t337dtelAECeEZwAIJ/YB8OcbAsWLFC0Br4yZcooHAQCAb3yyis655xzVL58eZUqVUqnnHKKHnjgAe3atUuhGkwy2zZv3uy6RAAIe8VcFwAAkcI+aKf28ssva86cOYfdftJJJx3R1xk/frySk5Pz9Nz//e9/uvvuu4/o60e6pKQkXXnllXrzzTfVsmVLL5RYcPrss890//33a+rUqZo7d64qV66sUDNmzJgMw6mFPwDAkSE4AUA+6dmzZ5rrX375pRec0t+e3u7du70P5jlVvHjxPNdYrFgxb0PmHn/8cS803X777XriiSdSbr/uuut0xRVXqEuXLt7o2axZswq1rpy8Ty677DIlJCQUWk0AEE2YqgcAhejcc89VgwYNtGTJEm8amH0Qvueee7z73n33XXXo0EHVqlVTXFycateurQcffNAbAclqjdPq1au96VhPPvmkxo0b5z3Pnt+0aVN9/fXX2a5xsus333yzpk+f7tVmzz355JM1e/bsw+q3aYZNmjRRiRIlvK/z/PPP5/u6KRvRady4sUqWLOmFAAueGzZsSPMYm3rWt29fHXPMMV69VatWVefOnb2fRdA333yjtm3beq9hr3X88cfrmmuuyfJr79mzxwtLJ554ooYPH37Y/R07dlSfPn28n40FY3PxxRerVq1aGb5e8+bNvZ9Xaq+++mrK91exYkV1795d69aty/H75EjY8bNjNWXKFO/1qlSpotKlS6tTp06H1ZDTY2F+/vlnL1QeffTR3mPr1q2re++997DH/fPPP97710bA4uPjvWNogTA1+2PD2Wef7T3GRs/stfLjeweAI8WfHQGgkP35559q376994HZPogGp3xNnDjR+6A4aNAg7/Ljjz/W0KFDlZiYmGbkIzOvv/66/v33X11//fXeh2MbObnkkku0atWqbEepFi5cqHfeeUc33nijypYtq2eeeUaXXnqp1q5dq6OOOsp7zLJly9SuXTsvpNiUNQt0tubHPiznF/sZ2IdpC30WXLZs2aKnn35an3/+uff1g1POrLYff/xRAwYM8ELk1q1bvQ/cVm/weps2bbzabGqiPc9ClX2P2f0c/v77bw0cODDTkbnevXvrpZde0syZM3XmmWeqW7du3m0WUq3uoDVr1njhKvWxe/jhhzVkyBAvZFx77bXatm2bRo0a5YWj1N9fVu+TrPz111+H3WbfR/qpelaHvUfuuusu72c1cuRItW7dWsuXL/eCT26OxXfffedNabT3mI3K2c//999/13vvved9ndTs+7YAa6+3dOlSvfDCC6pUqZIee+wx7347phZETz31VO+9ZaH4t99+874mADgXAAAUiJtuuimQ/p/ZVq1aebeNHTv2sMfv3r37sNuuv/76QKlSpQJ79+5Nua1Pnz6B4447LuX6H3/84b3mUUcdFfjrr79Sbn/33Xe92997772U24YNG3ZYTXY9NjY28Ntvv6Xc9u2333q3jxo1KuW2jh07erVs2LAh5bZff/01UKxYscNeMyNWd+nSpTO9f//+/YFKlSoFGjRoENizZ0/K7TNnzvRef+jQod71v//+27v+xBNPZPpa06ZN8x7z9ddfB3Jj5MiR3vPs+Zmxn7E95pJLLvGu79ixIxAXFxe47bbb0jzu8ccfD8TExATWrFnjXV+9enWgaNGigYcffjjN477//nvvZ5j69qzeJxkJHteMtrp166Y8bv78+d5t1atXDyQmJqbc/uabb3q3P/3007k6Fuacc84JlC1bNuX7DEpOTj6svmuuuSbNY7p27eq9b4P+7//+z3vctm3bcvR9A0BhYqoeABQy+yu6/SU/veBf+o2NHFnrZvtLvk1lsqlQ2bGRjwoVKqRct+caG3HKjo022NS7IPuLf7ly5VKea6NL1hDB1vfYVMKgOnXqeKMi+cGm1tnoh4162VTAIJu+WK9ePb3//vspP6fY2Fhv2pmNDmUkOBpio0IHDhzIcQ32czc26paZ4H02Emjs52Q/A1sX5edQn02HsxGpY4891rtuo13W1MNGXezYBjebLnfCCSdo/vz5OXqfZOXtt9/2Rt5SbzY6lp6NkKX+Hm1tlI0kfvDBB7k6FjZi9umnn3pTIIPfZ1BG0zdvuOGGNNftPWoja8GfZfC42bTVvDZAAYCCQnACgEJWvXp174N/ejZNqWvXrt7aD/swbtPMgo0lduzYke3rpv/gGgxRmYWLrJ4bfH7wufYh2tb/WFBKL6Pb8sKmthlb05KefVgP3m+BwqZ2WXMGm75m09xsWmLqltutWrXypvPZlEJbm2PrnyxA7Nu3L8sagmEiGKByGq4stNoaoUWLFnnXbaqarU+y24N+/fVXL1hZSLJjm3pbsWKF9zPOyfskK/azsBCcerN1VulZDelDjh3H4BqxnB6LYLC29Vg5kd171H5eLVq08KYx2rG1aYoWSAlRAEIBwQkAClnqkaXUi+btw/63337rre2w9SE2WhBc+5GTD45FixbN8PbUoyAF8VwXbrnlFv3yyy/eWhkbEbF1Q9bm3dbeBIPAW2+95QUZa3xhDQ1sVMQaHezcuTPT1w22ird1O5kJ3le/fv00TSOsgYN9yDd2WaRIEV1++eUpj7FjaHVZY4n0o0K2WaON7N4n4S6795l9zzaCZaObvXr18n7WFqYuvPDCw5qkAEBhIzgBQAiwaWc2ZckW5FtjAlsgb6MFqafeuWQL+C2g2EL99DK6LS+OO+4473LlypWH3We3Be8PsqmFt912mz766CP98MMP2r9/v5566qk0j7GpctagwKaevfbaa96o3uTJkzOtIdjNzRptZPZB3c7PZewYBVlnOrtuXegsINk0PZuGlnpao9VrAcGaI6QfFbLNai0sNvqVmtVlxzHYrTGnxyLYTdB+/vnFAucFF1ygESNG6KeffvKOnzVKST+VEQAKG8EJAELoL/GpR3gsCDz33HMKlfrsw721LN+4cWPK7fZhO7/OZ2Rtuy2gjR07Ns2UOnt9m8pm62uMrfnau3dvmudaKLGpc8Hn2dSv9KNljRo18i6zmq5no0Z2/iYLBxm107a1PRZurc15+qBjIyP2s7FOcTZymHqanrEOh/ZztOmD6Wuz6xacC4uFv9TTEW10btOmTSnr1XJ6LGyaoU0PnDBhgtfRMP33lFsZdQXMyXEDgMJAO3IACAFnnXWWN7pk5wj673//603peuWVV0Jqqpydr8lGd2wNSv/+/b0RmWeffdZb32JtrHPCGjU89NBDh91u5zOyRgQ2NdEaIti0xR49eqS0wLaRkFtvvdV7rE3RsxEJa7Jg0+Ws3fa0adO8x9qaGDNp0iQvdNqaMQtVFhLGjx/vrR276KKLsqzR2pfblD+rxab62Vopm0JmrcrtHEw2nc9ePz17XQtvFrwsINnzUrM67HsfPHiwt5bIGm3Y4//44w+vfmvlbc89EhaArJV9ejbVLXU7c/t52+ia/azt52btyG2NU79+/bz7rbV4To6Fsdb19lqnn3669z3YiJp9fxYyc/q+CLJpqjZVz4KZjWrZui87jna+LvsaAOBUofbwA4Aoklk78pNPPjnDx3/++eeBM888M1CyZMlAtWrVAnfeeWfgww8/9F7D2khn1448o/bcdru1gs6uHbnVmp59Dftaqc2bNy9w2mmnee3La9euHXjhhRe8NtwlSpTI9udhr5VZy2x7raApU6Z4X8NafFesWDFw1VVXBdavX59y//bt271669Wr57U3j4+PDzRr1sxrqR20dOnSQI8ePQLHHnus9zrWWvviiy8OfPPNN4GcSEpKCrz00kuBFi1aBMqVK+d9f3bc7r///sDOnTszfZ7Vat9P69atM33M22+/HTj77LO92m2z78O+n5UrV+bofZLbduSp3z/BduRvvPFGYPDgwd7Pxd5vHTp0OKydeE6ORdAPP/zgtRYvX76897OyFuhDhgw5rL70bcbtZ2y323s4+P7q3Lmz9/6395hd2nH85ZdfcvyzAICCEmP/cRvdAADhzEZObO1Q+nUzCM21dOedd563FstakAMAco41TgCAHLOW5KlZWLJz/5x77rnOagIAoDCwxgkAkGPWRe3qq6/2Lu1cPmPGjPHONXTnnXe6Lg0AgAJFcAIA5Fi7du30xhtveCebtRPR2slVH3nkkcNOqAoAQKRhjRMAAAAAZIM1TgAAAACQDYITAAAAAGQj6tY4JScne2d2t5MO2gkmAQAAAESnQCDgnSS9WrVqKlIk6zGlqAtOFppq1KjhugwAAAAAIWLdunU65phjsnxM1AUnG2kK/nDKlSvnuhwAAAAAjiQmJnqDKsGMkJWoC07B6XkWmghOAAAAAGJysISH5hAAAAAAkA2CEwAAAABkg+AEAAAAANmIujVOAAAAiBxJSUk6cOCA6zIQwooXL66iRYse8esQnAAAABCWdu7cqfXr13vn4gGyavxgrcbLlCmjI0FwAgAAQFiONFloKlWqlI4++ugcdUVD9AkEAtq2bZv3XjnhhBOOaOSJ4AQAAICwY9Pz7EOxhaaSJUu6LgchzN4jq1ev9t4zRxKcaA4BAACAsMVIEwrrPUJwAgAAAIBsEJwAAAAAIByC0+jRo1WzZk2VKFFCzZo10+LFizN97LnnnusNt6XfOnToUKg1AwAAAKHAPkePHDkyx49fsGCB9/n5n3/+KdC6Io3z4DRlyhQNGjRIw4YN09KlS9WwYUO1bdtWW7duzfDx77zzjjZt2pSy/fDDD94ir8svv7zQawcAAAByKqM//qfe7rvvvjy97tdff63rrrsux48/66yzvM/R8fHxKkgLIiygOe+qN2LECPXr1099+/b1ro8dO1bvv/++JkyYoLvvvvuwx1esWDHN9cmTJ3ttKDMLTvv27fO2oMTExHz/HgAAAIDsWFhJPXgwdOhQrVy5MuW21OcZso6B1nK9WLFiOeoalxuxsbGqUqVKrp4DxyNO+/fv15IlS9S6detDBRUp4l1ftGhRjl7jxRdfVPfu3VW6dOkM7x8+fLiXpoNbjRo18q1+AAAAhAY7B+6uXW62nJ5/18JKcLPPpTYaE7z+888/q2zZspo1a5YaN26suLg4LVy4UL///rs6d+6sypUre8GqadOmmjt3bpZT9ex1X3jhBXXt2tUbYLDzF82YMSPTkaCJEyeqfPny+vDDD3XSSSd5X6ddu3Zpgt7Bgwf13//+13vcUUcdpbvuukt9+vRRly5d8nzM/v77b/Xu3VsVKlTw6mzfvr1+/fXXlPvXrFmjjh07evfbZ/2TTz5ZH3zwQcpzr7rqqpR29PY9vvTSS4rY4LR9+3YvSdsbITW7vnnz5myfb2uhbKretddem+ljBg8erB07dqRs69aty5faAQAAEDp277YRGzebfe38YjOuHn30Ua1YsUKnnnqqdu7cqYsuukjz5s3TsmXLvEBjYWLt2rVZvs7999+vK664Qt999533fAsZf/31VxY/v9168skn9corr+jTTz/1Xv/2229Puf+xxx7Ta6+95oWTzz//3JvFNX369CP6Xq+++mp98803XqizQRMbZbNa7XxL5qabbvJmjlk933//vVdDcFRuyJAh+umnn7ygaT+rMWPGKCEhQRE9Ve9I2GjTKaecojPOOCPTx1hatw0AAAAIdQ888IAuvPDCNMtUrAdA0IMPPqhp06Z5YePmm2/OMpT06NHD23/kkUf0zDPPeIMOFrwyYmHFlszUrl3bu26vbbUEjRo1yhuQsFEs8+yzz6aM/uSFjSzZ92AhzNZcGQtmNjvMApktw7Hwdumll3qf902tWrVSnm/3nXbaaWrSpEnKqFtBcxqcLBVaY4ctW7akud2uZzfvcteuXd76ptQHNNzs3Ss984x0zTX2s3BdDQAAQPgqVUraudPd184vwSAQZCNO1jTCegDY1DmbMrdnz55sR5xstCrIprmVK1cu0+ZrxqbKBUOTqVq1asrjbdbWli1b0gxW2Gd4m1KYnJycp+/TRols/ZZ11A6yKYB169b17jM2NbB///766KOPvKU8FqKC35fdbtetuVybNm28KYPBABaRU/VsYZr9wG3oMch++Ha9efPmWT536tSp3tBdz549Fa66dZPuusuGUl1XAgAAEN5iYiwguNnsa+eX9Ov2bbqcjTDZqNFnn32m5cuXeyMw1isgK8WLF0/384nJMuRk9HibOufStddeq1WrVqlXr17eVD0LlTbyZWw9lK2BuvXWW7Vx40ZdcMEFaaYWRmQ7cmtFPn78eE2aNMlLl5YebTQp2GXPFozZsGBG0/QsWVoyDVcDB/qXY8ZY6nZdDQAAAEKNTWWzaXc2Rc4Ck83KWr16daHWYI0sKleu7LU9D7I+BTbak1fWhMJGz7766quU2/7880+vy2D9+vVTbrOpezfccIN3SqLbbrvNyw1B1hjCGlS8+uqrXnOMcePGKaLXOHXr1k3btm3z2jFaQ4hGjRpp9uzZKQ0jbBjSOu2lZj9Q6zJiw3bh7PzzpU6dJGtycscd0syZrisCAABAKLFucRYarCGEjQJZU4S8To87EgMGDPC6VdepU0f16tXzRn6ss53VlB0bLbKOgUH2HFu3Zd0C7bREzz//vHe/NcaoXr26d7u55ZZbvJGlE0880fta8+fP9wKXsexgM9es057NQps5c2bKfREbnIKLzzJb3GbtEtOzuY+uhw7zyxNPSLau7v33pTlzpFRrAQEAABDl7Jyn11xzjbd+x/oDWBtwF+clveuuu7xBDpsNZuub7IS7bdu29fazc84556S5bs+x0Sbr0Ddw4EBdfPHF3tRDe5w1nAhOG7RRLeust379em+NljW2+L//+7+UJT82K81G36wdecuWLb3+BwUpJhApCSSH7I1mw422yM0OQCi45Rbp6aftLwrSl19a9xTXFQEAAIS2vXv36o8//tDxxx+vEiVKuC4n6iQnJ3sjPNby3Dr9het7JTfZwPkaJ9hQo3TMMdaW0Z+6t2eP64oAAACAQ9asWeOtL/rll1+8qXfWl8DCyJVXXqloQXAKATbCNGuWVL68LQCU7P138KDrqgAAAABfkSJFNHHiRDVt2lQtWrTwwtPcuXMLfF1RKAmJNU6QGjSQ3n1XatNGspMwX3aZZNM0GXkGAACAazVq1PA6/EUzRpxCiK2bmzpViovzQ9RFF9m8S9dVAQAAACA4hZiOHaXZsyXr2Dh/vt+yfNs211UBAACEpijrcwaH7xGCUwg691w/NCUkSEuW+CNR69a5rgoAACB0BNtgWxtrICvB90hOWqdnhTVOIapxY+mzz/zzOv38s9SihX+ep7p1XVcGAADgXrFixVSqVClt27bNO++PNS8AMmqbbu8Re6/Ye+ZIEJxCWL16fpc9C0+//CK1bOlP4zv9dNeVAQAAuBUTE6OqVat6LbGtVTaQGQvVxx57rPeeORKcADcMbN0qtW8vLV3qr3167z2pVSvXVQEAAITGiALT9ZCV2NjYTEckc5MNGHEKA5Uq+Wue7OS4n3witWsnvfmm30gCAAAgmtkH4hKcvwWFgMmgYcICsJ0k18LS3r1S167+eZ4AAAAAFDyCUxgpWVJ6+22pVy8pKUnq2dOftgcAAACgYBGcwkzx4tLEiYfC0+WX+9P4AAAAABQcglMYsrVtEyZInTtL+/b5a59++MF1VQAAAEDkIjiFKWtDb2ucrLvezp3+2ifrvgcAAAAg/xGcwpg1kLE1T7VrS6tXS5dc4o9AAQAAAMhfBKcwd9RR0syZUny8f7LcW291XREAAAAQeQhOEaBePemNN/z9MWNoUw4AAADkN4JThGjfXrrnHn+/Xz9p5UrXFQEAAACRg+AUQe6//1CziB49pAMHXFcEAAAARAaCU4R12nv9daliRWnZMunhh11XBAAAAEQGglOEqVZNGj3a33/oIWnJEtcVAQAAAOGP4BSBunWTLrtMSkqS+vSR9u93XREAAAAQ3ghOESgmRnruOenoo6Uff5RGjnRdEQAAABDeCE4RykLTE0/4+w88IK1f77oiAAAAIHwRnCJYr15S8+bSrl3SHXe4rgYAAAAIXwSnCFakiN8owi7tpLgLFriuCAAAAAhPBKcId9pp0vXX+/u33y4lJ7uuCAAAAAg/BKcocN99Upkyfmvyt95yXQ0AAAAQfghOUaBSpUNrnO65RzpwwHVFAAAAQHghOEWJQYOkypWl33+Xxo1zXQ0AAAAQXghOUcKm6g0b5u8/9JC0Z4/rigAAAIDwQXCKIv/5j3TssdLmzdILL7iuBgAAAAgfBKcoEhsrDR7s7z/2mLRvn+uKAAAAgPBAcIoyfftK1atLGzZIEye6rgYAAAAIDwSnKBMXJ915p78/fDgd9gAAAICcIDhFoX79/A57a9ZIU6a4rgYAAAAIfQSnKFSypDRggL//1FNSIOC6IgAAACC0EZyi1A03SKVKScuXSx9/7LoaAAAAILQRnKLUUUf5jSKCo04AAAAAMkdwimK33irFxEizZkk//eS6GgAAACB0EZyiWO3aUteu/v7TT7uuBgAAAAhdBKcoN3Cgf/nqq9KOHa6rAQAAAEITwSnKtWwpnXyytHu39PLLrqsBAAAAQhPBKcrZGqcbb/T3n3uO1uQAAABARghOUM+eUpky0s8/S/Pnu64GAAAACD0EJ6hcOalXL39/9GjX1QAAAAChh+AET3C63owZ0ubNrqsBAAAAQovz4DR69GjVrFlTJUqUULNmzbR48eIsH//PP//opptuUtWqVRUXF6cTTzxRH3zwQaHVG6kaNJDOPFM6eJAmEQAAAEBIBacpU6Zo0KBBGjZsmJYuXaqGDRuqbdu22rp1a4aP379/vy688EKtXr1ab731llauXKnx48erevXqhV57JLr2Wv/yxRdpEgEAAACkFhMIuPuIbCNMTZs21bPPPutdT05OVo0aNTRgwADdfffdhz1+7NixeuKJJ/Tzzz+rePHiefqaiYmJio+P144dO1TOFvcgxb//SlWrSrt2SZ9+6rcqBwAAACJVbrKBsxEnGz1asmSJWrdufaiYIkW864sWLcrwOTNmzFDz5s29qXqVK1dWgwYN9MgjjygpKSnTr7Nv3z7vB5J6Q8bKlpW6d/f3X3jBdTUAAABA6HAWnLZv3+4FHgtAqdn1zZl0J1i1apU3Rc+eZ+uahgwZoqeeekoPPfRQpl9n+PDhXooMbjaiheyn602dauvJXFcDAAAAhAbnzSFyw6byVapUSePGjVPjxo3VrVs33Xvvvd4UvswMHjzYG3oLbuvWrSvUmsNNs2ZS/frSnj3SW2+5rgYAAACI8uCUkJCgokWLasuWLWlut+tVqlTJ8DnWSc+66Nnzgk466SRvhMqm/mXEOu/ZfMXUGzIXEyP17u3vv/qq62oAAACAKA9OsbGx3qjRvHnz0owo2XVbx5SRFi1a6LfffvMeF/TLL794gcpeD/njyiv9y08+kdaudV0NAAAAEOVT9awVubUTnzRpklasWKH+/ftr165d6tu3r3d/7969val2QXb/X3/9pYEDB3qB6f333/eaQ1izCOQfWwZ27rn+/uuvu64GAAAAcK+Yyy9ua5S2bdumoUOHetPtGjVqpNmzZ6c0jFi7dq3XaS/IGjt8+OGHuvXWW3Xqqad652+yEHXXXXc5/C4iU8+e0oIF0iuvSPbjtSl8AAAAQLRyeh4nFziPU85YRz1barZvn7RsmdSokeuKAAAAgCg8jxNCW/nyUseO/j5NIgAAABDtCE7IcrpecJ1TFucYBgAAACIewQmZat9eqlhR2rRJmj/fdTUAAACAOwQnZMo6vF9xhb/PdD0AAABEM4ITcjRd7+23pd27XVcDAAAAuEFwQpbOOkuqWVPauVOaMcN1NQAAAIAbBCdkyc7fFBx1YroeAAAAohXBCdkKBqfZs6U//3RdDQAAAFD4CE7IVt26UsOGfkvy6dNdVwMAAAAUPoITciTYXe/NN11XAgAAABQ+ghNy5PLL/ct586Tt211XAwAAABQughNy5IQTpNNO86frTZvmuhoAAACgcBGckGNM1wMAAEC0Ijgh19P15s+Xtm1zXQ0AAABQeAhOyLHataXGjZmuBwAAgOhDcEKeRp2YrgcAAIBoQnBCnqfrbd3quhoAAACgcBCckCu1aklNmkjJydI777iuBgAAACgcBCfkubve1KmuKwEAAAAKB8EJeZ6ut2CBtGWL62oAAACAgkdwQq7VrCmdcQbT9QAAABA9CE7IE7rrAQAAIJoQnJAnl13mX376KSfDBQAAQOQjOCHP0/XsZLg2XW/6dNfVAAAAAAWL4IQ8u/RS//Ltt11XAgAAABQsghOOODjNmyf9/bfragAAAICCQ3BCnp14onTKKdLBg9KMGa6rAQAAAAoOwQn50iTirbdcVwIAAAAUHIIT8mW63kcfSYmJrqsBAAAACgbBCUekfn2pXj1p/35p5kzX1QAAAAAFg+CEIxITQ3c9AAAARD6CE/JtndOsWdKuXa6rAQAAAPIfwQlHrGFDqVYtac8ePzwBAAAAkYbghHydrkd3PQAAAEQighPydbre++9Le/e6rgYAAADIXwQn5IumTaUaNaSdO/3W5AAAAEAkITghXzBdDwAAAJGM4IR8EwxOM2b453UCAAAAIgXBCfnmrLOkqlWlHTukefNcVwMAAADkH4IT8k2RIlLXrv4+J8MFAABAJCE4oUCm602fLh086LoaAAAAIH8QnJCvzjlHSkiQ/vxT+uQT19UAAAAA+YPghHxVrJjUpYu/T3c9AAAARAqCEwrsZLjTpklJSa6rAQAAAI4cwQn57rzzpPLlpS1bpM8/d10NAAAAcOQITsh3sbFS587+Pt31AAAAEAkITijQ7noWnJKTXVcDAAAAHBmCEwrEhRdKZctKGzZIixe7rgYAAAA4MgQnFIgSJaSLL/b3ma4HAACAcEdwQoFP17O25IGA62oAAACAMA9Oo0ePVs2aNVWiRAk1a9ZMi7OY2zVx4kTFxMSk2ex5CD3t20ulSkmrV0vLlrmuBgAAAAjj4DRlyhQNGjRIw4YN09KlS9WwYUO1bdtWW7duzfQ55cqV06ZNm1K2NWvWFGrNyBkLTRaeDCfDBQAAQDhzHpxGjBihfv36qW/fvqpfv77Gjh2rUqVKacKECZk+x0aZqlSpkrJVrly5UGtG7k+Gy3Q9AAAAhDOnwWn//v1asmSJWrdufaigIkW864sWLcr0eTt37tRxxx2nGjVqqHPnzvrxxx8zfey+ffuUmJiYZkPh6dBBiouTfv1V+uEH19UAAAAAYRictm/frqSkpMNGjOz65s2bM3xO3bp1vdGod999V6+++qqSk5N11llnaf369Rk+fvjw4YqPj0/ZLGyh8FhL8rZt/X266wEAACBcOZ+ql1vNmzdX79691ahRI7Vq1UrvvPOOjj76aD3//PMZPn7w4MHasWNHyrZu3bpCrznape6uBwAAAISjYi6/eEJCgooWLaotW7akud2u29qlnChevLhOO+00/fbbbxneHxcX521wp2NHO06Szaj8+WepXj3XFQEAAABhNOIUGxurxo0ba968eSm32dQ7u24jSzlhU/2+//57Va1atQArxZGoUEG64AJ/n+l6AAAACEfOp+pZK/Lx48dr0qRJWrFihfr3769du3Z5XfaMTcuz6XZBDzzwgD766COtWrXKa1/es2dPrx35tdde6/C7QE6n6xGcAAAAEI6cTtUz3bp107Zt2zR06FCvIYStXZo9e3ZKw4i1a9d6nfaC/v77b699uT22QoUK3ojVF1984bUyR+jq0kW64Qb/RLirVkm1armuCAAAAMi5mEAgus6uY+3IrbueNYqwE+mi8Nh0vY8/lh5/XLrjDtfVAAAAINol5iIbOJ+qh+g7GS7T9QAAABBuCE4oNF27SjEx0ldfSXSFBwAAQDghOKHQWIf5s8/29995x3U1AAAAQM4RnOCku97Uqa4rAQAAAHKO4AQnwenzz6X1611XAwAAAOQMwQmF6phjDk3XY9QJAAAA4YLghELXrZt/OWWK60oAAACAnCE4wUlbcjunsXXX++MP19UAAAAA2SM4wUl3vVat/P0333RdDQAAAJA9ghOc6N7dv2S6HgAAAMIBwQlOXHKJVLSotGyZ9OuvrqsBAAAAskZwghMJCVLr1v4+o04AAAAIdQQnOEN3PQAAAIQLghOc6dJFKl5c+uEH6ccfXVcDAAAAZI7gBGcqVJDatvX3GXUCAABAKCM4IWS66wUCrqsBAAAAMkZwglOdOkklSki//CJ9+63ragAAAICMEZzgVNmy0kUX+ftM1wMAAECoIjghpLrrMV0PAAAAoYjgBOc6dJBKlZL++EP65hvX1QAAAACHIzjBudKlpY4d/f3Jk11XAwAAAByO4ISQmq735ptScrLragAAAIC0CE4ICe3b+40i1q+XFi1yXQ0AAACQFsEJIcFaknfp4u/TXQ8AAAChhuCEkJuuN3WqlJTkuhoAAADgEIITQsaFF0oVKkibN0uffea6GgAAAOAQghNCRmys1LWrv890PQAAAIQSghNCcrreW29JBw+6rgYAAADwEZwQUs4/X0pIkLZvlz7+2HU1AAAAgI/ghJBSrJh02WX+PtP1AAAAECoITgjZ6XrvvCPt3++6GgAAAIDghBDUsqVUtar0zz/Shx+6rgYAAAAgOCEEFS0qXXGFv//GG66rAQAAAAhOCFFXXulfvvuutHOn62oAAAAQ7QhOCElNm0q1a0u7d0szZriuBgAAANGO4ISQFBNzaNTp9dddVwMAAIBoR3BCyOrRw7+0BhF//um6GgAAAEQzghNC1kknSaedJh08KL31lutqAAAAEM0ITghpTNcDAABAKCA4IaR17+6vd/r0U2ndOtfVAAAAIFoRnBDSjjlGOuccf3/yZNfVAAAAIFoRnBDymK4HAAAA1whOCHmXXioVLy4tXy799JPragAAABCNCE4IeUcdJbVr5++/8YbragAAABCNCE4Iu+l6gYDragAAABBtCE4ICx07SqVKSatWSYsXu64GAAAA0YbghLBQurTUpYu/T5MIAAAAFDaCE8Juut6UKdLBg66rAQAAQDQhOCFstGnjN4rYskWaP991NQAAAIgmBCeEDWtJfvnl/j7T9QAAABB1wWn06NGqWbOmSpQooWbNmmlxDlf/T548WTExMeoSXPyCqJmu9/bb0p49rqsBAABAtHAenKZMmaJBgwZp2LBhWrp0qRo2bKi2bdtq69atWT5v9erVuv3229WyZctCqxXutWgh1agh/fuv9P77rqsBAABAtHAenEaMGKF+/fqpb9++ql+/vsaOHatSpUppwoQJmT4nKSlJV111le6//37VqlWrUOuFW0WKHBp1evVV19UAAAAgWjgNTvv379eSJUvUunXrQwUVKeJdX7RoUabPe+CBB1SpUiX95z//yfZr7Nu3T4mJiWk2hLeePf3LDz6Q/vzTdTUAAACIBk6D0/bt273Ro8qVK6e53a5v3rw5w+csXLhQL774osaPH5+jrzF8+HDFx8enbDVsnhfCWoMGUsOG0oED0tSprqsBAABANHA+VS83/v33X/Xq1csLTQkJCTl6zuDBg7Vjx46Ubd26dQVeJwpv1InpegAAACgMxeSQhZ+iRYtqi52YJxW7XqVKlcMe//vvv3tNITp27JhyW3JysndZrFgxrVy5UrVr107znLi4OG9DZLF1TnfeKX3+ubRqlcRSNwAAAETsiFNsbKwaN26sefPmpQlCdr158+aHPb5evXr6/vvvtXz58pStU6dOOu+887x9puFFj2rVpAsu8Pc5pxMAAAAiesTJWCvyPn36qEmTJjrjjDM0cuRI7dq1y+uyZ3r37q3q1at7a5XsPE8NbIFLKuXLl/cu09+O6JiuN3euP13v3nulmBjXFQEAACBSOQ9O3bp107Zt2zR06FCvIUSjRo00e/bslIYRa9eu9TrtAel17Sr17y+tXCktWSI1aeK6IgAAAESqmEAgEFAUsXbk1l3PGkWUK1fOdTk4Qj16SJMnSwMHSiNHuq4GAAAAkZoNGMpBRHTXe+MN6eBB19UAAAAgUhGcENbatJGOPlraulWaM8d1NQAAAIhUBCeEteLFpe7d/X3O6QQAAICCQnBCxEzXmzbNTpLsuhoAAABEIoITwl7TptIJJ0h79kjTp7uuBgAAAJGI4ISwZ+dvCo46MV0PAAAABYHghIhw1VX+pZ0Qd9Mm19UAAAAg0hCcEBFq15aaN5eSk/3zOgEAAAD5ieCEiNGrl3/JdD0AAADkN4ITIsYVV0jFiklLl0o//eS6GgAAAEQSghMixlFHSRdd5O+/8orragAAABBJCE6IKL17H5quZ+udAAAAgPxAcEJEufhiqXx5af16acEC19UAAAAgqoPTunXrtN4+mf5/ixcv1i233KJx48blZ21ArsXFSd26+fsvv+y6GgAAAER1cLryyis1f/58b3/z5s268MILvfB077336oEHHsjvGoE8Tdd76y1p1y7X1QAAACBqg9MPP/ygM844w9t/88031aBBA33xxRd67bXXNHHixPyuEcgVO5+TndfJQtO0aa6rAQAAQNQGpwMHDijO5kRJmjt3rjp16uTt16tXT5s2bcrfCoFciok5NOrEdD0AAAA4C04nn3yyxo4dq88++0xz5sxRu3btvNs3btyoo6wnNOBYz57+5dy50oYNrqsBAABAVAanxx57TM8//7zOPfdc9ejRQw0bNvRunzFjRsoUPsClWrWkli2lQEB67TXX1QAAACDcxQQC9tEy95KSkpSYmKgKFSqk3LZ69WqVKlVKlSpVUqiymuPj47Vjxw6VK1fOdTkoQC+8IPXrJ9Wvb+vy/Cl8AAAAQF6yQZ5GnPbs2aN9+/alhKY1a9Zo5MiRWrlyZUiHJkSXyy/325P/9JO0bJnragAAABDO8hScOnfurJf//6r7f/75R82aNdNTTz2lLl26aMyYMfldI5An8fFSly7+Pk0iAAAAUOjBaenSpWppC0i8c+W8pcqVK3ujThamnnnmmSMqCMhPwe56r79u3SBdVwMAAICoCk67d+9W2bJlvf2PPvpIl1xyiYoUKaIzzzzTC1BAqGjTRrLZo9u2SR9+6LoaAAAARFVwqlOnjqZPn65169bpww8/VBv7dCpp69atNFxASClWTLrqKn+f6XoAAAAo1OA0dOhQ3X777apZs6bXfrx58+Ypo0+nnXZanosBCkKvXv7ljBnS33+7rgYAAABR1Y588+bN2rRpk3cOJ5umZxYvXuyNONWrV0+hinbk0cfe4aee6rckf/556brrXFcEAACAqGhHbqpUqeKNLm3cuFHr16/3brPRp1AOTYhOdv6mYJMIpusBAAAgL/IUnJKTk/XAAw946ey4447ztvLly+vBBx/07gNCja1zsoHRzz+Xfv/ddTUAAACIiuB077336tlnn9Wjjz6qZcuWedsjjzyiUaNGaciQIflfJXCEqlWTWrf291991XU1AAAAiIo1TtWqVdPYsWPVqVOnNLe/++67uvHGG7VhwwaFKtY4Ra/XXpN69pRq1ZJ++82fwgcAAIDolVjQa5z++uuvDNcy2W12HxCKunSRypSRVq2SvvjCdTUAAAAIJ3kKTtZJz6bqpWe3nWrty4AQVLq0dNll/j5NIgAAAFDgU/U++eQTdejQQccee2zKOZwWLVrknRD3gw8+UMuWLRWqmKoX3ebPl84/X4qPt5b6UokSrisCAABAxE7Va9WqlX755Rd17dpV//zzj7ddcskl+vHHH/XKK6/ktW6gwLVqJdWoIe3YIb33nutqAAAAEPEnwM3It99+q9NPP11JSUkKVYw44d57pUcekTp0kGbOdF0NAAAAIvoEuEC4Cp4Md9YsKYQbQAIAACCEEJwQderWlVq0sBM5S5Mmua4GAAAA4YDghKh07bX+5YQJfoACAAAAslJMuWANILJiTSKAcHD55dJ//yv9/rv06afSuee6rggAAAARE5xs4VR29/cOLiABQvycTt27S+PHSy++SHACAABAIXbVCwd01UPQ4sVSs2b+uZw2bZLKl3ddEQAAAAoTXfWAHGjaVGrQQNq7V3r9ddfVAAAAIJQRnBC1YmKk//zH37fpegAAAEBmCE6Iaj17SrGx0tKl0rJlrqsBAABAqCI4IaolJEhduvj7jDoBAAAgMwQnRL3gdL3XXpP27HFdDQAAAEIRwQlRr3Vr6dhj7Txk0rRprqsBAABAKCI4IeoVKSJdc42//8ILrqsBAABAKCI4AZL69vW77M2fL/3+u+tqAAAAEGpCIjiNHj1aNWvWVIkSJdSsWTMttjOTZuKdd95RkyZNVL58eZUuXVqNGjXSK6+8Uqj1IvLYVL0LL/T3X3rJdTUAAAAINc6D05QpUzRo0CANGzZMS5cuVcOGDdW2bVtt3bo1w8dXrFhR9957rxYtWqTvvvtOffv29bYPP/yw0GtHZLn22kPB6eBB19UAAAAglMQEAoGAywJshKlp06Z69tlnvevJycmqUaOGBgwYoLvvvjtHr3H66aerQ4cOevDBB7N9bGJiouLj47Vjxw6VK1fuiOtH5Ni3T6peXfrzT2nmTKlDB9cVAQAAoCDlJhs4HXHav3+/lixZotbW1ixYUJEi3nUbUcqOZb558+Zp5cqVOuecczJ8zL59+7wfSOoNyEhcnNSrl7/POZ0AAAAQMsFp+/btSkpKUuXKldPcbtc3b96c6fMsEZYpU0axsbHeSNOoUaN0YXCBSjrDhw/3UmRws9EsILtzOr33nrRli+tqAAAAECqcr3HKi7Jly2r58uX6+uuv9fDDD3trpBYsWJDhYwcPHuwFreC2bt26Qq8X4aNBA5s+6q9xevll19UAAAAgVBRz+cUTEhJUtGhRbUn3p327XqVKlUyfZ9P56tSp4+1bV70VK1Z4I0vnnnvuYY+Ni4vzNiA3o05ffeVP17v9dr9NOQAAAKKb0xEnm2rXuHFjb51SkDWHsOvNmzfP8evYc2wtE5AfunWTSpWSVq6UvvjCdTUAAAAIBc6n6tk0u/Hjx2vSpEneyFH//v21a9cur8W46d27tzfdLshGlubMmaNVq1Z5j3/qqae88zj17NnT4XeBSGINVSw8mRdecF0NAAAAFO1T9Uy3bt20bds2DR061GsIYVPvZs+endIwYu3atd7UvCALVTfeeKPWr1+vkiVLql69enr11Ve91wHyc7qenc/pzTelp5/2wxQAAACil/PzOBU2zuOEnLDfipNO8qfrjRsn9evnuiIAAABE7XmcgFBlDSGCrck5pxMAAAAITkAmeveWihXzO+z98IPragAAAOASwQnIhC2z69jR32fUCQAAILoRnIAsBKfrvfKKRMd7AACA6EVwArLQtq1Uvbr055/Su++6rgYAAACuEJyALNgap6uv9vfHj3ddDQAAAFwhOAHZuPZav8ve3LnSr7+6rgYAAAAuEJyAbNSsKbVv7+/bOZ0AAAAQfQhOQA7ccIN/+dJL0t69rqsBAABAYSM4ATlw0UVSjRp+k4h33nFdDQAAAAobwQnIgaJF/bVOZuxY19UAAACgsBGcgFyc08kC1GefST/+6LoaAAAAFCaCE5BDdj6nTp38/eefd10NAAAAChPBCchDk4iXX5Z273ZdDQAAAAoLwQnIhdatpVq1pB07pClTXFcDAACAwkJwAnKhSBHp+uv9fZpEAAAARA+CE5BLfftKxYtLixdLS5e6rgYAAACFgeAE5NLRR0uXXebv0yQCAAAgOhCcgDwITtd77TUpMdF1NQAAAChoBCcgD845R6pXT9q1S3r9ddfVAAAAoKARnIA8iIk51JrcmkQEAq4rAgAAQEEiOAF51Lu3VKKE9O230ldfua4GAAAABYngBORRhQpS9+7+/nPPua4GAAAABYngBByB/v39SzsZ7tatrqsBAABAQSE4AUfgjDOkZs2k/ful8eNdVwMAAICCQnACjtCAAYem6x044LoaAAAAFASCE3CELr9cqlxZ2rhRmjbNdTUAAAAoCAQn4AjFxh5qTT5qlOtqAAAAUBAITkA+uP56qVgxaeFCadky19UAAAAgvxGcgHxQtao/Zc8w6gQAABB5CE5APvnvf/3L11+Xtm93XQ0AAADyE8EJyCfWlrxJE2nfPlqTAwAARBqCE5BPYmIOtSYfM0Y6eNB1RQAAAMgvBCcgH3XrJh19tLRunTR9uutqAAAAkF8ITkA+ios71Jr86addVwMAAID8QnAC8ln//lLx4n5r8m++cV0NAAAA8gPBCSiA1uQ2Zc8w6gQAABAZCE5AAbjlFv9yyhRp40bX1QAAAOBIEZyAAtC4sXT22dKBA36HPQAAAIQ3ghNQwKNOY8dKe/e6rgYAAABHguAEFJDOnaXjjpO2b5def911NQAAADgSBCeggBQrduiEuCNHSoGA64oAAACQVwQnoAD95z9S6dLS999L8+e7rgYAAAB5RXACClD58tLVVx8adQIAAEB4IjgBBey///UvZ86UVq50XQ0AAADyguAEFLATT5Q6dfLXOD3xhOtqAAAAkBcEJ6AQ3HWXf/nKK5wQFwAAIBwRnIBCcNZZ/glx9+9nrRMAAEA4IjgBhTzqZCfE/ecf19UAAAAgNwhOQCG56CKpQQPp33/98AQAAIDwERLBafTo0apZs6ZKlCihZs2aafHixZk+dvz48WrZsqUqVKjgba1bt87y8UCoKFJEuvNOf9+m6+3d67oiAAAAhE1wmjJligYNGqRhw4Zp6dKlatiwodq2bautW7dm+PgFCxaoR48emj9/vhYtWqQaNWqoTZs22rBhQ6HXDuRW9+7SscdKW7ZIL7/suhoAAADkVEwgYE2S3bERpqZNm+rZZ5/1ricnJ3thaMCAAbr77ruzfX5SUpI38mTP7927d7aPT0xMVHx8vHbs2KFy5crly/cA5MbTT0u33CLVqSP9/LNUtKjrigAAAKJTYi6ygdMRp/3792vJkiXedLuUgooU8a7baFJO7N69WwcOHFDFihUzvH/fvn3eDyT1Brh07bWSvV1/+02aNs11NQAAAMgJp8Fp+/bt3ohR5cqV09xu1zdv3pyj17jrrrtUrVq1NOErteHDh3spMrjZaBbgUunS0s03+/vDh/snxgUAAEBoc77G6Ug8+uijmjx5sqZNm+Y1lsjI4MGDvaG34LZu3bpCrxNIb8AAP0AtXSrNnu26GgAAAIR0cEpISFDRokW1xVbKp2LXq1SpkuVzn3zySS84ffTRRzr11FMzfVxcXJw3XzH1BriWkCD17+/vP/ggo04AAAChzmlwio2NVePGjTVv3ryU26w5hF1v3rx5ps97/PHH9eCDD2r27Nlq0qRJIVUL5K/bbpNsoNSW882f77oaAAAAhPRUPWtFbudmmjRpklasWKH+/ftr165d6tu3r3e/dcqz6XZBjz32mIYMGaIJEyZ4536ytVC27dy50+F3AeSeDar263do1AkAAAChy3lw6tatmzftbujQoWrUqJGWL1/ujSQFG0asXbtWmzZtSnn8mDFjvG58l112mapWrZqy2WsA4eaOO6Tixe38ZNLCha6rAQAAQMiex6mwcR4nhJrrr5fGjZPatqVRBAAAQGEKm/M4AbCW+v5JcD/8UPr6a9fVAAAAICMEJ8CxWrWknj39/Ycecl0NAAAAMkJwAkKA9T+JiZFmzJC+/dZ1NQAAAEiP4ASEgLp1rVGKv8+oEwAAQOghOAEh4t57/cu33pJ++MF1NQAAAEiN4ASEiAYNpEsv9fcffth1NQAAAEiN4ASEkCFD/MspU6Sff3ZdDQAAAIIITkAIadhQ6tJFsrOrsdYJAAAgdBCcgBAddXr9ddY6AQAAhAqCExBiTj9duvxyf9Qp2DACAAAAbhGcgBD04INS0aL+eZ2++MJ1NQAAACA4ASF6XqdrrvH3777bH30CAACAOwQnIEQNHSqVKCF99pk0e7bragAAAKIbwQkIUcccIw0Y4O8PHiwlJ7uuCAAAIHoRnIAQZtP04uOlb7/1z+0EAAAANwhOQAirWFG6805//3//k/bvd10RAABAdCI4ASFu4ECpcmVp1Spp/HjX1QAAAEQnghMQ4kqX9htFmPvvlxITXVcEAAAQfQhOQBjo10868URp2zbp8cddVwMAABB9CE5AGCheXHr0UX9/xAhpwwbXFQEAAEQXghMQJrp0kVq0kPbsOTR1DwAAAIWD4ASEiZgY6ckn/f2XXpK+/951RQAAANGD4ASEkTPPlC6/XAoEDrUpBwAAQMEjOAFh5pFH/DVPs2dLc+e6rgYAACA6EJyAMFOnjtS/v79/xx1ScrLrigAAACIfwQkIQ0OGSOXKScuXSxMnuq4GAAAg8hGcgDCUkHCos97gwdKOHa4rAgAAiGwEJyBMDRgg1a0rbd0q3X+/62oAAAAiG8EJCFOxsdLTT/v7o0ZJP/3kuiIAAIDIRXACwljbtlLnztLBg9LAgX6bcgAAAOQ/ghMQ5kaMkOLi/Nbk777ruhoAAIDIRHACwlytWtLtt/v7t94q7dnjuiIAAIDIQ3ACIoB11jvmGGn1aunJJ11XAwAAEHkITkAEKF36UGB65BFp1SrXFQEAAEQWghMQIa64Qjr/fGnvXumGG2gUAQAAkJ8ITkCEiImRnn9eKlFCmjNHeu011xUBAABEDoITEEHq1JGGDj3UKGL7dtcVAQAARAaCExBhrMNegwZ+aAp22wMAAMCRITgBEaZ4cWn8eH/q3qRJ0rx5risCAAAIfwQnIAKdeaZ0003+/vXXc24nAACAI0VwAiLUww9L1atLv/8u3Xef62oAAADCG8EJiFDlyknPPefv2zmeFi50XREAAED4IjgBEaxTJ6lPHyk5WerdW/r3X9cVAQAAhCeCExDhnn5aOu446Y8//BblAAAAyD2CExDh4uP97nrWZe/FF6V333VdEQAAQPghOAFRoFWrQ+d06tdP2rLFdUUAAADhheAERIkHH5ROOUXats0PT4GA64oAAADCB8EJiBJxcdKrr0qxsdJ77/nT9gAAAJAzBCcgipx6qn9+J3PLLf45ngAAAJA9ghMQZayznq152rXLb1GelOS6IgAAgNDnPDiNHj1aNWvWVIkSJdSsWTMtXrw408f++OOPuvTSS73Hx8TEaOTIkYVaKxAJihb1u+yVLSt98YX06KOuKwIAAAh9ToPTlClTNGjQIA0bNkxLly5Vw4YN1bZtW23dujXDx+/evVu1atXSo48+qipVqhR6vUCksPM6Pfusvz9smLRwoeuKAAAAQpvT4DRixAj169dPffv2Vf369TV27FiVKlVKEyZMyPDxTZs21RNPPKHu3bsrzla6A8izXr2kq67yp+r16CFt3+66IgAAgNDlLDjt379fS5YsUevWrQ8VU6SId33RokX59nX27dunxMTENBsA/4S4Y8ZIJ54orV8v9ekjJSe7rgoAACA0OQtO27dvV1JSkipXrpzmdru+efPmfPs6w4cPV3x8fMpWo0aNfHttINzZOqc33/RblX/wgXTffa4rAgAACE3Om0MUtMGDB2vHjh0p27p161yXBISUhg2lceMOnSR36lTXFQEAAIQeZ8EpISFBRYsW1ZYtW9Lcbtfzs/GDrYUqV65cmg1AWtaW/Lbb/H2bsrdsmeuKAAAAQouz4BQbG6vGjRtr3rx5KbclJyd715s3b+6qLCBqPfaY1K6dtGeP1Lmz/RHDdUUAAAChw+lUPWtFPn78eE2aNEkrVqxQ//79tWvXLq/Lnundu7c31S51Q4nly5d7m+1v2LDB2//tt98cfhdA5Jzf6Y03/GYRNqP10kvtd851VQAAAKGhmMsv3q1bN23btk1Dhw71GkI0atRIs2fPTmkYsXbtWq/TXtDGjRt12mmnpVx/8sknva1Vq1ZasGCBk+8BiCTly0szZkjNmkmffy7ddJO//sk68AEAAESzmEAgEFAUsXbk1l3PGkWw3gnI2KxZ0sUX++3JR42Sbr7ZdUUAAABus0HEd9UDkHvt20uPP+7v33KLlGopIgAAQFQiOAHI0KBBUq9eUlKSdPnl0o8/uq4IAADAHYITgAzZuiZb33TmmdLff0sXXijRhwUAAEQrghOATJUoIb3/vnTKKdKmTdIFF1jTFtdVAQAAFD6CE4AsVawozZnjtym30GThyUIUAABANCE4AciWnSHAGkTUrOlP17Npe9u3u64KAACg8BCcAOTIMcf44alaNb9RRNu20o4drqsCAAAoHAQnADlWq5Yfno4+Wlq6VLroImnnTtdVAQAAFDyCE4BcqVfPX/NUvrz0xRdS587Snj2uqwIAAChYBCcAudawoTR7tlSmjPTxx1KnTtLu3a6rAgAAKDgEJwB50qyZNGuWH57mzpU6dpR27XJdFQAAQMEgOAHIs7PP9keeypb1R57atJH++st1VQAAAPmP4ATgiLRoIX344aE1Ty1bSuvXu64KAAAgfxGcAByx5s2lzz7zW5X/9JN01lnSihWuqwIAAMg/BCcA+aJBA3/EqW5dad06fxrfokWuqwIAAMgfBCcA+ea446SFC/3GEbbW6YILpFdfdV0VAADAkSM4AchXCQn+SXLbt/fP79Srl3TddZzrCQAAhDeCE4B8V7q09N570tChUkyMNH681KqVtGWL68oAAADyhuAEoEAULSrdf7/fca9iRenrr6Uzz/SbRwAAAIQbghOAAnXhhdKXX0p16kirV/vhafJk11UBAADkDsEJQIE74QS/w94550j//iv16CFde620a5frygAAAHKG4ASgUJtGDBnir3t68UWpSRPpu+9cVwYAAJA9ghOAQlOsmPTAA36AspPl/vyzdMYZ0ogR0sGDrqsDAADIHMEJQKE77zxp+XLpooukffuk226TTj/dPwcUAABAKCI4AXDi6KOlmTP9VuXWde/77/01UHfeKe3f77o6AACAtAhOAJyxtU7WJOKXX6S+faVAQHriCb/znrUvBwAACBUEJwDOHXWUNGGCNG2av79smb/2qXt3adUq19UBAAAQnACEkC5d/Cl7ffr4o1FTpkj16/sNJfbudV0dAACIZgQnACGlalVp4kR/1OmCC/zmEcOGSSefLE2aRPc9AADgBsEJQEhq2FCaM0eaPNlvXW5T9q6+WqpXzw9WBCgAAFCYCE4AQpZN1+vWzW8eYU0jrBPf77/7jSTq1pVeekk6cMB1lQAAIBoQnACEvNKlpdtvl/74Q3rySalSJX8E6ppr/AD14ou0MAcAAAWL4AQgrAKUnSzXAtRTT/kByvatpXnt2tLIkVJiousqAQBAJCI4AQg7pUpJgwb5oWnECL+hxPr10q23SlWq+F35Fi70zwsFAACQHwhOAMI6QFlYsgA1bpx00knSnj3Syy9LLVtKjRv7jSR27nRdKQAACHcEJwBhLy5O6tdP+vFHadEif+peyZJ+S3NrJGFT+qzJhJ1gl/NBAQCAvIgJBKJrMktiYqLi4+O1Y8cOlStXznU5AArIn39K48dLEyZIv/566Hb7te/aVerRQzr/fKl4cZdVAgCAcMkGBCcAEc3+hVu61D8flG22FiooIUG69FJ/NMqm9hUr5rJSAABQ2AhOWSA4AdErOVn6/HM/QE2dKm3bdug+++fAwtN55/mbnYC3aFGX1QIAgIJGcMoCwQmAOXhQ+vhj6c03/bVPf/2V9v4KFaRzzvFDlE3pO/lkqQirQgEAiCgEpywQnACkl5QkLV8uzZ/vb599Jv37b9rH2LS+c889NCJVr54UE+OqYgAAkB8ITlkgOAHIyWjUkiWHgpSdE2r37rSPsfNFWZA66yypUSPp1FOl+HhXFQMAgLwgOGWB4AQgt/bvl77++lCQ+uKLjNua16rlh6jU2zHHMDIFAECoIjhlgeAE4Ejt2yd9+aW0YIHfsc+m+a1dm/FjK1b0R6Nsat+JJx7aatakFToAAK4RnLJAcAJQUOeN+vZbP0QFtxUr/Gl/GbHW57Vrpw1Txx8vHXusVKOGVKpUYX8HAABEn0SCU+YITgAKi03n++kn6YcfpF9+8beVK/0T8u7Zk/VzrRmFhajUmwUqm/pXubK/lS7NNEAAAI4EwSkLBCcAoXA+qQ0bDgWpYKiy6X5r1kg7d+bsdUqWPBSiKlU6tJ/6ugUwmy5oW4kSBf2dAQAQXghOWSA4AQhl9i/yjh1+iApu69Yd2rfAtWXL4V3+csKCUzBE2Xmqgpe2lS3rnwTYLrPabAoho1wAgGjMBsUKrSoAQLYslJQv72/WVCIzu3b5ASr1tnXr4bfZ2qu///ZHuWzq4MaN/pZXdhLg1EHKRr1yu1mAy+h2a5YRG5v2MvU+gQ0A4BLBCQDCkK1vsvbntmXHQpOd0NcC1F9/HboM7ttm96feEhPTXrfpgzYaZq9lI2K2FbaiRbMOVtkFr/T71qDDXjO4pb+e2W0F9djMnm+bBVYLjnaZet8ugxsAIAqC0+jRo/XEE09o8+bNatiwoUaNGqUzzjgj08dPnTpVQ4YM0erVq3XCCSfoscce00UXXVSoNQNAuLAP2HZyXtusDXpeWGCy6YHpQ5U1ubDNRrOC+3nZ7PkHDvjnzLJL29JLSjr0eKQVDE8ZBauc3FbQ97v6mul/Rnm9np+v5fJrhetrF+bX4rUL92t16BBe62+dB6cpU6Zo0KBBGjt2rJo1a6aRI0eqbdu2WrlypSrZ6uZ0vvjiC/Xo0UPDhw/XxRdfrNdff11dunTR0qVL1aBBAyffAwBEOvsgWqaMv1WtWvBfz0a3rJV7+jCV0X5e7rdLC2KpN/t66W/L7PaCfqwF1dz+vIIjggAQLjZvDq/g5Lw5hIWlpk2b6tlnn/WuJycnq0aNGhowYIDuvvvuwx7frVs37dq1SzNnzky57cwzz1SjRo288JUdmkMAAEJdMAQFA1XwevAy9X5Gtx3p/QXxmoX5NdMHyPSfdLK6npvHFuRrUQd1REMd777rNypyKWyaQ+zfv19LlizR4MGDU24rUqSIWrdurUWLFmX4HLvdRqhSsxGq6dOnZ/j4ffv2eVvqHw4AAKHMprIE1zcBAEJDEZdffPv27UpKSlJlO9lIKnbd1jtlxG7PzeNtSp+lyOBmo1kAAAAAEDbBqTDYaJYNvQW3dXZCFAAAAADIBadT9RISElS0aFFtsZONpGLXq1SpkuFz7PbcPD4uLs7bAAAAACAsR5xiY2PVuHFjzZs3L+U2aw5h15s3b57hc+z21I83c+bMyfTxAAAAABD27cit0UOfPn3UpEkT79xN1o7cuub17dvXu793796qXr26t1bJDBw4UK1atdJTTz2lDh06aPLkyfrmm280btw4x98JAAAAgEjlPDhZe/Ft27Zp6NChXoMHays+e/bslAYQa9eu9TrtBZ111lneuZv+97//6Z577vFOgGsd9TiHEwAAAICIPY9TYeM8TgAAAABymw0ivqseAAAAABwpghMAAAAAZIPgBAAAAADZIDgBAAAAQDYITgAAAACQDYITAAAAAGSD4AQAAAAA2SA4AQAAAEA2CE4AAAAAkA2CEwAAAABkg+AEAAAAANkopigTCAS8y8TERNelAAAAAHAomAmCGSErURec/v33X++yRo0arksBAAAAECIZIT4+PsvHxARyEq8iSHJysjZu3KiyZcsqJibGWbK14LZu3TqVK1fOSQ3IfxzXyMRxjUwc18jDMY1MHNfIlBhCx9WikIWmatWqqUiRrFcxRd2Ik/1AjjnmGIUCe6O4frMg/3FcIxPHNTJxXCMPxzQycVwjU7kQOa7ZjTQF0RwCAAAAALJBcAIAAACAbBCcHIiLi9OwYcO8S0QOjmtk4rhGJo5r5OGYRiaOa2SKC9PjGnXNIQAAAAAgtxhxAgAAAIBsEJwAAAAAIBsEJwAAAADIBsEJAAAAALJBcHJg9OjRqlmzpkqUKKFmzZpp8eLFrktCDt13332KiYlJs9WrVy/l/r179+qmm27SUUcdpTJlyujSSy/Vli1bnNaMw3366afq2LGjd5ZwO4bTp09Pc7/1zBk6dKiqVq2qkiVLqnXr1vr111/TPOavv/7SVVdd5Z24r3z58vrPf/6jnTt3FvJ3gtwc16uvvvqw39927dqleQzHNbQMHz5cTZs2VdmyZVWpUiV16dJFK1euTPOYnPy7u3btWnXo0EGlSpXyXueOO+7QwYMHC/m7QW6O67nnnnvY7+sNN9yQ5jEc19AyZswYnXrqqSkntW3evLlmzZoVUb+rBKdCNmXKFA0aNMhrwbh06VI1bNhQbdu21datW12Xhhw6+eSTtWnTppRt4cKFKffdeuuteu+99zR16lR98skn2rhxoy655BKn9eJwu3bt8n737I8YGXn88cf1zDPPaOzYsfrqq69UunRp7/fU/tEPsg/XP/74o+bMmaOZM2d6H9qvu+66QvwukNvjaiwopf79feONN9Lcz3ENLfbvqH3Q+vLLL71jcuDAAbVp08Y71jn9dzcpKcn7ILZ//3598cUXmjRpkiZOnOj9cQShe1xNv3790vy+2r/NQRzX0HPMMcfo0Ucf1ZIlS/TNN9/o/PPPV+fOnb1/UyPmd9XakaPwnHHGGYGbbrop5XpSUlKgWrVqgeHDhzutCzkzbNiwQMOGDTO8759//gkUL148MHXq1JTbVqxYYe3+A4sWLSrEKpEbdnymTZuWcj05OTlQpUqVwBNPPJHm2MbFxQXeeOMN7/pPP/3kPe/rr79OecysWbMCMTExgQ0bNhTyd4CcHFfTp0+fQOfOnTN9Dsc19G3dutU7Rp988kmO/9394IMPAkWKFAls3rw55TFjxowJlCtXLrBv3z4H3wWyO66mVatWgYEDB2b6HI5reKhQoULghRdeiJjfVUacCpElaEvhNu0nqEiRIt71RYsWOa0NOWdTtmwqUK1atby/TtuwsrFja381S318bRrfsccey/ENI3/88Yc2b96c5jjGx8d702qDx9EubRpXkyZNUh5jj7ffZxuhQuhasGCBN/2jbt266t+/v/7888+U+ziuoW/Hjh3eZcWKFXP8765dnnLKKapcuXLKY2wEOTExMeUv4Qit4xr02muvKSEhQQ0aNNDgwYO1e/fulPs4rqEtKSlJkydP9kYRbcpepPyuFnNdQDTZvn2790ZK/YYwdv3nn392Vhdyzj4827CxfeiyaQP333+/WrZsqR9++MH7sB0bG+t98Ep/fO0+hIfgscro9zR4n13ah+/UihUr5v1Pn2Mdumyank0LOf744/X777/rnnvuUfv27b3/WRctWpTjGuKSk5N1yy23qEWLFt4HaZOTf3ftMqPf5+B9CL3jaq688kodd9xx3h8qv/vuO911113eOqh33nnHu5/jGpq+//57LyjZ1HZbxzRt2jTVr19fy5cvj4jfVYITkAv2ISvIFkBakLJ/2N98802viQCA0NW9e/eUffurpv0O165d2xuFuuCCC5zWhuzZmhj7I1XqdaWI3OOaem2h/b5asx77PbU/etjvLUJT3bp1vZBko4hvvfWW+vTp461nihRM1StENtxsf9VM30HErlepUsVZXcg7+8vJiSeeqN9++807hjYd859//knzGI5veAkeq6x+T+0yfUMX6/pjHdk41uHDptvav8v2+2s4rqHr5ptv9pp1zJ8/31uAHpSTf3ftMqPf5+B9CL3jmhH7Q6VJ/fvKcQ09sbGxqlOnjho3bux1T7SGPU8//XTE/K4SnAr5zWRvpHnz5qUZorbrNqyJ8GNtiu2vX/aXMDu2xYsXT3N8bVqBrYHi+IYPm8Zl/0CnPo42v9rWuASPo13aP/42Zzvo448/9n6fg/9zR+hbv369t8bJfn8NxzX0WJ8P+3Bt033sWNjvZ2o5+XfXLm36UOpQbJ3crF2yTSFC6B3XjNgohkn9+8pxDX3Jycnat29f5Pyuuu5OEW0mT57sdeeaOHGi18HpuuuuC5QvXz5NBxGErttuuy2wYMGCwB9//BH4/PPPA61btw4kJCR4HYHMDTfcEDj22GMDH3/8ceCbb74JNG/e3NsQWv7999/AsmXLvM3+GRwxYoS3v2bNGu/+Rx991Pu9fPfddwPfffed14nt+OOPD+zZsyflNdq1axc47bTTAl999VVg4cKFgRNOOCHQo0cPh98Vsjqudt/tt9/udW+y39+5c+cGTj/9dO+47d27N+U1OK6hpX///oH4+Hjv391NmzalbLt37055THb/7h48eDDQoEGDQJs2bQLLly8PzJ49O3D00UcHBg8e7Oi7QnbH9bfffgs88MAD3vG031f7t7hWrVqBc845J+U1OK6h5+677/Y6I9oxs/932nXrSvrRRx9FzO8qwcmBUaNGeW+c2NhYrz35l19+6bok5FC3bt0CVatW9Y5d9erVvev2D3yQfbC+8cYbvfabpUqVCnTt2tX7nwFCy/z5870P1uk3a1cdbEk+ZMiQQOXKlb0/dFxwwQWBlStXpnmNP//80/tAXaZMGa9Vat++fb0P5wjN42ofyOx/xvY/YWuJe9xxxwX69et32B+tOK6hJaPjadtLL72Uq393V69eHWjfvn2gZMmS3h+77I9gBw4ccPAdISfHde3atV5IqlixovdvcJ06dQJ33HFHYMeOHWleh+MaWq655hrv31b7jGT/1tr/O4OhKVJ+V2PsP65HvQAAAAAglLHGCQAAAACyQXACAAAAgGwQnAAAAAAgGwQnAAAAAMgGwQkAAAAAskFwAgAAAIBsEJwAAAAAIBsEJwAAAADIBsEJAIAsxMTEaPr06a7LAAA4RnACAISsq6++2gsu6bd27dq5Lg0AEGWKuS4AAICsWEh66aWX0twWFxfnrB4AQHRixAkAENIsJFWpUiXNVqFCBe8+G30aM2aM2rdvr5IlS6pWrVp666230jz/+++/1/nnn+/df9RRR+m6667Tzp070zxmwoQJOvnkk72vVbVqVd18881p7t++fbu6du2qUqVK6YQTTtCMGTNS7vv777911VVX6eijj/a+ht2fPugBAMIfwQkAENaGDBmiSy+9VN9++60XYLp3764VK1Z49+3atUtt27b1gtbXX3+tqVOnau7cuWmCkQWvm266yQtUFrIsFNWpUyfN17j//vt1xRVX6LvvvtNFF13kfZ2//vor5ev/9NNPmjVrlvd17fUSEhIK+acAAChoMYFAIFDgXwUAgDyucXr11VdVokSJNLffc8893mYjTjfccIMXVoLOPPNMnX766Xruuec0fvx43XXXXVq3bp1Kly7t3f/BBx+oY8eO2rhxoypXrqzq1aurb9++euihhzKswb7G//73Pz344IMpYaxMmTJeULJphJ06dfKCko1aAQAiF2ucAAAh7bzzzksTjEzFihVT9ps3b57mPru+fPlyb99GgBo2bJgSmkyLFi2UnJyslStXeqHIAtQFF1yQZQ2nnnpqyr69Vrly5bR161bvev/+/b0Rr6VLl6pNmzbq0qWLzjrrrCP8rgEAoYbgBAAIaRZU0k+dyy+2Jiknihcvnua6BS4LX8bWV61Zs8YbyZozZ44Xwmzq35NPPlkgNQMA3GCNEwAgrH355ZeHXT/ppJO8fbu0tU82vS7o888/V5EiRVS3bl2VLVtWNWvW1Lx5846oBmsM0adPH29a4ciRIzVu3Lgjej0AQOhhxAkAENL27dunzZs3p7mtWLFiKQ0YrOFDkyZNdPbZZ+u1117T4sWL9eKLL3r3WROHYcOGeaHmvvvu07Zt2zRgwAD16tXLW99k7HZbJ1WpUiVv9Ojff//1wpU9LieGDh2qxo0be135rNaZM2emBDcAQOQgOAEAQtrs2bO9FuGp2WjRzz//nNLxbvLkybrxxhu9x73xxhuqX7++d5+1D//www81cOBANW3a1Ltu65FGjBiR8loWqvbu3av/+7//0+233+4FsssuuyzH9cXGxmrw4MFavXq1N/WvZcuWXj0AgMhCVz0AQNiytUbTpk3zGjIAAFCQWOMEAAAAANkgOAEAAABANljjBAAIW8w2BwAUFkacAAAAACAbBCcAAAAAyAbBCQAAAACyQXACAAAAgGwQnAAAAAAgGwQnAAAAAMgGwQkAAAAAskFwAgAAAABl7f8Bnxrk4L9hjaMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 71.42857142857143%\n",
      "Precision: 0.6363636363636364\n",
      "Recall: 0.7777777777777778\n",
      "F1-Score: 0.7\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "model.eval() \n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:                            # Iterate over each batch (i.e. one patient)\n",
    "        patient_features = data.x                       # Get features (shape: [num_nodes, in_channels])\n",
    "        patient_edges = data.edge_index                 # Get edges (shape: [2, num_edges])\n",
    "        patient_label = data.y.float()                  # Get label (shape: [1])\n",
    "\n",
    "        # Ensure correct format\n",
    "        patient_features = patient_features.float()    \n",
    "        patient_edges = patient_edges.to(torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(patient_features, patient_edges, data.batch)  # Use the batch info to aggregate across nodes\n",
    "\n",
    "        # Apply sigmoid to the output logits and get the predicted class (0 or 1)\n",
    "        pred = torch.sigmoid(output.squeeze())\n",
    "        predicted_class = (pred >= 0.5).float()                     # Threshold at 0.5 to classify as 0 or 1\n",
    "        \n",
    "        # Collect the labels and predictions for metrics\n",
    "        all_labels.append(patient_label.cpu().numpy())\n",
    "        all_predictions.append(predicted_class.cpu().numpy())\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct += (predicted_class == patient_label).sum().item()\n",
    "        total += patient_label.size(0)  # Increment by the number of samples in this batch\n",
    "\n",
    "# Accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy}%\")\n",
    "\n",
    "# Calculate Metrics\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "Test classification with clinical and image embeddings only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)             # Binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Clinical-Only Model\n",
      "Train Features:  torch.Size([84, 4864])\n",
      "Test Features:  torch.Size([21, 4864])\n",
      "Train Labels:  torch.Size([84, 1])\n",
      "Test Labels:  torch.Size([21, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pascal/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.7977571657725743\n",
      "Epoch 2/100, Loss: 0.659267211377266\n",
      "Epoch 3/100, Loss: 0.6246952010052544\n",
      "Epoch 4/100, Loss: 0.6372317419431749\n",
      "Epoch 5/100, Loss: 0.6029512291951549\n",
      "Epoch 6/100, Loss: 0.5547081781551242\n",
      "Epoch 7/100, Loss: 0.5722527677814165\n",
      "Epoch 8/100, Loss: 0.5300398470966944\n",
      "Epoch 9/100, Loss: 0.5182685316872916\n",
      "Epoch 10/100, Loss: 0.5467858914330247\n",
      "Epoch 11/100, Loss: 0.48113278838545304\n",
      "Epoch 12/100, Loss: 0.4783707143755497\n",
      "Epoch 13/100, Loss: 0.4506806438190064\n",
      "Epoch 14/100, Loss: 0.4620192650195566\n",
      "Epoch 15/100, Loss: 0.473275496995484\n",
      "Epoch 16/100, Loss: 0.44333283490102204\n",
      "Epoch 17/100, Loss: 0.4072700066469787\n",
      "Epoch 18/100, Loss: 0.4447579072959256\n",
      "Epoch 19/100, Loss: 0.41493307881449126\n",
      "Epoch 20/100, Loss: 0.3947565267028819\n",
      "Epoch 21/100, Loss: 0.34699716952707693\n",
      "Epoch 22/100, Loss: 0.4108151936126801\n",
      "Epoch 23/100, Loss: 0.3370890647273059\n",
      "Epoch 24/100, Loss: 0.3646644712416457\n",
      "Epoch 25/100, Loss: 0.3285990756121789\n",
      "Epoch 26/100, Loss: 0.256508462204974\n",
      "Epoch 27/100, Loss: 0.2923878432615571\n",
      "Epoch 28/100, Loss: 0.2828457249892641\n",
      "Epoch 29/100, Loss: 0.38061694757385295\n",
      "Epoch 30/100, Loss: 0.31358787797491805\n",
      "Epoch 31/100, Loss: 0.33180522604584983\n",
      "Epoch 32/100, Loss: 0.27740864941842164\n",
      "Epoch 33/100, Loss: 0.26363584438557347\n",
      "Epoch 34/100, Loss: 0.25683895456009914\n",
      "Epoch 35/100, Loss: 0.23121145393286366\n",
      "Epoch 36/100, Loss: 0.25830944753135465\n",
      "Epoch 37/100, Loss: 0.2176855239958968\n",
      "Epoch 38/100, Loss: 0.2665926142518249\n",
      "Epoch 39/100, Loss: 0.2461413161093636\n",
      "Epoch 40/100, Loss: 0.22031065941240377\n",
      "Epoch 41/100, Loss: 0.20382373418500843\n",
      "Epoch 42/100, Loss: 0.2332014121416034\n",
      "Epoch 43/100, Loss: 0.2756038404095029\n",
      "Epoch 44/100, Loss: 0.2083418641715873\n",
      "Epoch 45/100, Loss: 0.18111108208917068\n",
      "Epoch 46/100, Loss: 0.17213723960769503\n",
      "Epoch 47/100, Loss: 0.15870182236512317\n",
      "Epoch 48/100, Loss: 0.17392133185190264\n",
      "Epoch 49/100, Loss: 0.13163592719398107\n",
      "Epoch 50/100, Loss: 0.22384678479629574\n",
      "Epoch 51/100, Loss: 0.17548405810593218\n",
      "Epoch 52/100, Loss: 0.14293580322634233\n",
      "Epoch 53/100, Loss: 0.191634333961338\n",
      "Epoch 54/100, Loss: 0.12049298162128794\n",
      "Epoch 55/100, Loss: 0.20103193467665875\n",
      "Epoch 56/100, Loss: 0.14905665175816382\n",
      "Epoch 57/100, Loss: 0.14149871775716774\n",
      "Epoch 58/100, Loss: 0.15074830873170494\n",
      "Epoch 59/100, Loss: 0.13040315059687319\n",
      "Epoch 60/100, Loss: 0.151849585607354\n",
      "Epoch 61/100, Loss: 0.14227433388479532\n",
      "Epoch 62/100, Loss: 0.1355153914193161\n",
      "Epoch 63/100, Loss: 0.13391707475805215\n",
      "Epoch 64/100, Loss: 0.172760426690668\n",
      "Epoch 65/100, Loss: 0.09482476936408395\n",
      "Epoch 66/100, Loss: 0.16983544562082337\n",
      "Epoch 67/100, Loss: 0.15083754499806148\n",
      "Epoch 68/100, Loss: 0.06738894497307445\n",
      "Epoch 69/100, Loss: 0.07047014363567283\n",
      "Epoch 70/100, Loss: 0.09171264741917422\n",
      "Epoch 71/100, Loss: 0.08886016876343875\n",
      "Epoch 72/100, Loss: 0.12491683040764201\n",
      "Epoch 73/100, Loss: 0.10234265745491944\n",
      "Epoch 74/100, Loss: 0.08448652958747624\n",
      "Epoch 75/100, Loss: 0.09314127983580331\n",
      "Epoch 76/100, Loss: 0.07136192921740032\n",
      "Epoch 77/100, Loss: 0.0649842416376948\n",
      "Epoch 78/100, Loss: 0.10515563820866676\n",
      "Epoch 79/100, Loss: 0.12156502331555126\n",
      "Epoch 80/100, Loss: 0.09929184520420653\n",
      "Epoch 81/100, Loss: 0.1391520007201625\n",
      "Epoch 82/100, Loss: 0.08487733899075947\n",
      "Epoch 83/100, Loss: 0.07287550665809238\n",
      "Epoch 84/100, Loss: 0.11872298982029243\n",
      "Epoch 85/100, Loss: 0.06844057811213743\n",
      "Epoch 86/100, Loss: 0.08512970420085338\n",
      "Epoch 87/100, Loss: 0.03516186220337149\n",
      "Epoch 88/100, Loss: 0.09387017179847608\n",
      "Epoch 89/100, Loss: 0.06909170448608316\n",
      "Epoch 90/100, Loss: 0.06583891760694323\n",
      "Epoch 91/100, Loss: 0.06833160554261236\n",
      "Epoch 92/100, Loss: 0.04218222851298653\n",
      "Epoch 93/100, Loss: 0.04363233474807439\n",
      "Epoch 94/100, Loss: 0.043513776480731604\n",
      "Epoch 95/100, Loss: 0.07986057146462915\n",
      "Epoch 96/100, Loss: 0.05523267897396243\n",
      "Epoch 97/100, Loss: 0.0420085432948371\n",
      "Epoch 98/100, Loss: 0.0387420279582401\n",
      "Epoch 99/100, Loss: 0.07363812101889929\n",
      "Epoch 100/100, Loss: 0.065351464881127\n",
      "Clinical-Only Model - Precision: 0.7000, Recall: 0.7778, F1-Score: 0.7368\n",
      "Test Accuracy: 76.19047619047619%\n",
      "Precision: 0.7\n",
      "Recall: 0.7777777777777778\n",
      "F1-Score: 0.7368421052631579\n",
      "\n",
      "Training Image-Only Model\n",
      "Train Features:  torch.Size([84, 4608])\n",
      "Test Features:  torch.Size([21, 4608])\n",
      "Train Labels:  torch.Size([84, 1])\n",
      "Test Labels:  torch.Size([21, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vn/9n4mdvbj54lbr8ffl0577zgw0000gn/T/ipykernel_64356/1274329738.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_features = torch.tensor(feature_set.reshape(len(feature_set), -1))\n",
      "/var/folders/vn/9n4mdvbj54lbr8ffl0577zgw0000gn/T/ipykernel_64356/1274329738.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_features = torch.tensor((test_clinical_embeddings if modality == 'Clinical' else test_image_features).reshape(len(test_labels), -1))\n",
      "/opt/anaconda3/envs/pascal/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.6919646149589902\n",
      "Epoch 2/100, Loss: 0.6904212534427643\n",
      "Epoch 3/100, Loss: 0.6893700361251831\n",
      "Epoch 4/100, Loss: 0.6765212221514612\n",
      "Epoch 5/100, Loss: 0.6788055744199526\n",
      "Epoch 6/100, Loss: 0.6776932396349453\n",
      "Epoch 7/100, Loss: 0.6867202165580931\n",
      "Epoch 8/100, Loss: 0.6688961968535468\n",
      "Epoch 9/100, Loss: 0.6766731586484682\n",
      "Epoch 10/100, Loss: 0.6924322183643069\n",
      "Epoch 11/100, Loss: 0.6787276856956028\n",
      "Epoch 12/100, Loss: 0.6726928225585392\n",
      "Epoch 13/100, Loss: 0.6762829227816491\n",
      "Epoch 14/100, Loss: 0.6789655692520595\n",
      "Epoch 15/100, Loss: 0.680005078514417\n",
      "Epoch 16/100, Loss: 0.6856783412042118\n",
      "Epoch 17/100, Loss: 0.6558653013337226\n",
      "Epoch 18/100, Loss: 0.6790824493482\n",
      "Epoch 19/100, Loss: 0.6674521302893048\n",
      "Epoch 20/100, Loss: 0.6703120306843803\n",
      "Epoch 21/100, Loss: 0.6799657213545981\n",
      "Epoch 22/100, Loss: 0.6713446378707886\n",
      "Epoch 23/100, Loss: 0.6597807215792793\n",
      "Epoch 24/100, Loss: 0.6644426548764819\n",
      "Epoch 25/100, Loss: 0.667571179213978\n",
      "Epoch 26/100, Loss: 0.6575485095381737\n",
      "Epoch 27/100, Loss: 0.655199470264571\n",
      "Epoch 28/100, Loss: 0.6529576079476447\n",
      "Epoch 29/100, Loss: 0.6455857558619409\n",
      "Epoch 30/100, Loss: 0.6589317651731628\n",
      "Epoch 31/100, Loss: 0.6506559710417475\n",
      "Epoch 32/100, Loss: 0.6563702974291075\n",
      "Epoch 33/100, Loss: 0.6618875765374729\n",
      "Epoch 34/100, Loss: 0.6355894248755205\n",
      "Epoch 35/100, Loss: 0.6546330198290802\n",
      "Epoch 36/100, Loss: 0.6475625864806629\n",
      "Epoch 37/100, Loss: 0.6562413864192509\n",
      "Epoch 38/100, Loss: 0.6501562316857633\n",
      "Epoch 39/100, Loss: 0.6393090454595429\n",
      "Epoch 40/100, Loss: 0.6370556503534317\n",
      "Epoch 41/100, Loss: 0.6374342329800129\n",
      "Epoch 42/100, Loss: 0.6489512090172086\n",
      "Epoch 43/100, Loss: 0.6454169397198019\n",
      "Epoch 44/100, Loss: 0.6384320785956723\n",
      "Epoch 45/100, Loss: 0.6290520378166721\n",
      "Epoch 46/100, Loss: 0.6179338954389095\n",
      "Epoch 47/100, Loss: 0.6224455714580559\n",
      "Epoch 48/100, Loss: 0.598374219345195\n",
      "Epoch 49/100, Loss: 0.6502540748388994\n",
      "Epoch 50/100, Loss: 0.6298604619999727\n",
      "Epoch 51/100, Loss: 0.6428652203508786\n",
      "Epoch 52/100, Loss: 0.6153446689602875\n",
      "Epoch 53/100, Loss: 0.6496462261392957\n",
      "Epoch 54/100, Loss: 0.625267034130437\n",
      "Epoch 55/100, Loss: 0.6304894091472739\n",
      "Epoch 56/100, Loss: 0.6339075240705695\n",
      "Epoch 57/100, Loss: 0.6078318529540584\n",
      "Epoch 58/100, Loss: 0.6195128496204104\n",
      "Epoch 59/100, Loss: 0.5946766691548484\n",
      "Epoch 60/100, Loss: 0.6284462173602411\n",
      "Epoch 61/100, Loss: 0.6029927615253698\n",
      "Epoch 62/100, Loss: 0.6075499002777395\n",
      "Epoch 63/100, Loss: 0.5849514859063285\n",
      "Epoch 64/100, Loss: 0.6343340777925083\n",
      "Epoch 65/100, Loss: 0.6083938690523306\n",
      "Epoch 66/100, Loss: 0.6030489335812274\n",
      "Epoch 67/100, Loss: 0.6116257203476769\n",
      "Epoch 68/100, Loss: 0.6177863255143166\n",
      "Epoch 69/100, Loss: 0.5989952965506485\n",
      "Epoch 70/100, Loss: 0.6195117374438615\n",
      "Epoch 71/100, Loss: 0.5739970317199117\n",
      "Epoch 72/100, Loss: 0.5859041178510302\n",
      "Epoch 73/100, Loss: 0.5940377630648159\n",
      "Epoch 74/100, Loss: 0.5640419002267576\n",
      "Epoch 75/100, Loss: 0.5853811330383732\n",
      "Epoch 76/100, Loss: 0.5714670536773545\n",
      "Epoch 77/100, Loss: 0.610930835118606\n",
      "Epoch 78/100, Loss: 0.5832210737502291\n",
      "Epoch 79/100, Loss: 0.6060949611433205\n",
      "Epoch 80/100, Loss: 0.598419843152875\n",
      "Epoch 81/100, Loss: 0.5658472303655886\n",
      "Epoch 82/100, Loss: 0.5708044807293585\n",
      "Epoch 83/100, Loss: 0.5660014670519602\n",
      "Epoch 84/100, Loss: 0.5892770327627659\n",
      "Epoch 85/100, Loss: 0.5904226375832444\n",
      "Epoch 86/100, Loss: 0.5692899072143648\n",
      "Epoch 87/100, Loss: 0.5697364736170996\n",
      "Epoch 88/100, Loss: 0.5743452346928063\n",
      "Epoch 89/100, Loss: 0.5441494917585736\n",
      "Epoch 90/100, Loss: 0.5655939029973178\n",
      "Epoch 91/100, Loss: 0.5679911417620522\n",
      "Epoch 92/100, Loss: 0.5374727399487581\n",
      "Epoch 93/100, Loss: 0.5609095789314735\n",
      "Epoch 94/100, Loss: 0.5315282860593427\n",
      "Epoch 95/100, Loss: 0.5472781102483472\n",
      "Epoch 96/100, Loss: 0.5512933685843434\n",
      "Epoch 97/100, Loss: 0.5631494534512361\n",
      "Epoch 98/100, Loss: 0.5555544099548743\n",
      "Epoch 99/100, Loss: 0.5353521209077111\n",
      "Epoch 100/100, Loss: 0.5697198364706266\n",
      "Image-Only Model - Precision: 0.7500, Recall: 0.3333, F1-Score: 0.4615\n",
      "Test Accuracy: 66.66666666666667%\n",
      "Precision: 0.75\n",
      "Recall: 0.3333333333333333\n",
      "F1-Score: 0.46153846153846156\n"
     ]
    }
   ],
   "source": [
    "# Experiment: Train Clinical-only and Image-only Models\n",
    "for modality, feature_set in [('Clinical', train_clinical_embeddings), ('Image', train_image_features)]:\n",
    "    print(f\"\\nTraining {modality}-Only Model\")\n",
    "    \n",
    "    train_labels = train_labels.clone().detach().float().view(-1, 1)\n",
    "    test_labels = test_labels.clone().detach().float().view(-1, 1)\n",
    "    train_features = torch.tensor(feature_set.reshape(len(feature_set), -1))\n",
    "    test_features = torch.tensor((test_clinical_embeddings if modality == 'Clinical' else test_image_features).reshape(len(test_labels), -1))\n",
    "\n",
    "    print(\"Train Features: \", train_features.shape)\n",
    "    print(\"Test Features: \", test_features.shape)\n",
    "    print(\"Train Labels: \", train_labels.shape)\n",
    "    print(\"Test Labels: \", test_labels.shape)\n",
    "    \n",
    "    train_dataset = TensorDataset(train_features, train_labels)\n",
    "    test_dataset = TensorDataset(test_features, test_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    model = MLP(input_dim=train_features.shape[1])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=w_decay)\n",
    "    \n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(features.float())\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(output.view(-1), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_labels, all_predictions = [], []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            output = model(features.float())\n",
    "\n",
    "            pred = torch.sigmoid(output.squeeze()) >= 0.5\n",
    "            \n",
    "            all_labels.append(labels.cpu().numpy().flatten())\n",
    "            all_predictions.append(pred.cpu().numpy().flatten())\n",
    "\n",
    "            # Count correct predictions\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)  # Increment by the number of samples in this batch\n",
    "    \n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    print(f\"{modality}-Only Model - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy}%\")\n",
    "\n",
    "    # Calculate Metrics\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pascal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
