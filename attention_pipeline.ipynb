{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# At the beginning of your notebook, add:\n",
    "import torch\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Check for GPU availability and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Optional: Set memory usage behavior for CUDA if available\n",
    "if torch.cuda.is_available():\n",
    "    # Optional: for better performance on some systems\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA Memory Cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to imports\n",
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a benchmarking wrapper\n",
    "def benchmark_function(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Start timing and power measurement\n",
    "        start_time = time.time()\n",
    "        start_cpu_percent = psutil.cpu_percent(interval=None)\n",
    "        \n",
    "        # Execute the function\n",
    "        result = func(*args, **kwargs)\n",
    "\n",
    "# Enhanced benchmarking wrapper\n",
    "def benchmark_function(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Initialize power measurement if GPU is available\n",
    "        gpu_power_samples = []\n",
    "        gpu_power_interval = 0.1  # seconds between power samples\n",
    "        \n",
    "        if torch.cuda.is_available() and PYNVML_AVAILABLE:\n",
    "            try:\n",
    "                pynvml.nvmlInit()\n",
    "                device_count = pynvml.nvmlDeviceGetCount()\n",
    "                handles = [pynvml.nvmlDeviceGetHandleByIndex(i) for i in range(device_count)]\n",
    "                \n",
    "                # Start power sampling in a separate thread\n",
    "                import threading\n",
    "                import time\n",
    "                \n",
    "                def sample_gpu_power():\n",
    "                    while not stop_sampling.is_set():\n",
    "                        try:\n",
    "                            powers = [pynvml.nvmlDeviceGetPowerUsage(h) / 1000.0 for h in handles]  # Convert mW to W\n",
    "                            gpu_power_samples.append(sum(powers))  # Total power across all GPUs\n",
    "                            time.sleep(gpu_power_interval)\n",
    "                        except:\n",
    "                            break\n",
    "                \n",
    "                stop_sampling = threading.Event()\n",
    "                power_thread = threading.Thread(target=sample_gpu_power)\n",
    "                power_thread.daemon = True\n",
    "                power_thread.start()\n",
    "            except:\n",
    "                print(\"Failed to initialize NVML for GPU power measurement\")\n",
    "        \n",
    "        # Start timing and CPU measurement\n",
    "        start_time = time.time()\n",
    "        start_cpu_percent = psutil.cpu_percent(interval=None)\n",
    "        \n",
    "        # Memory before execution\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            start_gpu_mem = torch.cuda.memory_allocated() / 1e9  # GB\n",
    "        \n",
    "        start_ram = psutil.virtual_memory().used / 1e9  # GB\n",
    "        \n",
    "        # Execute the function\n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        # Stop timing and calculate metrics\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        end_cpu_percent = psutil.cpu_percent(interval=None)\n",
    "        \n",
    "        # Memory after execution\n",
    "        end_ram = psutil.virtual_memory().used / 1e9  # GB\n",
    "        ram_used = end_ram - start_ram\n",
    "        \n",
    "        # GPU metrics\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory_peak = torch.cuda.max_memory_allocated() / 1e9  # GB\n",
    "            gpu_memory_used = gpu_memory_peak - start_gpu_mem\n",
    "            print(f\"GPU Memory Peak: {gpu_memory_peak:.2f} GB\")\n",
    "            print(f\"GPU Memory Used: {gpu_memory_used:.2f} GB\")\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        # Stop power sampling if active\n",
    "        if 'stop_sampling' in locals():\n",
    "            stop_sampling.set()\n",
    "            if 'power_thread' in locals():\n",
    "                power_thread.join(timeout=1.0)\n",
    "            \n",
    "            if gpu_power_samples:\n",
    "                avg_power = sum(gpu_power_samples) / len(gpu_power_samples) if gpu_power_samples else 0\n",
    "                max_power = max(gpu_power_samples) if gpu_power_samples else 0\n",
    "                print(f\"GPU Avg Power: {avg_power:.2f} W\")\n",
    "                print(f\"GPU Max Power: {max_power:.2f} W\")\n",
    "                print(f\"GPU Energy Used: {avg_power * elapsed_time:.2f} J\")\n",
    "        \n",
    "        # CPU usage\n",
    "        cpu_percent = (start_cpu_percent + end_cpu_percent) / 2\n",
    "        ram_percent = psutil.virtual_memory().percent\n",
    "        \n",
    "        print(f\"\\nPerformance Metrics:\")\n",
    "        print(f\"Execution Time: {elapsed_time:.2f} seconds\")\n",
    "        print(f\"CPU Usage: {cpu_percent:.2f}%\")\n",
    "        print(f\"RAM Usage: {ram_percent:.2f}% (Used: {ram_used:.2f} GB)\")\n",
    "        \n",
    "        # System info\n",
    "        print(f\"\\nSystem Information:\")\n",
    "        print(f\"OS: {platform.system()} {platform.version()}\")\n",
    "        print(f\"CPU: {platform.processor()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        \n",
    "        return result, elapsed_time\n",
    "    \n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, GATConv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "n_clinical = 38 \n",
    "n_image_nodes = 6*6\n",
    "n_nodes = n_clinical + n_image_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples:  84\n",
      "Test Samples:  21\n",
      "Train labels shape: torch.Size([84])\n",
      "Test labels shape: torch.Size([21])\n"
     ]
    }
   ],
   "source": [
    "# Load Ground-Truth Values\n",
    "train_labels = pd.read_csv(\"data/labels/train_labels.csv\")\n",
    "train_labels = train_labels.iloc[:, 1].tolist()                 # (n_train,)\n",
    "test_labels = pd.read_csv(\"data/labels/test_labels.csv\")\n",
    "test_labels = test_labels.iloc[:, 1].tolist()                   # (n_test,)\n",
    "\n",
    "n_train = len(train_labels) # 84\n",
    "n_test = len(test_labels)   # 21\n",
    "\n",
    "print('Training Samples: ', n_train)\n",
    "print('Test Samples: ', n_test)\n",
    "\n",
    "# Convert to tensors\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "print(\"Train labels shape:\", train_labels.shape)                # Should be (n_train,)\n",
    "print(\"Test labels shape:\", test_labels.shape)                  # Should be (n_test,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Image Embeddings:  (84, 6, 6, 128)\n",
      "Train Clinical Embeddings:  (84, 38, 128)\n",
      "Test Image Embeddings:  (21, 6, 6, 128)\n",
      "Test Clinical Embeddings:  (21, 38, 128)\n"
     ]
    }
   ],
   "source": [
    "# Load and normalise Embeddings\n",
    "train_image_embeddings = np.load(\"data/image_data/train_image_embeddings.npy\")             # (n_train, 6, 6, embed_dim)\n",
    "train_clinical_embeddings = np.load(\"data/clinical_data/train_embeddings.npy\")          # (n_train, 38, embed_dim)\n",
    "test_image_embeddings = np.load(\"data/image_data/test_image_embeddings.npy\")               # (n_test, 6, 6, embed_dim)\n",
    "test_clinical_embeddings = np.load(\"data/clinical_data/test_embeddings.npy\")            # (n_test, 38, embed_dim)\n",
    "\n",
    "print(\"Train Image Embeddings: \", train_image_embeddings.shape)\n",
    "print(\"Train Clinical Embeddings: \", train_clinical_embeddings.shape)\n",
    "print(\"Test Image Embeddings: \",test_image_embeddings.shape)\n",
    "print(\"Test Clinical Embeddings: \", test_clinical_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureAttention(torch.nn.Module):\n",
    "    def __init__(self, clinical_dim, image_dim):\n",
    "        super(FeatureAttention, self).__init__()\n",
    "        self.attn_layer = torch.nn.Linear(clinical_dim + image_dim, 2)\n",
    "\n",
    "    def forward(self, clinical, image):\n",
    "        # Flatten features to (batch_size, clinical_dim + image_dim) for attention scoring\n",
    "        clinical_flat = clinical.mean(dim=-1)       # Shape: (batch_size, clinical_dim)\n",
    "        image_flat = image.mean(dim=-1)             # Shape: (batch_size, image_dim)\n",
    "\n",
    "        combined = torch.cat([clinical_flat, image_flat], dim=1)                        # Shape: (batch_size, clinical_dim + image_dim)\n",
    "        attn_weights = torch.softmax(self.attn_layer(combined), dim=1)                  # Learn weight for each feature type\n",
    "\n",
    "        # Expand attention weights and apply to original features\n",
    "        attn_clinical = attn_weights[:, 0].unsqueeze(1).unsqueeze(-1) * clinical        # Shape: (batch, clinical_dim, 128)\n",
    "        attn_image = attn_weights[:, 1].unsqueeze(1).unsqueeze(-1) * image              # Shape: (batch, image_dim, 128)\n",
    "\n",
    "        return torch.cat([attn_clinical, attn_image], dim=1), attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_attention = FeatureAttention(n_clinical, n_image_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Train Image Embeddings:  torch.Size([84, 36, 128])\n",
      "Combined Train Embeddings:  torch.Size([84, 74, 128])\n",
      "Reshaped Test Image Embeddings:  torch.Size([21, 36, 128])\n",
      "Combined Test Embeddings:  torch.Size([21, 74, 128])\n",
      "Features Attention Weights\n",
      "Train Attention Weights:  tensor([[0.5333, 0.4667],\n",
      "        [0.5200, 0.4800],\n",
      "        [0.5192, 0.4808],\n",
      "        [0.5347, 0.4653],\n",
      "        [0.5174, 0.4826]], grad_fn=<SliceBackward0>)\n",
      "Test Attention Weights:  tensor([[0.5187, 0.4813],\n",
      "        [0.5261, 0.4739],\n",
      "        [0.5313, 0.4687],\n",
      "        [0.5275, 0.4725],\n",
      "        [0.5227, 0.4773]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Reshape image embeddings to match size of clinical embeddings\n",
    "train_image_features = torch.tensor(train_image_embeddings.reshape(n_train, 36, embed_dim))                             # Shape: [n_train, 36, embed_dim]\n",
    "test_image_features = torch.tensor(test_image_embeddings.reshape(n_test, 36, embed_dim))                                # Shape: [n_test, 36, embed_dim]\n",
    "\n",
    "# Feature Attention \n",
    "train_patient_features, train_att_weights = feature_attention(torch.tensor(train_clinical_embeddings).float(), train_image_features.float())        # Shape: [n_train, 74, embed_dim]\n",
    "test_patient_features, test_att_weights = feature_attention(torch.tensor(test_clinical_embeddings).float(), test_image_features.float())            # Shape: [n_test, 74, embed_dim]\n",
    "\n",
    "print('Reshaped Train Image Embeddings: ', train_image_features.shape)\n",
    "print('Combined Train Embeddings: ', train_patient_features.shape)\n",
    "print('Reshaped Test Image Embeddings: ', test_image_features.shape)\n",
    "print('Combined Test Embeddings: ', test_patient_features.shape)\n",
    "\n",
    "print('Features Attention Weights')\n",
    "print('Train Attention Weights: ', train_att_weights[:5])\n",
    "print('Test Attention Weights: ', test_att_weights[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patient_edges(n_clinical, n_nodes):\n",
    "    \"\"\"\n",
    "    Creates bidirectional edges between clinical nodes and image nodes.\n",
    "    Adds a self-edge to each node.\n",
    "\n",
    "    Total edges = n_nodes (self-edges) + 2 * n_clinical * n_image_nodes (bidirectional edges)\n",
    "\n",
    "    Parameters:\n",
    "    - n_clinical: number of clinical nodes (for a specific patient)\n",
    "    - n_image_nodes: number of image nodes (for a specific patient)\n",
    "    \"\"\"\n",
    "    node_ids = np.expand_dims(np.arange(n_nodes, dtype=int), 0)\n",
    "    # self-edges = preserves some features of each own node during a graph convolution\n",
    "    self_edges = np.concatenate((node_ids, node_ids), 0)\n",
    "\n",
    "    # clinical nodes\n",
    "    c_array_asc = np.expand_dims(np.arange(n_clinical), 0)\n",
    "    all_edges = self_edges[:]\n",
    "\n",
    "    for i in range(n_clinical, n_nodes):\n",
    "        # image nodes\n",
    "        i_array = np.expand_dims(np.array([i]*n_clinical), 0)\n",
    "\n",
    "        # image --> clinical\n",
    "        inter_edges_ic = np.concatenate((i_array, c_array_asc), 0)\n",
    "        # clinical --> image\n",
    "        inter_edges_ci = np.concatenate((c_array_asc, i_array), 0)\n",
    "\n",
    "        # bidirectional edges\n",
    "        inter_edges_i = np.concatenate((inter_edges_ic, inter_edges_ci), 1)\n",
    "        all_edges = np.concatenate((all_edges, inter_edges_i), 1)\n",
    "\n",
    "    return torch.tensor(all_edges, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_list(patient_features, patient_labels):\n",
    "    \"\"\"\n",
    "    Generates a sub-graph for each patient given its embeddings\n",
    "\n",
    "    Parameters:\n",
    "    - patient_features: combined clinical and image embeddings of one patient\n",
    "    - patient_labels: groud truth values\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for i in range(len(patient_labels)):\n",
    "        # Create the graph for each patient\n",
    "        patient_edges = create_patient_edges(n_clinical, n_nodes)   # Shape: [2, num_edges]\n",
    "        patient_y = patient_labels[i]                               # Target label for this patient\n",
    "\n",
    "        data = Data(x=patient_features[i], edge_index=patient_edges, y=patient_y)\n",
    "        data_list.append(data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Patients:  84\n",
      "Test Patients:  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pascal/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_data_list = get_data_list(train_patient_features, train_labels)\n",
    "test_data_list = get_data_list(test_patient_features, test_labels)\n",
    "\n",
    "# Batch size 1 for individual patients\n",
    "train_loader = DataLoader(train_data_list, batch_size=1, shuffle=False, num_workers=0)  \n",
    "test_loader = DataLoader(test_data_list, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Train Patients: \", len(train_loader))\n",
    "print(\"Test Patients: \", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "We define the Graph Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)          # Second GCN layer\n",
    "        self.fc = torch.nn.Linear(hidden_channels, 1)                   # Fully connected layer for binary classification\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Apply graph convolution\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Global pooling (mean) across all nodes\n",
    "        x = global_mean_pool(x, batch)  # This will aggregate node features into one scalar per graph\n",
    "        \n",
    "        # Pass the aggregated feature through a fully connected layer to get a single logit\n",
    "        x = self.fc(x)  # Output size is (batch_size, 1)\n",
    "        return x  # Output a single logit for each patient (before applying sigmoid in loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Graph Attention Network\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, heads=2, dropout=0.5):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, dropout=dropout)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, 1)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)          # Aggregate node features\n",
    "        x = self.fc(x)                          # Binary classification output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Model Parameters\n",
    "learning_rate = 0.0001\n",
    "w_decay = 5e-4\n",
    "hidden_channels = 128\n",
    "\n",
    "# Initialize Model\n",
    "model = GCN(in_channels=embed_dim, hidden_channels=hidden_channels)\n",
    "# model = GAT(in_channels=embed_dim, hidden_channels=hidden_channels)\n",
    "\n",
    "# Move data to device during training loop:\n",
    "for data in train_loader:\n",
    "    data = data.to(device)  # Move batch to GPU\n",
    "    patient_features = data.x\n",
    "    patient_edges = data.edge_index\n",
    "    patient_label = data.y.float()\n",
    "    batch = data.batch\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=w_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.6920209512824104\n",
      "Epoch 2/300, Loss: 0.6878475468783152\n",
      "Epoch 3/300, Loss: 0.6846859405438105\n",
      "Epoch 4/300, Loss: 0.6818806642577762\n",
      "Epoch 5/300, Loss: 0.6792778315998259\n",
      "Epoch 6/300, Loss: 0.6767677487361998\n",
      "Epoch 7/300, Loss: 0.6742061312709536\n",
      "Epoch 8/300, Loss: 0.6715303595576968\n",
      "Epoch 9/300, Loss: 0.6686201393604279\n",
      "Epoch 10/300, Loss: 0.6653779337093944\n",
      "Epoch 11/300, Loss: 0.6617446186996642\n",
      "Epoch 12/300, Loss: 0.6576290180285772\n",
      "Epoch 13/300, Loss: 0.6529373782021659\n",
      "Epoch 14/300, Loss: 0.6476758573026884\n",
      "Epoch 15/300, Loss: 0.6418059166698229\n",
      "Epoch 16/300, Loss: 0.6352536972789538\n",
      "Epoch 17/300, Loss: 0.6280229134219033\n",
      "Epoch 18/300, Loss: 0.6200759982069334\n",
      "Epoch 19/300, Loss: 0.6114232391119003\n",
      "Epoch 20/300, Loss: 0.6020943401824861\n",
      "Epoch 21/300, Loss: 0.5921491561901002\n",
      "Epoch 22/300, Loss: 0.5817277135238761\n",
      "Epoch 23/300, Loss: 0.5708674781379246\n",
      "Epoch 24/300, Loss: 0.5597417168319225\n",
      "Epoch 25/300, Loss: 0.5485136711171695\n",
      "Epoch 26/300, Loss: 0.5372820922306606\n",
      "Epoch 27/300, Loss: 0.5262262684603533\n",
      "Epoch 28/300, Loss: 0.5153319746965453\n",
      "Epoch 29/300, Loss: 0.5046904922596046\n",
      "Epoch 30/300, Loss: 0.49433902243063566\n",
      "Epoch 31/300, Loss: 0.48417241405695677\n",
      "Epoch 32/300, Loss: 0.47426756750792265\n",
      "Epoch 33/300, Loss: 0.4646674297483904\n",
      "Epoch 34/300, Loss: 0.45519160860705943\n",
      "Epoch 35/300, Loss: 0.445870793380198\n",
      "Epoch 36/300, Loss: 0.43675633071966113\n",
      "Epoch 37/300, Loss: 0.427569332488236\n",
      "Epoch 38/300, Loss: 0.4187400106872831\n",
      "Epoch 39/300, Loss: 0.4098653110365073\n",
      "Epoch 40/300, Loss: 0.40136585392368335\n",
      "Epoch 41/300, Loss: 0.3927115777817865\n",
      "Epoch 42/300, Loss: 0.38422147896406905\n",
      "Epoch 43/300, Loss: 0.3758907985784823\n",
      "Epoch 44/300, Loss: 0.3672989113810694\n",
      "Epoch 45/300, Loss: 0.35933311749249697\n",
      "Epoch 46/300, Loss: 0.3510949129815258\n",
      "Epoch 47/300, Loss: 0.3425629876354443\n",
      "Epoch 48/300, Loss: 0.33473944630739944\n",
      "Epoch 49/300, Loss: 0.3265201087037678\n",
      "Epoch 50/300, Loss: 0.31858586018539164\n",
      "Epoch 51/300, Loss: 0.3107932608358429\n",
      "Epoch 52/300, Loss: 0.3026708493984881\n",
      "Epoch 53/300, Loss: 0.2949835021198461\n",
      "Epoch 54/300, Loss: 0.28726737445408834\n",
      "Epoch 55/300, Loss: 0.2797384070027398\n",
      "Epoch 56/300, Loss: 0.27205984218051615\n",
      "Epoch 57/300, Loss: 0.26458090825915515\n",
      "Epoch 58/300, Loss: 0.25716886779645437\n",
      "Epoch 59/300, Loss: 0.24995585975799864\n",
      "Epoch 60/300, Loss: 0.2427771287281454\n",
      "Epoch 61/300, Loss: 0.23565241173907583\n",
      "Epoch 62/300, Loss: 0.22873868710441803\n",
      "Epoch 63/300, Loss: 0.22181995731010656\n",
      "Epoch 64/300, Loss: 0.21503789051002103\n",
      "Epoch 65/300, Loss: 0.2081134407008427\n",
      "Epoch 66/300, Loss: 0.20179677216133096\n",
      "Epoch 67/300, Loss: 0.1951379306900933\n",
      "Epoch 68/300, Loss: 0.18899203398160308\n",
      "Epoch 69/300, Loss: 0.18254482671753586\n",
      "Epoch 70/300, Loss: 0.17669176558376334\n",
      "Epoch 71/300, Loss: 0.170602437306332\n",
      "Epoch 72/300, Loss: 0.1650162170671614\n",
      "Epoch 73/300, Loss: 0.15912670814811594\n",
      "Epoch 74/300, Loss: 0.1536044659800436\n",
      "Epoch 75/300, Loss: 0.14814339863785012\n",
      "Epoch 76/300, Loss: 0.14272417215570818\n",
      "Epoch 77/300, Loss: 0.1376673649978156\n",
      "Epoch 78/300, Loss: 0.13254797272132973\n",
      "Epoch 79/300, Loss: 0.12750462724185732\n",
      "Epoch 80/300, Loss: 0.12277344084287463\n",
      "Epoch 81/300, Loss: 0.11802308151237369\n",
      "Epoch 82/300, Loss: 0.11343056447750703\n",
      "Epoch 83/300, Loss: 0.10887340367836866\n",
      "Epoch 84/300, Loss: 0.10472311103263737\n",
      "Epoch 85/300, Loss: 0.10040688268565559\n",
      "Epoch 86/300, Loss: 0.09640317608018666\n",
      "Epoch 87/300, Loss: 0.09267495868046115\n",
      "Epoch 88/300, Loss: 0.08872855021504511\n",
      "Epoch 89/300, Loss: 0.08502937329379685\n",
      "Epoch 90/300, Loss: 0.08170160515417246\n",
      "Epoch 91/300, Loss: 0.07824773009876178\n",
      "Epoch 92/300, Loss: 0.07480888015050996\n",
      "Epoch 93/300, Loss: 0.07187894722331326\n",
      "Epoch 94/300, Loss: 0.06866347693035582\n",
      "Epoch 95/300, Loss: 0.06572415733874204\n",
      "Epoch 96/300, Loss: 0.0630814092635698\n",
      "Epoch 97/300, Loss: 0.060255491559150276\n",
      "Epoch 98/300, Loss: 0.05762482063491483\n",
      "Epoch 99/300, Loss: 0.05521193017889643\n",
      "Epoch 100/300, Loss: 0.05286405867334885\n",
      "Epoch 101/300, Loss: 0.05060893998049886\n",
      "Epoch 102/300, Loss: 0.04842359687746639\n",
      "Epoch 103/300, Loss: 0.04636308025380521\n",
      "Epoch 104/300, Loss: 0.04448654148632497\n",
      "Epoch 105/300, Loss: 0.04254588245055943\n",
      "Epoch 106/300, Loss: 0.04069724344764073\n",
      "Epoch 107/300, Loss: 0.03902630258446106\n",
      "Epoch 108/300, Loss: 0.03729568103504394\n",
      "Epoch 109/300, Loss: 0.03579467526850725\n",
      "Epoch 110/300, Loss: 0.03426987641523839\n",
      "Epoch 111/300, Loss: 0.032839937298837926\n",
      "Epoch 112/300, Loss: 0.03147775533089274\n",
      "Epoch 113/300, Loss: 0.030215474806347908\n",
      "Epoch 114/300, Loss: 0.028946775975305917\n",
      "Epoch 115/300, Loss: 0.02777360995224811\n",
      "Epoch 116/300, Loss: 0.026651933246087552\n",
      "Epoch 117/300, Loss: 0.025595312265924347\n",
      "Epoch 118/300, Loss: 0.024588713608815246\n",
      "Epoch 119/300, Loss: 0.023635514901662734\n",
      "Epoch 120/300, Loss: 0.022717937556764597\n",
      "Epoch 121/300, Loss: 0.02187150838726094\n",
      "Epoch 122/300, Loss: 0.021023497831764375\n",
      "Epoch 123/300, Loss: 0.02025823148234189\n",
      "Epoch 124/300, Loss: 0.019549387971576744\n",
      "Epoch 125/300, Loss: 0.018841176777948408\n",
      "Epoch 126/300, Loss: 0.01815236603764635\n",
      "Epoch 127/300, Loss: 0.017541035919430147\n",
      "Epoch 128/300, Loss: 0.016941062020470844\n",
      "Epoch 129/300, Loss: 0.016376483953128365\n",
      "Epoch 130/300, Loss: 0.015866230619012628\n",
      "Epoch 131/300, Loss: 0.015360787403386058\n",
      "Epoch 132/300, Loss: 0.014865621793901036\n",
      "Epoch 133/300, Loss: 0.014405650293645897\n",
      "Epoch 134/300, Loss: 0.013994609964038335\n",
      "Epoch 135/300, Loss: 0.01357316900545794\n",
      "Epoch 136/300, Loss: 0.013212989058141812\n",
      "Epoch 137/300, Loss: 0.012841292991878004\n",
      "Epoch 138/300, Loss: 0.012486112571118604\n",
      "Epoch 139/300, Loss: 0.012154877165981657\n",
      "Epoch 140/300, Loss: 0.01185754488774158\n",
      "Epoch 141/300, Loss: 0.01154307725633413\n",
      "Epoch 142/300, Loss: 0.011284504228430288\n",
      "Epoch 143/300, Loss: 0.010998981291451113\n",
      "Epoch 144/300, Loss: 0.010774091321663667\n",
      "Epoch 145/300, Loss: 0.010519422566125971\n",
      "Epoch 146/300, Loss: 0.01031327712699743\n",
      "Epoch 147/300, Loss: 0.010067775319771871\n",
      "Epoch 148/300, Loss: 0.009887695577582348\n",
      "Epoch 149/300, Loss: 0.00967539971042323\n",
      "Epoch 150/300, Loss: 0.00948835431339559\n",
      "Epoch 151/300, Loss: 0.009318463864846345\n",
      "Epoch 152/300, Loss: 0.009171999650813268\n",
      "Epoch 153/300, Loss: 0.008996750406915902\n",
      "Epoch 154/300, Loss: 0.008857886889187126\n",
      "Epoch 155/300, Loss: 0.008690727939693773\n",
      "Epoch 156/300, Loss: 0.008566402356545023\n",
      "Epoch 157/300, Loss: 0.00843076064750529\n",
      "Epoch 158/300, Loss: 0.008308512260852871\n",
      "Epoch 159/300, Loss: 0.008185212902772895\n",
      "Epoch 160/300, Loss: 0.00807599012278647\n",
      "Epoch 161/300, Loss: 0.007946975484476886\n",
      "Epoch 162/300, Loss: 0.007870890514146382\n",
      "Epoch 163/300, Loss: 0.00774975106454045\n",
      "Epoch 164/300, Loss: 0.007661297554565191\n",
      "Epoch 165/300, Loss: 0.007564701351988982\n",
      "Epoch 166/300, Loss: 0.007481223483753193\n",
      "Epoch 167/300, Loss: 0.0073898239517441195\n",
      "Epoch 168/300, Loss: 0.007318415115835261\n",
      "Epoch 169/300, Loss: 0.007225136933703186\n",
      "Epoch 170/300, Loss: 0.00716535600871578\n",
      "Epoch 171/300, Loss: 0.007082663223464773\n",
      "Epoch 172/300, Loss: 0.007010427428120551\n",
      "Epoch 173/300, Loss: 0.006934224697653083\n",
      "Epoch 174/300, Loss: 0.006871415389232638\n",
      "Epoch 175/300, Loss: 0.006796713105047185\n",
      "Epoch 176/300, Loss: 0.006743825277619361\n",
      "Epoch 177/300, Loss: 0.006672581182359308\n",
      "Epoch 178/300, Loss: 0.006624253518239613\n",
      "Epoch 179/300, Loss: 0.006555811580816882\n",
      "Epoch 180/300, Loss: 0.006503191128916576\n",
      "Epoch 181/300, Loss: 0.006441935609505546\n",
      "Epoch 182/300, Loss: 0.006396807960768263\n",
      "Epoch 183/300, Loss: 0.006338290792356054\n",
      "Epoch 184/300, Loss: 0.006295515473304901\n",
      "Epoch 185/300, Loss: 0.006239166119221162\n",
      "Epoch 186/300, Loss: 0.0062042074139548615\n",
      "Epoch 187/300, Loss: 0.006146306696984503\n",
      "Epoch 188/300, Loss: 0.0061125687226942305\n",
      "Epoch 189/300, Loss: 0.0060641067368178555\n",
      "Epoch 190/300, Loss: 0.006031132829789192\n",
      "Epoch 191/300, Loss: 0.005969435718535873\n",
      "Epoch 192/300, Loss: 0.005948115558599622\n",
      "Epoch 193/300, Loss: 0.005909367896977206\n",
      "Epoch 194/300, Loss: 0.005871489670262096\n",
      "Epoch 195/300, Loss: 0.005823317362700665\n",
      "Epoch 196/300, Loss: 0.005799347176896251\n",
      "Epoch 197/300, Loss: 0.005757351208547336\n",
      "Epoch 198/300, Loss: 0.0057329206059864345\n",
      "Epoch 199/300, Loss: 0.005698121560105275\n",
      "Epoch 200/300, Loss: 0.005661992515842043\n",
      "Epoch 201/300, Loss: 0.005623485869251813\n",
      "Epoch 202/300, Loss: 0.005608406628059895\n",
      "Epoch 203/300, Loss: 0.00557882455810428\n",
      "Epoch 204/300, Loss: 0.005542609708213049\n",
      "Epoch 205/300, Loss: 0.005521464472968164\n",
      "Epoch 206/300, Loss: 0.005498352191455272\n",
      "Epoch 207/300, Loss: 0.00547182591098659\n",
      "Epoch 208/300, Loss: 0.005452582282087533\n",
      "Epoch 209/300, Loss: 0.005426236845202936\n",
      "Epoch 210/300, Loss: 0.005399556280609645\n",
      "Epoch 211/300, Loss: 0.005388731659676878\n",
      "Epoch 212/300, Loss: 0.005358965291041243\n",
      "Epoch 213/300, Loss: 0.005339106821189432\n",
      "Epoch 214/300, Loss: 0.005321448608420775\n",
      "Epoch 215/300, Loss: 0.0053022215815083824\n",
      "Epoch 216/300, Loss: 0.0052812065182232274\n",
      "Epoch 217/300, Loss: 0.005263330062115233\n",
      "Epoch 218/300, Loss: 0.005248276019945443\n",
      "Epoch 219/300, Loss: 0.005238191589956135\n",
      "Epoch 220/300, Loss: 0.005207615426767816\n",
      "Epoch 221/300, Loss: 0.00519073980496981\n",
      "Epoch 222/300, Loss: 0.00519003772386147\n",
      "Epoch 223/300, Loss: 0.00516710332224538\n",
      "Epoch 224/300, Loss: 0.005150981445548417\n",
      "Epoch 225/300, Loss: 0.005129634255981823\n",
      "Epoch 226/300, Loss: 0.005122749784848206\n",
      "Epoch 227/300, Loss: 0.005111298429926795\n",
      "Epoch 228/300, Loss: 0.0051014197011554074\n",
      "Epoch 229/300, Loss: 0.0050853431346968\n",
      "Epoch 230/300, Loss: 0.005077353631509344\n",
      "Epoch 231/300, Loss: 0.005063703684082758\n",
      "Epoch 232/300, Loss: 0.005050589627866351\n",
      "Epoch 233/300, Loss: 0.005043517957297522\n",
      "Epoch 234/300, Loss: 0.005034885908034967\n",
      "Epoch 235/300, Loss: 0.005023787617270429\n",
      "Epoch 236/300, Loss: 0.005007742793492433\n",
      "Epoch 237/300, Loss: 0.005008594317315908\n",
      "Epoch 238/300, Loss: 0.004996445623678456\n",
      "Epoch 239/300, Loss: 0.0049911589184249905\n",
      "Epoch 240/300, Loss: 0.0049747587812203975\n",
      "Epoch 241/300, Loss: 0.004967509130567226\n",
      "Epoch 242/300, Loss: 0.004966118067954318\n",
      "Epoch 243/300, Loss: 0.004955168032232055\n",
      "Epoch 244/300, Loss: 0.004949887858639305\n",
      "Epoch 245/300, Loss: 0.004938460120450421\n",
      "Epoch 246/300, Loss: 0.004932063674902885\n",
      "Epoch 247/300, Loss: 0.004924329442045678\n",
      "Epoch 248/300, Loss: 0.004924632600612287\n",
      "Epoch 249/300, Loss: 0.0049105076834397125\n",
      "Epoch 250/300, Loss: 0.004905873728440012\n",
      "Epoch 251/300, Loss: 0.004903001450499272\n",
      "Epoch 252/300, Loss: 0.004900404289275919\n",
      "Epoch 253/300, Loss: 0.004889597833598272\n",
      "Epoch 254/300, Loss: 0.004876326707148475\n",
      "Epoch 255/300, Loss: 0.004874283044922275\n",
      "Epoch 256/300, Loss: 0.004878621617724545\n",
      "Epoch 257/300, Loss: 0.004874274566536874\n",
      "Epoch 258/300, Loss: 0.004857519556046307\n",
      "Epoch 259/300, Loss: 0.004856088477339341\n",
      "Epoch 260/300, Loss: 0.004848249418889273\n",
      "Epoch 261/300, Loss: 0.004849232339691463\n",
      "Epoch 262/300, Loss: 0.004842464576160411\n",
      "Epoch 263/300, Loss: 0.004836233143609327\n",
      "Epoch 264/300, Loss: 0.0048334259260526735\n",
      "Epoch 265/300, Loss: 0.004833118115702174\n",
      "Epoch 266/300, Loss: 0.004827240743261505\n",
      "Epoch 267/300, Loss: 0.004819359219063777\n",
      "Epoch 268/300, Loss: 0.0048131021201499435\n",
      "Epoch 269/300, Loss: 0.004815163899007132\n",
      "Epoch 270/300, Loss: 0.004812806520047326\n",
      "Epoch 271/300, Loss: 0.004800849271520305\n",
      "Epoch 272/300, Loss: 0.004805168496387341\n",
      "Epoch 273/300, Loss: 0.004799602946159506\n",
      "Epoch 274/300, Loss: 0.004793595925942047\n",
      "Epoch 275/300, Loss: 0.004794812324845259\n",
      "Epoch 276/300, Loss: 0.004790400278654262\n",
      "Epoch 277/300, Loss: 0.004787849625362807\n",
      "Epoch 278/300, Loss: 0.0047730501829659\n",
      "Epoch 279/300, Loss: 0.004781249361029421\n",
      "Epoch 280/300, Loss: 0.004775825800085155\n",
      "Epoch 281/300, Loss: 0.004774599323818238\n",
      "Epoch 282/300, Loss: 0.004771845226864188\n",
      "Epoch 283/300, Loss: 0.004763179504783787\n",
      "Epoch 284/300, Loss: 0.004759487237523344\n",
      "Epoch 285/300, Loss: 0.004762337783256008\n",
      "Epoch 286/300, Loss: 0.004750655951980235\n",
      "Epoch 287/300, Loss: 0.004748686518445632\n",
      "Epoch 288/300, Loss: 0.0047467630522999085\n",
      "Epoch 289/300, Loss: 0.004745204663704766\n",
      "Epoch 290/300, Loss: 0.004740304863382082\n",
      "Epoch 291/300, Loss: 0.00474326800989128\n",
      "Epoch 292/300, Loss: 0.004732608520897112\n",
      "Epoch 293/300, Loss: 0.004731955685264532\n",
      "Epoch 294/300, Loss: 0.004724128978521253\n",
      "Epoch 295/300, Loss: 0.0047330641027526965\n",
      "Epoch 296/300, Loss: 0.0047283036594417\n",
      "Epoch 297/300, Loss: 0.004722218473941149\n",
      "Epoch 298/300, Loss: 0.004714273754835828\n",
      "Epoch 299/300, Loss: 0.00471648866110954\n",
      "Epoch 300/300, Loss: 0.004712846912381946\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "train_losses = []\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for data in train_loader:                                               # Iterate over each batch (here, each batch is one patient)\n",
    "                                                                            # Data object contains 'x' (features), 'edge_index' (graph edges), 'y' (labels)\n",
    "        patient_features = data.x                                           # Shape: (num_nodes, in_channels)\n",
    "        patient_edges = data.edge_index                                     # Shape: (2, num_edges)\n",
    "        patient_label = data.y.float()                                      # Target label\n",
    "        batch = data.batch\n",
    "\n",
    "        # Ensure correct format\n",
    "        patient_features = patient_features.float()\n",
    "        patient_edges = patient_edges.to(torch.long)                 \n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(patient_features, patient_edges, batch)               # Output shape: (1, 1)\n",
    "        \n",
    "        # Binary Classification Loss\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(output.view(-1), patient_label)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Print loss after each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 71.42857142857143%\n",
      "Precision: 0.6153846153846154\n",
      "Recall: 0.8888888888888888\n",
      "F1-Score: 0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "model.eval() \n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:                            # Iterate over each batch (i.e. one patient)\n",
    "        patient_features = data.x                       # Get features (shape: [num_nodes, in_channels])\n",
    "        patient_edges = data.edge_index                 # Get edges (shape: [2, num_edges])\n",
    "        patient_label = data.y.float()                  # Get label (shape: [1])\n",
    "\n",
    "        # Ensure correct format\n",
    "        patient_features = patient_features.float()    \n",
    "        patient_edges = patient_edges.to(torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(patient_features, patient_edges, data.batch)  # Use the batch info to aggregate across nodes\n",
    "\n",
    "        # Apply sigmoid to the output logits and get the predicted class (0 or 1)\n",
    "        pred = torch.sigmoid(output.squeeze())\n",
    "        predicted_class = (pred >= 0.5).float()                     # Threshold at 0.5 to classify as 0 or 1\n",
    "        \n",
    "        # Collect the labels and predictions for metrics\n",
    "        all_labels.append(patient_label.cpu().numpy())\n",
    "        all_predictions.append(predicted_class.cpu().numpy())\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct += (predicted_class == patient_label).sum().item()\n",
    "        total += patient_label.size(0)  # Increment by the number of samples in this batch\n",
    "\n",
    "# Accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy}%\")\n",
    "\n",
    "# Calculate Metrics\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized testing with metrics and benchmarking\n",
    "@benchmark_function\n",
    "def test_model_optimized(model, test_loader):\n",
    "    \"\"\"\n",
    "    Test the model with comprehensive metrics and benchmarking\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    all_labels = []\n",
    "    all_probas = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader, desc=\"Testing\"):\n",
    "            data = data.to(device)\n",
    "            patient_features = data.x.float()\n",
    "            patient_edges = data.edge_index.long()\n",
    "            patient_label = data.y.float()\n",
    "            batch = data.batch\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(patient_features, patient_edges, batch)\n",
    "            proba = torch.sigmoid(output.squeeze()).cpu().numpy()\n",
    "            pred = (proba >= 0.5).astype(float)\n",
    "            \n",
    "            # Store results\n",
    "            all_labels.append(patient_label.cpu().numpy())\n",
    "            all_probas.append(proba)\n",
    "            all_predictions.append(pred)\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probas = np.array(all_probas)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (all_predictions == all_labels).mean() * 100\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probas)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'predictions': all_predictions,\n",
    "        'probabilities': all_probas,\n",
    "        'true_labels': all_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "Test classification with clinical and image embeddings only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)             # Binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Clinical-Only Model\n",
      "Train Features:  torch.Size([84, 4864])\n",
      "Test Features:  torch.Size([21, 4864])\n",
      "Train Labels:  torch.Size([84, 1])\n",
      "Test Labels:  torch.Size([21, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pascal/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.8802260821241708\n",
      "Epoch 2/300, Loss: 0.7158462093655198\n",
      "Epoch 3/300, Loss: 0.6561866663839846\n",
      "Epoch 4/300, Loss: 0.671134847016739\n",
      "Epoch 5/300, Loss: 0.61417486151636\n",
      "Epoch 6/300, Loss: 0.5526924502268057\n",
      "Epoch 7/300, Loss: 0.5295529374963648\n",
      "Epoch 8/300, Loss: 0.5019920853325553\n",
      "Epoch 9/300, Loss: 0.4681862158551147\n",
      "Epoch 10/300, Loss: 0.5360424005179777\n",
      "Epoch 11/300, Loss: 0.4829019083283908\n",
      "Epoch 12/300, Loss: 0.41138505720283275\n",
      "Epoch 13/300, Loss: 0.4360533303124179\n",
      "Epoch 14/300, Loss: 0.4146284644340908\n",
      "Epoch 15/300, Loss: 0.4266284666061568\n",
      "Epoch 16/300, Loss: 0.3389327160421766\n",
      "Epoch 17/300, Loss: 0.389443481229418\n",
      "Epoch 18/300, Loss: 0.37014546584957014\n",
      "Epoch 19/300, Loss: 0.28989216323350864\n",
      "Epoch 20/300, Loss: 0.3418952292792732\n",
      "Epoch 21/300, Loss: 0.3274699876400562\n",
      "Epoch 22/300, Loss: 0.3073837414936168\n",
      "Epoch 23/300, Loss: 0.34645308139288644\n",
      "Epoch 24/300, Loss: 0.3119964970213741\n",
      "Epoch 25/300, Loss: 0.2880344306574137\n",
      "Epoch 26/300, Loss: 0.26805029412571457\n",
      "Epoch 27/300, Loss: 0.2744098046061178\n",
      "Epoch 28/300, Loss: 0.24143667109209282\n",
      "Epoch 29/300, Loss: 0.22901459039218794\n",
      "Epoch 30/300, Loss: 0.23917104241184006\n",
      "Epoch 31/300, Loss: 0.18954150138056103\n",
      "Epoch 32/300, Loss: 0.28016579223140614\n",
      "Epoch 33/300, Loss: 0.19549377658169456\n",
      "Epoch 34/300, Loss: 0.3092157032753156\n",
      "Epoch 35/300, Loss: 0.20221891435184738\n",
      "Epoch 36/300, Loss: 0.21427727669498328\n",
      "Epoch 37/300, Loss: 0.22051902483595617\n",
      "Epoch 38/300, Loss: 0.13900171318864468\n",
      "Epoch 39/300, Loss: 0.19482157211290232\n",
      "Epoch 40/300, Loss: 0.23320235383427956\n",
      "Epoch 41/300, Loss: 0.13406868926349752\n",
      "Epoch 42/300, Loss: 0.16927070511352008\n",
      "Epoch 43/300, Loss: 0.16170086450269613\n",
      "Epoch 44/300, Loss: 0.12859497320911964\n",
      "Epoch 45/300, Loss: 0.15738417480192657\n",
      "Epoch 46/300, Loss: 0.17330942869129795\n",
      "Epoch 47/300, Loss: 0.15147968059493302\n",
      "Epoch 48/300, Loss: 0.1386871441479642\n",
      "Epoch 49/300, Loss: 0.14233911204530308\n",
      "Epoch 50/300, Loss: 0.1331602319538295\n",
      "Epoch 51/300, Loss: 0.14387363944561976\n",
      "Epoch 52/300, Loss: 0.14199053548794116\n",
      "Epoch 53/300, Loss: 0.11590471894429333\n",
      "Epoch 54/300, Loss: 0.11258401622177949\n",
      "Epoch 55/300, Loss: 0.0951980871465347\n",
      "Epoch 56/300, Loss: 0.11146355944571991\n",
      "Epoch 57/300, Loss: 0.09039776448785476\n",
      "Epoch 58/300, Loss: 0.07755058636578364\n",
      "Epoch 59/300, Loss: 0.12257832326052047\n",
      "Epoch 60/300, Loss: 0.11328888562115927\n",
      "Epoch 61/300, Loss: 0.09055429212835772\n",
      "Epoch 62/300, Loss: 0.051460574033251494\n",
      "Epoch 63/300, Loss: 0.11352119358016169\n",
      "Epoch 64/300, Loss: 0.096005112233426\n",
      "Epoch 65/300, Loss: 0.06276087129297653\n",
      "Epoch 66/300, Loss: 0.10254081530079365\n",
      "Epoch 67/300, Loss: 0.1328275769658051\n",
      "Epoch 68/300, Loss: 0.08543812965021266\n",
      "Epoch 69/300, Loss: 0.06054963396790823\n",
      "Epoch 70/300, Loss: 0.060224283376865764\n",
      "Epoch 71/300, Loss: 0.05604590687112507\n",
      "Epoch 72/300, Loss: 0.0873090757759182\n",
      "Epoch 73/300, Loss: 0.11520046605941667\n",
      "Epoch 74/300, Loss: 0.06893400207216109\n",
      "Epoch 75/300, Loss: 0.0600352972478822\n",
      "Epoch 76/300, Loss: 0.10636612173108788\n",
      "Epoch 77/300, Loss: 0.0609933850890315\n",
      "Epoch 78/300, Loss: 0.10473447603380957\n",
      "Epoch 79/300, Loss: 0.10108958299844095\n",
      "Epoch 80/300, Loss: 0.07376281773679441\n",
      "Epoch 81/300, Loss: 0.038953126104454394\n",
      "Epoch 82/300, Loss: 0.05803369370097535\n",
      "Epoch 83/300, Loss: 0.05966931441064266\n",
      "Epoch 84/300, Loss: 0.06995583502089979\n",
      "Epoch 85/300, Loss: 0.08071569511743949\n",
      "Epoch 86/300, Loss: 0.1292728217226078\n",
      "Epoch 87/300, Loss: 0.04436192354637561\n",
      "Epoch 88/300, Loss: 0.03438101330035124\n",
      "Epoch 89/300, Loss: 0.06961248411906917\n",
      "Epoch 90/300, Loss: 0.0702088507684098\n",
      "Epoch 91/300, Loss: 0.040146747146207945\n",
      "Epoch 92/300, Loss: 0.076023125747719\n",
      "Epoch 93/300, Loss: 0.1010280041092135\n",
      "Epoch 94/300, Loss: 0.05519937642856586\n",
      "Epoch 95/300, Loss: 0.0393403146662565\n",
      "Epoch 96/300, Loss: 0.07199528826957417\n",
      "Epoch 97/300, Loss: 0.08400788567185609\n",
      "Epoch 98/300, Loss: 0.03684445163391746\n",
      "Epoch 99/300, Loss: 0.05674435111506045\n",
      "Epoch 100/300, Loss: 0.04497965082874776\n",
      "Epoch 101/300, Loss: 0.03937821354167316\n",
      "Epoch 102/300, Loss: 0.05827907455013491\n",
      "Epoch 103/300, Loss: 0.04231084482426152\n",
      "Epoch 104/300, Loss: 0.03145917816388093\n",
      "Epoch 105/300, Loss: 0.027545790688972005\n",
      "Epoch 106/300, Loss: 0.02796031347607248\n",
      "Epoch 107/300, Loss: 0.04297574970071746\n",
      "Epoch 108/300, Loss: 0.016149837570687168\n",
      "Epoch 109/300, Loss: 0.05296051447374933\n",
      "Epoch 110/300, Loss: 0.06004332286245046\n",
      "Epoch 111/300, Loss: 0.024745270719713764\n",
      "Epoch 112/300, Loss: 0.06341086645966978\n",
      "Epoch 113/300, Loss: 0.033101792206938034\n",
      "Epoch 114/300, Loss: 0.02974177087825494\n",
      "Epoch 115/300, Loss: 0.044901053123083\n",
      "Epoch 116/300, Loss: 0.036446804958745936\n",
      "Epoch 117/300, Loss: 0.03455461895703265\n",
      "Epoch 118/300, Loss: 0.0280940466427122\n",
      "Epoch 119/300, Loss: 0.024294872301393915\n",
      "Epoch 120/300, Loss: 0.044289207858472845\n",
      "Epoch 121/300, Loss: 0.020755090288804925\n",
      "Epoch 122/300, Loss: 0.018249699510864473\n",
      "Epoch 123/300, Loss: 0.008813231885172865\n",
      "Epoch 124/300, Loss: 0.03680566947086341\n",
      "Epoch 125/300, Loss: 0.0931970015630145\n",
      "Epoch 126/300, Loss: 0.0441434360240347\n",
      "Epoch 127/300, Loss: 0.021288967069331307\n",
      "Epoch 128/300, Loss: 0.05370857491091208\n",
      "Epoch 129/300, Loss: 0.037466924968022654\n",
      "Epoch 130/300, Loss: 0.033275704271758175\n",
      "Epoch 131/300, Loss: 0.08653677647202684\n",
      "Epoch 132/300, Loss: 0.05740071485853347\n",
      "Epoch 133/300, Loss: 0.058899756874996455\n",
      "Epoch 134/300, Loss: 0.15993445569548875\n",
      "Epoch 135/300, Loss: 0.11213687970514268\n",
      "Epoch 136/300, Loss: 0.09702941038542147\n",
      "Epoch 137/300, Loss: 0.01459736348423685\n",
      "Epoch 138/300, Loss: 0.0443373373338698\n",
      "Epoch 139/300, Loss: 0.027477153220604948\n",
      "Epoch 140/300, Loss: 0.0370961076609959\n",
      "Epoch 141/300, Loss: 0.016580061337651705\n",
      "Epoch 142/300, Loss: 0.027425789101045577\n",
      "Epoch 143/300, Loss: 0.04024884333968116\n",
      "Epoch 144/300, Loss: 0.08421918099788182\n",
      "Epoch 145/300, Loss: 0.035840877834826944\n",
      "Epoch 146/300, Loss: 0.018724234583025887\n",
      "Epoch 147/300, Loss: 0.039931106344140974\n",
      "Epoch 148/300, Loss: 0.03879526902189786\n",
      "Epoch 149/300, Loss: 0.01773619643403465\n",
      "Epoch 150/300, Loss: 0.016805926213347026\n",
      "Epoch 151/300, Loss: 0.01345235099977576\n",
      "Epoch 152/300, Loss: 0.01339781220197064\n",
      "Epoch 153/300, Loss: 0.07593643689659858\n",
      "Epoch 154/300, Loss: 0.012330985195424051\n",
      "Epoch 155/300, Loss: 0.022378544387254555\n",
      "Epoch 156/300, Loss: 0.018266562311907736\n",
      "Epoch 157/300, Loss: 0.008108817516371217\n",
      "Epoch 158/300, Loss: 0.008855255287857725\n",
      "Epoch 159/300, Loss: 0.03936394678312283\n",
      "Epoch 160/300, Loss: 0.07089175470020592\n",
      "Epoch 161/300, Loss: 0.018687352708417967\n",
      "Epoch 162/300, Loss: 0.03928747981457818\n",
      "Epoch 163/300, Loss: 0.021860589611539066\n",
      "Epoch 164/300, Loss: 0.04528806108161334\n",
      "Epoch 165/300, Loss: 0.01804986926205988\n",
      "Epoch 166/300, Loss: 0.03620792620269969\n",
      "Epoch 167/300, Loss: 0.008836915218541972\n",
      "Epoch 168/300, Loss: 0.011274511223105896\n",
      "Epoch 169/300, Loss: 0.02964729813805474\n",
      "Epoch 170/300, Loss: 0.08473049017116249\n",
      "Epoch 171/300, Loss: 0.03445255113515649\n",
      "Epoch 172/300, Loss: 0.07092184067315782\n",
      "Epoch 173/300, Loss: 0.05077569298343195\n",
      "Epoch 174/300, Loss: 0.015832675568187682\n",
      "Epoch 175/300, Loss: 0.021842835115718866\n",
      "Epoch 176/300, Loss: 0.02819878428289215\n",
      "Epoch 177/300, Loss: 0.011941888196881408\n",
      "Epoch 178/300, Loss: 0.0417842374169484\n",
      "Epoch 179/300, Loss: 0.014601339448237468\n",
      "Epoch 180/300, Loss: 0.02042415751478036\n",
      "Epoch 181/300, Loss: 0.05636254046848607\n",
      "Epoch 182/300, Loss: 0.06466579598284079\n",
      "Epoch 183/300, Loss: 0.03268572713088374\n",
      "Epoch 184/300, Loss: 0.09164580005271124\n",
      "Epoch 185/300, Loss: 0.02229222027279669\n",
      "Epoch 186/300, Loss: 0.0186852393552675\n",
      "Epoch 187/300, Loss: 0.08128733211047871\n",
      "Epoch 188/300, Loss: 0.019003647369596487\n",
      "Epoch 189/300, Loss: 0.023822825859313262\n",
      "Epoch 190/300, Loss: 0.012802897819410745\n",
      "Epoch 191/300, Loss: 0.034329544571860826\n",
      "Epoch 192/300, Loss: 0.011497355796915545\n",
      "Epoch 193/300, Loss: 0.027825422974220547\n",
      "Epoch 194/300, Loss: 0.02423940869805108\n",
      "Epoch 195/300, Loss: 0.008282893451586457\n",
      "Epoch 196/300, Loss: 0.012992664453683153\n",
      "Epoch 197/300, Loss: 0.004065708791251021\n",
      "Epoch 198/300, Loss: 0.02909981220871976\n",
      "Epoch 199/300, Loss: 0.00920390958505557\n",
      "Epoch 200/300, Loss: 0.01122086537131953\n",
      "Epoch 201/300, Loss: 0.00788281715488603\n",
      "Epoch 202/300, Loss: 0.0166297842464948\n",
      "Epoch 203/300, Loss: 0.06326951314573079\n",
      "Epoch 204/300, Loss: 0.0274771591759376\n",
      "Epoch 205/300, Loss: 0.010478103246494896\n",
      "Epoch 206/300, Loss: 0.01035072323648326\n",
      "Epoch 207/300, Loss: 0.002182569095139914\n",
      "Epoch 208/300, Loss: 0.02025957102108476\n",
      "Epoch 209/300, Loss: 0.008665720157090868\n",
      "Epoch 210/300, Loss: 0.0817741860092281\n",
      "Epoch 211/300, Loss: 0.011100419205044449\n",
      "Epoch 212/300, Loss: 0.015115148418808524\n",
      "Epoch 213/300, Loss: 0.012390441966834417\n",
      "Epoch 214/300, Loss: 0.009969462438534936\n",
      "Epoch 215/300, Loss: 0.014029600763904967\n",
      "Epoch 216/300, Loss: 0.019080947867726294\n",
      "Epoch 217/300, Loss: 0.03391583929255565\n",
      "Epoch 218/300, Loss: 0.08770212509578727\n",
      "Epoch 219/300, Loss: 0.04265594032889823\n",
      "Epoch 220/300, Loss: 0.037099384399180396\n",
      "Epoch 221/300, Loss: 0.03652243438252561\n",
      "Epoch 222/300, Loss: 0.016000980035948957\n",
      "Epoch 223/300, Loss: 0.008163951269097561\n",
      "Epoch 224/300, Loss: 0.01319089747266201\n",
      "Epoch 225/300, Loss: 0.01111998434613426\n",
      "Epoch 226/300, Loss: 0.02425615922526745\n",
      "Epoch 227/300, Loss: 0.005120929869560274\n",
      "Epoch 228/300, Loss: 0.006964211600740963\n",
      "Epoch 229/300, Loss: 0.004959843248360619\n",
      "Epoch 230/300, Loss: 0.009199533373971645\n",
      "Epoch 231/300, Loss: 0.010801207479573128\n",
      "Epoch 232/300, Loss: 0.014057356225508178\n",
      "Epoch 233/300, Loss: 0.03890603428428412\n",
      "Epoch 234/300, Loss: 0.014166170810569631\n",
      "Epoch 235/300, Loss: 0.00897282096241016\n",
      "Epoch 236/300, Loss: 0.06793415482234545\n",
      "Epoch 237/300, Loss: 0.020964988357370457\n",
      "Epoch 238/300, Loss: 0.05113750221461984\n",
      "Epoch 239/300, Loss: 0.02733034493116633\n",
      "Epoch 240/300, Loss: 0.030249052591006714\n",
      "Epoch 241/300, Loss: 0.06975841484617523\n",
      "Epoch 242/300, Loss: 0.01220844131085741\n",
      "Epoch 243/300, Loss: 0.06009531104432007\n",
      "Epoch 244/300, Loss: 0.022261649906344584\n",
      "Epoch 245/300, Loss: 0.015832699685393015\n",
      "Epoch 246/300, Loss: 0.03375356184303242\n",
      "Epoch 247/300, Loss: 0.013347224068580132\n",
      "Epoch 248/300, Loss: 0.006737745819962491\n",
      "Epoch 249/300, Loss: 0.013770882544877324\n",
      "Epoch 250/300, Loss: 0.00787315735283292\n",
      "Epoch 251/300, Loss: 0.029247713489139064\n",
      "Epoch 252/300, Loss: 0.019045300310556644\n",
      "Epoch 253/300, Loss: 0.03735566090057708\n",
      "Epoch 254/300, Loss: 0.013179765651700441\n",
      "Epoch 255/300, Loss: 0.016972040320732178\n",
      "Epoch 256/300, Loss: 0.0117740342159282\n",
      "Epoch 257/300, Loss: 0.010002858347744439\n",
      "Epoch 258/300, Loss: 0.00226701742738623\n",
      "Epoch 259/300, Loss: 0.015798767324904178\n",
      "Epoch 260/300, Loss: 0.014980378865138937\n",
      "Epoch 261/300, Loss: 0.04723335526529101\n",
      "Epoch 262/300, Loss: 0.007862251080274163\n",
      "Epoch 263/300, Loss: 0.08075561164060212\n",
      "Epoch 264/300, Loss: 0.04147616992763623\n",
      "Epoch 265/300, Loss: 0.032954464319563094\n",
      "Epoch 266/300, Loss: 0.013269544907239908\n",
      "Epoch 267/300, Loss: 0.005111279745293949\n",
      "Epoch 268/300, Loss: 0.0393361207008347\n",
      "Epoch 269/300, Loss: 0.012869496565584913\n",
      "Epoch 270/300, Loss: 0.011169328441758674\n",
      "Epoch 271/300, Loss: 0.02875809298222358\n",
      "Epoch 272/300, Loss: 0.021731298468066502\n",
      "Epoch 273/300, Loss: 0.00847930047606609\n",
      "Epoch 274/300, Loss: 0.010608574277422244\n",
      "Epoch 275/300, Loss: 0.013424361524858259\n",
      "Epoch 276/300, Loss: 0.010264799920669532\n",
      "Epoch 277/300, Loss: 0.007063508365891583\n",
      "Epoch 278/300, Loss: 0.0022067139879722625\n",
      "Epoch 279/300, Loss: 0.011399030048846607\n",
      "Epoch 280/300, Loss: 0.0032122518993305724\n",
      "Epoch 281/300, Loss: 0.00457317425372359\n",
      "Epoch 282/300, Loss: 0.009879086495602726\n",
      "Epoch 283/300, Loss: 0.012497629994812985\n",
      "Epoch 284/300, Loss: 0.014933375859479117\n",
      "Epoch 285/300, Loss: 0.01656054841571475\n",
      "Epoch 286/300, Loss: 0.015715347045537682\n",
      "Epoch 287/300, Loss: 0.006917585216030851\n",
      "Epoch 288/300, Loss: 0.010512508803118709\n",
      "Epoch 289/300, Loss: 0.014527801245537103\n",
      "Epoch 290/300, Loss: 0.012548797082899063\n",
      "Epoch 291/300, Loss: 0.05489907834201911\n",
      "Epoch 292/300, Loss: 0.0505744976157889\n",
      "Epoch 293/300, Loss: 0.018482159113417683\n",
      "Epoch 294/300, Loss: 0.05428169502700044\n",
      "Epoch 295/300, Loss: 0.029515254464646305\n",
      "Epoch 296/300, Loss: 0.005479909500545671\n",
      "Epoch 297/300, Loss: 0.009797728464274683\n",
      "Epoch 298/300, Loss: 0.0072687708435850255\n",
      "Epoch 299/300, Loss: 0.005403200609002737\n",
      "Epoch 300/300, Loss: 0.009280404405305663\n",
      "Clinical-Only Model\n",
      "Test Accuracy: 71.42857142857143%\n",
      "Precision: 0.6363636363636364\n",
      "Recall: 0.7777777777777778\n",
      "F1-Score: 0.7\n",
      "\n",
      "Training Image-Only Model\n",
      "Train Features:  torch.Size([84, 4608])\n",
      "Test Features:  torch.Size([21, 4608])\n",
      "Train Labels:  torch.Size([84, 1])\n",
      "Test Labels:  torch.Size([21, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vn/9n4mdvbj54lbr8ffl0577zgw0000gn/T/ipykernel_70190/4210919272.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_features = torch.tensor(feature_set.reshape(len(feature_set), -1))\n",
      "/var/folders/vn/9n4mdvbj54lbr8ffl0577zgw0000gn/T/ipykernel_70190/4210919272.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_features = torch.tensor((test_clinical_embeddings if modality == 'Clinical' else test_image_features).reshape(len(test_labels), -1))\n",
      "/opt/anaconda3/envs/pascal/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.6941930921304793\n",
      "Epoch 2/300, Loss: 0.6930999056923957\n",
      "Epoch 3/300, Loss: 0.6967348427999587\n",
      "Epoch 4/300, Loss: 0.6744392790964672\n",
      "Epoch 5/300, Loss: 0.6920562560359637\n",
      "Epoch 6/300, Loss: 0.6849753136436144\n",
      "Epoch 7/300, Loss: 0.6810790133618173\n",
      "Epoch 8/300, Loss: 0.6779138059133575\n",
      "Epoch 9/300, Loss: 0.6814671758384931\n",
      "Epoch 10/300, Loss: 0.6997014131574404\n",
      "Epoch 11/300, Loss: 0.6915550437711534\n",
      "Epoch 12/300, Loss: 0.6813615613750049\n",
      "Epoch 13/300, Loss: 0.6728436315343493\n",
      "Epoch 14/300, Loss: 0.6789491570421627\n",
      "Epoch 15/300, Loss: 0.6749929317406246\n",
      "Epoch 16/300, Loss: 0.690930921406973\n",
      "Epoch 17/300, Loss: 0.6822790968276206\n",
      "Epoch 18/300, Loss: 0.6796494612381572\n",
      "Epoch 19/300, Loss: 0.6834891002093043\n",
      "Epoch 20/300, Loss: 0.6739480144211224\n",
      "Epoch 21/300, Loss: 0.683431058767296\n",
      "Epoch 22/300, Loss: 0.6826155373737925\n",
      "Epoch 23/300, Loss: 0.6712520739861897\n",
      "Epoch 24/300, Loss: 0.6641010705913816\n",
      "Epoch 25/300, Loss: 0.6768714863629568\n",
      "Epoch 26/300, Loss: 0.6591386716990244\n",
      "Epoch 27/300, Loss: 0.6748371358428683\n",
      "Epoch 28/300, Loss: 0.6846710982776824\n",
      "Epoch 29/300, Loss: 0.6730072629593667\n",
      "Epoch 30/300, Loss: 0.6734861053881191\n",
      "Epoch 31/300, Loss: 0.6643100845671835\n",
      "Epoch 32/300, Loss: 0.6759342270947638\n",
      "Epoch 33/300, Loss: 0.673795220042978\n",
      "Epoch 34/300, Loss: 0.6584268421644256\n",
      "Epoch 35/300, Loss: 0.6621276420496759\n",
      "Epoch 36/300, Loss: 0.6635478706586928\n",
      "Epoch 37/300, Loss: 0.656984636471385\n",
      "Epoch 38/300, Loss: 0.6677131869253659\n",
      "Epoch 39/300, Loss: 0.6635135188698769\n",
      "Epoch 40/300, Loss: 0.6667797597391265\n",
      "Epoch 41/300, Loss: 0.655976785435563\n",
      "Epoch 42/300, Loss: 0.6386103388809022\n",
      "Epoch 43/300, Loss: 0.668823222319285\n",
      "Epoch 44/300, Loss: 0.6688424217559042\n",
      "Epoch 45/300, Loss: 0.6577215741078059\n",
      "Epoch 46/300, Loss: 0.6455019417972792\n",
      "Epoch 47/300, Loss: 0.668701056923185\n",
      "Epoch 48/300, Loss: 0.6531875417346046\n",
      "Epoch 49/300, Loss: 0.6673040734160514\n",
      "Epoch 50/300, Loss: 0.6537825800478458\n",
      "Epoch 51/300, Loss: 0.6378895739714304\n",
      "Epoch 52/300, Loss: 0.6512622474914506\n",
      "Epoch 53/300, Loss: 0.6398929385911851\n",
      "Epoch 54/300, Loss: 0.6585384901790392\n",
      "Epoch 55/300, Loss: 0.6476658511729467\n",
      "Epoch 56/300, Loss: 0.6304475317398707\n",
      "Epoch 57/300, Loss: 0.6323562129622414\n",
      "Epoch 58/300, Loss: 0.6477228928179968\n",
      "Epoch 59/300, Loss: 0.6411861025151753\n",
      "Epoch 60/300, Loss: 0.6420125404284114\n",
      "Epoch 61/300, Loss: 0.6346114286709399\n",
      "Epoch 62/300, Loss: 0.6366211349765459\n",
      "Epoch 63/300, Loss: 0.6269608234010992\n",
      "Epoch 64/300, Loss: 0.6323703502615293\n",
      "Epoch 65/300, Loss: 0.6231799501748312\n",
      "Epoch 66/300, Loss: 0.6296704195085026\n",
      "Epoch 67/300, Loss: 0.6334200717863583\n",
      "Epoch 68/300, Loss: 0.6346696056425571\n",
      "Epoch 69/300, Loss: 0.6421513992938257\n",
      "Epoch 70/300, Loss: 0.6135240865073034\n",
      "Epoch 71/300, Loss: 0.6481477453240326\n",
      "Epoch 72/300, Loss: 0.6290770845399016\n",
      "Epoch 73/300, Loss: 0.6256579275996912\n",
      "Epoch 74/300, Loss: 0.6329570142995744\n",
      "Epoch 75/300, Loss: 0.6258114249933333\n",
      "Epoch 76/300, Loss: 0.6109539266853106\n",
      "Epoch 77/300, Loss: 0.6197944240910667\n",
      "Epoch 78/300, Loss: 0.6197716609707901\n",
      "Epoch 79/300, Loss: 0.6067182200827769\n",
      "Epoch 80/300, Loss: 0.6286730828384558\n",
      "Epoch 81/300, Loss: 0.6154164162774881\n",
      "Epoch 82/300, Loss: 0.590796831817854\n",
      "Epoch 83/300, Loss: 0.6204284745312872\n",
      "Epoch 84/300, Loss: 0.615312399076564\n",
      "Epoch 85/300, Loss: 0.6019875457776445\n",
      "Epoch 86/300, Loss: 0.6025996417516754\n",
      "Epoch 87/300, Loss: 0.5923117251978034\n",
      "Epoch 88/300, Loss: 0.6016738856477397\n",
      "Epoch 89/300, Loss: 0.6139798145741224\n",
      "Epoch 90/300, Loss: 0.6137625779069605\n",
      "Epoch 91/300, Loss: 0.6120210656275352\n",
      "Epoch 92/300, Loss: 0.5797543839684555\n",
      "Epoch 93/300, Loss: 0.6042357460551319\n",
      "Epoch 94/300, Loss: 0.589155379150595\n",
      "Epoch 95/300, Loss: 0.5978860496765092\n",
      "Epoch 96/300, Loss: 0.6039278333385786\n",
      "Epoch 97/300, Loss: 0.6062882139037052\n",
      "Epoch 98/300, Loss: 0.5864333120130357\n",
      "Epoch 99/300, Loss: 0.5768079416205486\n",
      "Epoch 100/300, Loss: 0.599978529759461\n",
      "Epoch 101/300, Loss: 0.5729164732176633\n",
      "Epoch 102/300, Loss: 0.5884948117392403\n",
      "Epoch 103/300, Loss: 0.5981927037771259\n",
      "Epoch 104/300, Loss: 0.5773814269119785\n",
      "Epoch 105/300, Loss: 0.6002365497960931\n",
      "Epoch 106/300, Loss: 0.5923725121344129\n",
      "Epoch 107/300, Loss: 0.5824587420072584\n",
      "Epoch 108/300, Loss: 0.5823951151576781\n",
      "Epoch 109/300, Loss: 0.5614386890970525\n",
      "Epoch 110/300, Loss: 0.5661672250190306\n",
      "Epoch 111/300, Loss: 0.5782309888108146\n",
      "Epoch 112/300, Loss: 0.5582334328376289\n",
      "Epoch 113/300, Loss: 0.5708881357596034\n",
      "Epoch 114/300, Loss: 0.5671970605173902\n",
      "Epoch 115/300, Loss: 0.5546256831980177\n",
      "Epoch 116/300, Loss: 0.5691480703563208\n",
      "Epoch 117/300, Loss: 0.5497602862749427\n",
      "Epoch 118/300, Loss: 0.5719937676829951\n",
      "Epoch 119/300, Loss: 0.5547614925252717\n",
      "Epoch 120/300, Loss: 0.5436657993566423\n",
      "Epoch 121/300, Loss: 0.555751619417043\n",
      "Epoch 122/300, Loss: 0.5481842453369782\n",
      "Epoch 123/300, Loss: 0.5467671871274\n",
      "Epoch 124/300, Loss: 0.5388695159428087\n",
      "Epoch 125/300, Loss: 0.5505653440154025\n",
      "Epoch 126/300, Loss: 0.5554108576893452\n",
      "Epoch 127/300, Loss: 0.535034990406573\n",
      "Epoch 128/300, Loss: 0.5367777252104133\n",
      "Epoch 129/300, Loss: 0.5599129466960827\n",
      "Epoch 130/300, Loss: 0.5527604427271788\n",
      "Epoch 131/300, Loss: 0.5492419552590165\n",
      "Epoch 132/300, Loss: 0.5289547026512169\n",
      "Epoch 133/300, Loss: 0.5458351401944778\n",
      "Epoch 134/300, Loss: 0.5178581430421522\n",
      "Epoch 135/300, Loss: 0.5352373894281863\n",
      "Epoch 136/300, Loss: 0.5178130219047445\n",
      "Epoch 137/300, Loss: 0.5278126571460494\n",
      "Epoch 138/300, Loss: 0.5093683249184063\n",
      "Epoch 139/300, Loss: 0.5556440648644985\n",
      "Epoch 140/300, Loss: 0.5110490111900228\n",
      "Epoch 141/300, Loss: 0.5366122046086405\n",
      "Epoch 142/300, Loss: 0.5137000727644634\n",
      "Epoch 143/300, Loss: 0.5380523033174021\n",
      "Epoch 144/300, Loss: 0.5075437373722836\n",
      "Epoch 145/300, Loss: 0.5316692830196449\n",
      "Epoch 146/300, Loss: 0.5113851437733198\n",
      "Epoch 147/300, Loss: 0.5159993424146835\n",
      "Epoch 148/300, Loss: 0.5238047172149111\n",
      "Epoch 149/300, Loss: 0.5196312236200485\n",
      "Epoch 150/300, Loss: 0.4975615573958272\n",
      "Epoch 151/300, Loss: 0.47701762657102553\n",
      "Epoch 152/300, Loss: 0.5166295273174044\n",
      "Epoch 153/300, Loss: 0.5071608739505921\n",
      "Epoch 154/300, Loss: 0.5131458273778359\n",
      "Epoch 155/300, Loss: 0.48314323519644814\n",
      "Epoch 156/300, Loss: 0.5188701622869952\n",
      "Epoch 157/300, Loss: 0.4804402492175411\n",
      "Epoch 158/300, Loss: 0.5110587441539836\n",
      "Epoch 159/300, Loss: 0.502047566716404\n",
      "Epoch 160/300, Loss: 0.522765345006649\n",
      "Epoch 161/300, Loss: 0.5113491413177966\n",
      "Epoch 162/300, Loss: 0.5138143427681089\n",
      "Epoch 163/300, Loss: 0.4603117793199739\n",
      "Epoch 164/300, Loss: 0.49190754374666584\n",
      "Epoch 165/300, Loss: 0.4905360682335283\n",
      "Epoch 166/300, Loss: 0.48025678963001284\n",
      "Epoch 167/300, Loss: 0.4988312451972715\n",
      "Epoch 168/300, Loss: 0.49106047854071394\n",
      "Epoch 169/300, Loss: 0.49959900298355414\n",
      "Epoch 170/300, Loss: 0.48005190160724204\n",
      "Epoch 171/300, Loss: 0.4927942439902663\n",
      "Epoch 172/300, Loss: 0.5123267994599051\n",
      "Epoch 173/300, Loss: 0.4878405821932358\n",
      "Epoch 174/300, Loss: 0.45418375083855134\n",
      "Epoch 175/300, Loss: 0.4888930708070153\n",
      "Epoch 176/300, Loss: 0.49445580344624995\n",
      "Epoch 177/300, Loss: 0.49732239970693454\n",
      "Epoch 178/300, Loss: 0.48065516937779085\n",
      "Epoch 179/300, Loss: 0.4745407595210487\n",
      "Epoch 180/300, Loss: 0.48104080898615176\n",
      "Epoch 181/300, Loss: 0.5002033159253187\n",
      "Epoch 182/300, Loss: 0.4664897918479428\n",
      "Epoch 183/300, Loss: 0.46732513370114337\n",
      "Epoch 184/300, Loss: 0.4828846020723826\n",
      "Epoch 185/300, Loss: 0.48578894166192704\n",
      "Epoch 186/300, Loss: 0.45539106881811414\n",
      "Epoch 187/300, Loss: 0.483394092214959\n",
      "Epoch 188/300, Loss: 0.4586774675907301\n",
      "Epoch 189/300, Loss: 0.4607505958216886\n",
      "Epoch 190/300, Loss: 0.4875321663384966\n",
      "Epoch 191/300, Loss: 0.45810876731411554\n",
      "Epoch 192/300, Loss: 0.4758955158038242\n",
      "Epoch 193/300, Loss: 0.4509110361576036\n",
      "Epoch 194/300, Loss: 0.4523334360869956\n",
      "Epoch 195/300, Loss: 0.4496487966527985\n",
      "Epoch 196/300, Loss: 0.452345420289064\n",
      "Epoch 197/300, Loss: 0.4399122120923407\n",
      "Epoch 198/300, Loss: 0.4498906795845306\n",
      "Epoch 199/300, Loss: 0.40897966699655325\n",
      "Epoch 200/300, Loss: 0.4407344727915188\n",
      "Epoch 201/300, Loss: 0.4360604924863548\n",
      "Epoch 202/300, Loss: 0.4261635711192347\n",
      "Epoch 203/300, Loss: 0.4621846394399784\n",
      "Epoch 204/300, Loss: 0.4289621968172945\n",
      "Epoch 205/300, Loss: 0.4482971131004715\n",
      "Epoch 206/300, Loss: 0.4517279645577738\n",
      "Epoch 207/300, Loss: 0.4413439505151473\n",
      "Epoch 208/300, Loss: 0.4242154260060661\n",
      "Epoch 209/300, Loss: 0.44520411459191\n",
      "Epoch 210/300, Loss: 0.42887242402191206\n",
      "Epoch 211/300, Loss: 0.44326771109869395\n",
      "Epoch 212/300, Loss: 0.39885883228687036\n",
      "Epoch 213/300, Loss: 0.4642685783622992\n",
      "Epoch 214/300, Loss: 0.4288896928809707\n",
      "Epoch 215/300, Loss: 0.4492415818212543\n",
      "Epoch 216/300, Loss: 0.42735559230805803\n",
      "Epoch 217/300, Loss: 0.4360856507146742\n",
      "Epoch 218/300, Loss: 0.44002757986083363\n",
      "Epoch 219/300, Loss: 0.45126375147984144\n",
      "Epoch 220/300, Loss: 0.464373669456888\n",
      "Epoch 221/300, Loss: 0.4397536196962132\n",
      "Epoch 222/300, Loss: 0.45183962334111866\n",
      "Epoch 223/300, Loss: 0.4213776864870895\n",
      "Epoch 224/300, Loss: 0.4048342482208335\n",
      "Epoch 225/300, Loss: 0.4042699491034173\n",
      "Epoch 226/300, Loss: 0.4511074167382633\n",
      "Epoch 227/300, Loss: 0.42382366517302306\n",
      "Epoch 228/300, Loss: 0.4582853909922137\n",
      "Epoch 229/300, Loss: 0.43446688166780706\n",
      "Epoch 230/300, Loss: 0.4025089497660182\n",
      "Epoch 231/300, Loss: 0.4378366266928857\n",
      "Epoch 232/300, Loss: 0.41883873247880254\n",
      "Epoch 233/300, Loss: 0.3938382526866917\n",
      "Epoch 234/300, Loss: 0.40386460961252896\n",
      "Epoch 235/300, Loss: 0.38874275371719186\n",
      "Epoch 236/300, Loss: 0.4350254936431329\n",
      "Epoch 237/300, Loss: 0.4056947943427076\n",
      "Epoch 238/300, Loss: 0.39039589624302434\n",
      "Epoch 239/300, Loss: 0.4168362157915475\n",
      "Epoch 240/300, Loss: 0.4226029814037905\n",
      "Epoch 241/300, Loss: 0.3994072670575406\n",
      "Epoch 242/300, Loss: 0.42044048482247964\n",
      "Epoch 243/300, Loss: 0.4227988126374493\n",
      "Epoch 244/300, Loss: 0.40466611577236716\n",
      "Epoch 245/300, Loss: 0.3836977750988321\n",
      "Epoch 246/300, Loss: 0.4328618639597802\n",
      "Epoch 247/300, Loss: 0.39738356809677844\n",
      "Epoch 248/300, Loss: 0.4355090428178387\n",
      "Epoch 249/300, Loss: 0.40146477168777245\n",
      "Epoch 250/300, Loss: 0.37954375576295263\n",
      "Epoch 251/300, Loss: 0.4016873575472432\n",
      "Epoch 252/300, Loss: 0.37455567035302856\n",
      "Epoch 253/300, Loss: 0.4038427964725911\n",
      "Epoch 254/300, Loss: 0.44028045970023977\n",
      "Epoch 255/300, Loss: 0.36748522301994463\n",
      "Epoch 256/300, Loss: 0.384767202116975\n",
      "Epoch 257/300, Loss: 0.43480437188831966\n",
      "Epoch 258/300, Loss: 0.405640822745075\n",
      "Epoch 259/300, Loss: 0.3855163118427126\n",
      "Epoch 260/300, Loss: 0.36360802368463263\n",
      "Epoch 261/300, Loss: 0.4103236921627762\n",
      "Epoch 262/300, Loss: 0.38186271047189146\n",
      "Epoch 263/300, Loss: 0.42978595275324205\n",
      "Epoch 264/300, Loss: 0.36886649169658386\n",
      "Epoch 265/300, Loss: 0.39295557516908003\n",
      "Epoch 266/300, Loss: 0.4016581915136071\n",
      "Epoch 267/300, Loss: 0.3519028718848772\n",
      "Epoch 268/300, Loss: 0.3501677185744311\n",
      "Epoch 269/300, Loss: 0.3798798394896685\n",
      "Epoch 270/300, Loss: 0.3973196422363496\n",
      "Epoch 271/300, Loss: 0.38517739202729034\n",
      "Epoch 272/300, Loss: 0.3742769115063384\n",
      "Epoch 273/300, Loss: 0.40199633841326604\n",
      "Epoch 274/300, Loss: 0.37144261508798254\n",
      "Epoch 275/300, Loss: 0.36179850171243916\n",
      "Epoch 276/300, Loss: 0.3411934677703054\n",
      "Epoch 277/300, Loss: 0.39061379065844326\n",
      "Epoch 278/300, Loss: 0.36143190398468583\n",
      "Epoch 279/300, Loss: 0.35081738990218236\n",
      "Epoch 280/300, Loss: 0.3947230055426059\n",
      "Epoch 281/300, Loss: 0.37814347997187897\n",
      "Epoch 282/300, Loss: 0.41475302150943855\n",
      "Epoch 283/300, Loss: 0.35419530100625707\n",
      "Epoch 284/300, Loss: 0.3681417822431069\n",
      "Epoch 285/300, Loss: 0.41685666397877086\n",
      "Epoch 286/300, Loss: 0.3762064420377664\n",
      "Epoch 287/300, Loss: 0.38211111537561054\n",
      "Epoch 288/300, Loss: 0.34948681336317394\n",
      "Epoch 289/300, Loss: 0.36299472933773724\n",
      "Epoch 290/300, Loss: 0.3433966700905341\n",
      "Epoch 291/300, Loss: 0.3550396580728938\n",
      "Epoch 292/300, Loss: 0.37060161765276417\n",
      "Epoch 293/300, Loss: 0.35694569234675283\n",
      "Epoch 294/300, Loss: 0.36847540402699713\n",
      "Epoch 295/300, Loss: 0.33063317285199273\n",
      "Epoch 296/300, Loss: 0.3749948675062226\n",
      "Epoch 297/300, Loss: 0.3430623372537195\n",
      "Epoch 298/300, Loss: 0.33372152902822044\n",
      "Epoch 299/300, Loss: 0.33634523627569296\n",
      "Epoch 300/300, Loss: 0.41079056727470364\n",
      "Image-Only Model\n",
      "Test Accuracy: 61.904761904761905%\n",
      "Precision: 0.5714285714285714\n",
      "Recall: 0.4444444444444444\n",
      "F1-Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Experiment: Train Clinical-only and Image-only Models\n",
    "for modality, feature_set in [('Clinical', train_clinical_embeddings), ('Image', train_image_features)]:\n",
    "    print(f\"\\nTraining {modality}-Only Model\")\n",
    "    \n",
    "    train_labels = train_labels.clone().detach().float().view(-1, 1)\n",
    "    test_labels = test_labels.clone().detach().float().view(-1, 1)\n",
    "    train_features = torch.tensor(feature_set.reshape(len(feature_set), -1))\n",
    "    test_features = torch.tensor((test_clinical_embeddings if modality == 'Clinical' else test_image_features).reshape(len(test_labels), -1))\n",
    "\n",
    "    print(\"Train Features: \", train_features.shape)\n",
    "    print(\"Test Features: \", test_features.shape)\n",
    "    print(\"Train Labels: \", train_labels.shape)\n",
    "    print(\"Test Labels: \", test_labels.shape)\n",
    "    \n",
    "    train_dataset = TensorDataset(train_features, train_labels)\n",
    "    test_dataset = TensorDataset(test_features, test_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    test_model = MLP(input_dim=train_features.shape[1])\n",
    "    test_optimizer = torch.optim.Adam(test_model.parameters(), lr=learning_rate, weight_decay=w_decay)\n",
    "    \n",
    "    epochs = 300\n",
    "    for epoch in range(epochs):\n",
    "        test_model.train()\n",
    "        total_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            test_optimizer.zero_grad()\n",
    "            output = test_model(features.float())\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(output.view(-1), labels.view(-1))\n",
    "            loss.backward()\n",
    "            test_optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "    \n",
    "    test_model.eval()\n",
    "    all_labels, all_predictions = [], []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            output = test_model(features.float())\n",
    "\n",
    "            pred = torch.sigmoid(output.squeeze()) >= 0.5\n",
    "            \n",
    "            all_labels.append(labels.cpu().numpy().flatten())\n",
    "            all_predictions.append(pred.cpu().numpy().flatten())\n",
    "\n",
    "            # Count correct predictions\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)  # Increment by the number of samples in this batch\n",
    "    \n",
    "    print(f\"{modality}-Only Model\")\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy}%\")\n",
    "\n",
    "    # Calculate Metrics\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
