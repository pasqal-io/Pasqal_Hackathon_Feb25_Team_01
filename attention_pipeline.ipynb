{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, GATConv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "n_clinical = 38 \n",
    "n_image_nodes = 6*6\n",
    "n_nodes = n_clinical + n_image_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples:  84\n",
      "Test Samples:  21\n",
      "Train labels shape: torch.Size([84])\n",
      "Test labels shape: torch.Size([21])\n"
     ]
    }
   ],
   "source": [
    "# Load Ground-Truth Values\n",
    "train_labels = pd.read_csv(\"data/labels/train_labels.csv\")\n",
    "train_labels = train_labels.iloc[:, 1].tolist()                 # (n_train,)\n",
    "test_labels = pd.read_csv(\"data/labels/test_labels.csv\")\n",
    "test_labels = test_labels.iloc[:, 1].tolist()                   # (n_test,)\n",
    "\n",
    "n_train = len(train_labels) # 84\n",
    "n_test = len(test_labels)   # 21\n",
    "\n",
    "print('Training Samples: ', n_train)\n",
    "print('Test Samples: ', n_test)\n",
    "\n",
    "# Convert to tensors\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "print(\"Train labels shape:\", train_labels.shape)                # Should be (n_train,)\n",
    "print(\"Test labels shape:\", test_labels.shape)                  # Should be (n_test,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Image Embeddings:  (84, 6, 6, 128)\n",
      "Train Clinical Embeddings:  (84, 38, 128)\n",
      "Test Image Embeddings:  (21, 6, 6, 128)\n",
      "Test Clinical Embeddings:  (21, 38, 128)\n"
     ]
    }
   ],
   "source": [
    "# Load and normalise Embeddings\n",
    "train_image_embeddings = np.load(\"data/image_embeddings/train_image_embeddings.npy\")             # (n_train, 6, 6, embed_dim)\n",
    "train_clinical_embeddings = np.load(\"data/clinical_data/train_embeddings.npy\")          # (n_train, 38, embed_dim)\n",
    "test_image_embeddings = np.load(\"data/image_embeddings/test_image_embeddings.npy\")               # (n_test, 6, 6, embed_dim)\n",
    "test_clinical_embeddings = np.load(\"data/clinical_data/test_embeddings.npy\")            # (n_test, 38, embed_dim)\n",
    "\n",
    "print(\"Train Image Embeddings: \", train_image_embeddings.shape)\n",
    "print(\"Train Clinical Embeddings: \", train_clinical_embeddings.shape)\n",
    "print(\"Test Image Embeddings: \",test_image_embeddings.shape)\n",
    "print(\"Test Clinical Embeddings: \", test_clinical_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureAttention(torch.nn.Module):\n",
    "    def __init__(self, clinical_dim, image_dim):\n",
    "        super(FeatureAttention, self).__init__()\n",
    "        self.attn_layer = torch.nn.Linear(clinical_dim + image_dim, 2)\n",
    "\n",
    "    def forward(self, clinical, image):\n",
    "        # Flatten features to (batch_size, clinical_dim + image_dim) for attention scoring\n",
    "        clinical_flat = clinical.mean(dim=-1)       # Shape: (batch_size, clinical_dim)\n",
    "        image_flat = image.mean(dim=-1)             # Shape: (batch_size, image_dim)\n",
    "\n",
    "        combined = torch.cat([clinical_flat, image_flat], dim=1)                        # Shape: (batch_size, clinical_dim + image_dim)\n",
    "        attn_weights = torch.softmax(self.attn_layer(combined), dim=1)                  # Learn weight for each feature type\n",
    "\n",
    "        # Expand attention weights and apply to original features\n",
    "        attn_clinical = attn_weights[:, 0].unsqueeze(1).unsqueeze(-1) * clinical        # Shape: (batch, clinical_dim, 128)\n",
    "        attn_image = attn_weights[:, 1].unsqueeze(1).unsqueeze(-1) * image              # Shape: (batch, image_dim, 128)\n",
    "\n",
    "        return torch.cat([attn_clinical, attn_image], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_attention = FeatureAttention(n_clinical, n_image_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Train Image Embeddings:  torch.Size([84, 36, 128])\n",
      "Combined Train Embeddings:  torch.Size([84, 74, 128])\n",
      "Reshaped Test Image Embeddings:  torch.Size([21, 36, 128])\n",
      "Combined Test Embeddings:  torch.Size([21, 74, 128])\n"
     ]
    }
   ],
   "source": [
    "# Reshape image embeddings to match size of clinical embeddings\n",
    "train_image_features = torch.tensor(train_image_embeddings.reshape(n_train, 36, embed_dim))                             # Shape: [n_train, 36, embed_dim]\n",
    "test_image_features = torch.tensor(test_image_embeddings.reshape(n_test, 36, embed_dim))                                # Shape: [n_test, 36, embed_dim]\n",
    "\n",
    "# Combine clinical and image features\n",
    "# train_patient_features = torch.cat([torch.tensor(train_clinical_embeddings), train_image_features], dim=1)              # Shape: [n_train, 74, embed_dim]\n",
    "# test_patient_features = torch.cat([torch.tensor(test_clinical_embeddings), test_image_features], dim=1)                 # Shape: [n_test, 74, embed_dim]\n",
    "\n",
    "# Feature Attention \n",
    "train_patient_features = feature_attention(torch.tensor(train_clinical_embeddings).float(), train_image_features.float())  \n",
    "test_patient_features = feature_attention(torch.tensor(test_clinical_embeddings).float(), test_image_features.float())  \n",
    "\n",
    "print('Reshaped Train Image Embeddings: ', train_image_features.shape)\n",
    "print('Combined Train Embeddings: ', train_patient_features.shape)\n",
    "print('Reshaped Test Image Embeddings: ', test_image_features.shape)\n",
    "print('Combined Test Embeddings: ', test_patient_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patient_edges(n_clinical, n_nodes):\n",
    "    \"\"\"\n",
    "    Creates bidirectional edges between clinical nodes and image nodes.\n",
    "    Adds a self-edge to each node.\n",
    "\n",
    "    Total edges = n_nodes (self-edges) + 2 * n_clinical * n_image_nodes (bidirectional edges)\n",
    "\n",
    "    Parameters:\n",
    "    - n_clinical: number of clinical nodes (for a specific patient)\n",
    "    - n_image_nodes: number of image nodes (for a specific patient)\n",
    "    \"\"\"\n",
    "    node_ids = np.expand_dims(np.arange(n_nodes, dtype=int), 0)\n",
    "    # self-edges = preserves some features of each own node during a graph convolution\n",
    "    self_edges = np.concatenate((node_ids, node_ids), 0)\n",
    "\n",
    "    # clinical nodes\n",
    "    c_array_asc = np.expand_dims(np.arange(n_clinical), 0)\n",
    "    all_edges = self_edges[:]\n",
    "\n",
    "    for i in range(n_clinical, n_nodes):\n",
    "        # image nodes\n",
    "        i_array = np.expand_dims(np.array([i]*n_clinical), 0)\n",
    "\n",
    "        # image --> clinical\n",
    "        inter_edges_ic = np.concatenate((i_array, c_array_asc), 0)\n",
    "        # clinical --> image\n",
    "        inter_edges_ci = np.concatenate((c_array_asc, i_array), 0)\n",
    "\n",
    "        # bidirectional edges\n",
    "        inter_edges_i = np.concatenate((inter_edges_ic, inter_edges_ci), 1)\n",
    "        all_edges = np.concatenate((all_edges, inter_edges_i), 1)\n",
    "\n",
    "    return torch.tensor(all_edges, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_list(patient_features, patient_labels):\n",
    "    \"\"\"\n",
    "    Generates a sub-graph for each patient given its embeddings\n",
    "\n",
    "    Parameters:\n",
    "    - patient_features: combined clinical and image embeddings of one patient\n",
    "    - patient_labels: groud truth values\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for i in range(len(patient_labels)):\n",
    "        # Create the graph for each patient\n",
    "        patient_edges = create_patient_edges(n_clinical, n_nodes)   # Shape: [2, num_edges]\n",
    "        patient_y = patient_labels[i]                               # Target label for this patient\n",
    "\n",
    "        data = Data(x=patient_features[i], edge_index=patient_edges, y=patient_y)\n",
    "        data_list.append(data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Patients:  84\n",
      "Test Patients:  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usama.Khatab\\Projects\\miniconda3\\envs\\hackathon\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_data_list = get_data_list(train_patient_features, train_labels)\n",
    "test_data_list = get_data_list(test_patient_features, test_labels)\n",
    "\n",
    "# Batch size 1 for individual patients\n",
    "train_loader = DataLoader(train_data_list, batch_size=1, shuffle=False, num_workers=0)  \n",
    "test_loader = DataLoader(test_data_list, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Train Patients: \", len(train_loader))\n",
    "print(\"Test Patients: \", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "We define the Graph Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)          # Second GCN layer\n",
    "        self.fc = torch.nn.Linear(hidden_channels, 1)                   # Fully connected layer for binary classification\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Apply graph convolution\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Global pooling (mean) across all nodes\n",
    "        x = global_mean_pool(x, batch)  # This will aggregate node features into one scalar per graph\n",
    "        \n",
    "        # Pass the aggregated feature through a fully connected layer to get a single logit\n",
    "        x = self.fc(x)  # Output size is (batch_size, 1)\n",
    "        return x  # Output a single logit for each patient (before applying sigmoid in loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Graph Attention Network\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, heads=2, dropout=0.5):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, dropout=dropout)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, 1)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)          # Aggregate node features\n",
    "        x = self.fc(x)                          # Binary classification output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Model Parameters\n",
    "learning_rate = 0.0001\n",
    "w_decay = 5e-4\n",
    "hidden_channels = 128\n",
    "\n",
    "# Initialize Model\n",
    "model = GCN(in_channels=embed_dim, hidden_channels=hidden_channels)\n",
    "# model = GAT(in_channels=embed_dim, hidden_channels=hidden_channels)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=w_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.6945177557922545\n",
      "Epoch 2/300, Loss: 0.6874157530920846\n",
      "Epoch 3/300, Loss: 0.6818149174962725\n",
      "Epoch 4/300, Loss: 0.6767099868683588\n",
      "Epoch 5/300, Loss: 0.6718790226039433\n",
      "Epoch 6/300, Loss: 0.6670009955054238\n",
      "Epoch 7/300, Loss: 0.6618031046929813\n",
      "Epoch 8/300, Loss: 0.6562490090727806\n",
      "Epoch 9/300, Loss: 0.6500730550005323\n",
      "Epoch 10/300, Loss: 0.6432474521653992\n",
      "Epoch 11/300, Loss: 0.6355129412951923\n",
      "Epoch 12/300, Loss: 0.6267759072638693\n",
      "Epoch 13/300, Loss: 0.6170415484479496\n",
      "Epoch 14/300, Loss: 0.6063769190084367\n",
      "Epoch 15/300, Loss: 0.594651318022183\n",
      "Epoch 16/300, Loss: 0.5820070091812384\n",
      "Epoch 17/300, Loss: 0.5685073990552199\n",
      "Epoch 18/300, Loss: 0.5543968578179678\n",
      "Epoch 19/300, Loss: 0.539655235462955\n",
      "Epoch 20/300, Loss: 0.524535685955059\n",
      "Epoch 21/300, Loss: 0.5093473966366479\n",
      "Epoch 22/300, Loss: 0.4942114075557107\n",
      "Epoch 23/300, Loss: 0.4789400906080291\n",
      "Epoch 24/300, Loss: 0.46408143479909214\n",
      "Epoch 25/300, Loss: 0.44941037478635\n",
      "Epoch 26/300, Loss: 0.4349288571005066\n",
      "Epoch 27/300, Loss: 0.420686367743959\n",
      "Epoch 28/300, Loss: 0.40677708017063285\n",
      "Epoch 29/300, Loss: 0.39297941929128555\n",
      "Epoch 30/300, Loss: 0.3790657252982436\n",
      "Epoch 31/300, Loss: 0.3658963622569683\n",
      "Epoch 32/300, Loss: 0.35243608377918245\n",
      "Epoch 33/300, Loss: 0.33984643532451064\n",
      "Epoch 34/300, Loss: 0.326975378360311\n",
      "Epoch 35/300, Loss: 0.3148968821263961\n",
      "Epoch 36/300, Loss: 0.30273979437160525\n",
      "Epoch 37/300, Loss: 0.2906670065823987\n",
      "Epoch 38/300, Loss: 0.27915587082610555\n",
      "Epoch 39/300, Loss: 0.26802322923599387\n",
      "Epoch 40/300, Loss: 0.2567481701359308\n",
      "Epoch 41/300, Loss: 0.24586874138566644\n",
      "Epoch 42/300, Loss: 0.2353939327378092\n",
      "Epoch 43/300, Loss: 0.22534733044379945\n",
      "Epoch 44/300, Loss: 0.2156086880852291\n",
      "Epoch 45/300, Loss: 0.2056185143332446\n",
      "Epoch 46/300, Loss: 0.19658767906851912\n",
      "Epoch 47/300, Loss: 0.1876596488693308\n",
      "Epoch 48/300, Loss: 0.17898494466776138\n",
      "Epoch 49/300, Loss: 0.17040845888879544\n",
      "Epoch 50/300, Loss: 0.16247698158977505\n",
      "Epoch 51/300, Loss: 0.15464994916467167\n",
      "Epoch 52/300, Loss: 0.14726044855264522\n",
      "Epoch 53/300, Loss: 0.13980899898524513\n",
      "Epoch 54/300, Loss: 0.13324134378087496\n",
      "Epoch 55/300, Loss: 0.1263941032428279\n",
      "Epoch 56/300, Loss: 0.1199845217827585\n",
      "Epoch 57/300, Loss: 0.11403232766832887\n",
      "Epoch 58/300, Loss: 0.10822635737098435\n",
      "Epoch 59/300, Loss: 0.10274812968788995\n",
      "Epoch 60/300, Loss: 0.09731280081233049\n",
      "Epoch 61/300, Loss: 0.09244766291570679\n",
      "Epoch 62/300, Loss: 0.08763106104904528\n",
      "Epoch 63/300, Loss: 0.08299754515413886\n",
      "Epoch 64/300, Loss: 0.07871670884571272\n",
      "Epoch 65/300, Loss: 0.07464830778092489\n",
      "Epoch 66/300, Loss: 0.07058942098043579\n",
      "Epoch 67/300, Loss: 0.06688296590020909\n",
      "Epoch 68/300, Loss: 0.06340808073782087\n",
      "Epoch 69/300, Loss: 0.060086497850691215\n",
      "Epoch 70/300, Loss: 0.05687578387786949\n",
      "Epoch 71/300, Loss: 0.05378646157642547\n",
      "Epoch 72/300, Loss: 0.050940522599819904\n",
      "Epoch 73/300, Loss: 0.04824290721236799\n",
      "Epoch 74/300, Loss: 0.045749328870787816\n",
      "Epoch 75/300, Loss: 0.04324280052301111\n",
      "Epoch 76/300, Loss: 0.04093344199702033\n",
      "Epoch 77/300, Loss: 0.038856474366612236\n",
      "Epoch 78/300, Loss: 0.036706801095124184\n",
      "Epoch 79/300, Loss: 0.034821235658592777\n",
      "Epoch 80/300, Loss: 0.03295395496840705\n",
      "Epoch 81/300, Loss: 0.031241604002030073\n",
      "Epoch 82/300, Loss: 0.029652273385999416\n",
      "Epoch 83/300, Loss: 0.028138926942132242\n",
      "Epoch 84/300, Loss: 0.02672836710100556\n",
      "Epoch 85/300, Loss: 0.02521164862903792\n",
      "Epoch 86/300, Loss: 0.0240050765202856\n",
      "Epoch 87/300, Loss: 0.022812587287237222\n",
      "Epoch 88/300, Loss: 0.021685497216114784\n",
      "Epoch 89/300, Loss: 0.020601945489537803\n",
      "Epoch 90/300, Loss: 0.019591796011581612\n",
      "Epoch 91/300, Loss: 0.018673239416582617\n",
      "Epoch 92/300, Loss: 0.017782664549252862\n",
      "Epoch 93/300, Loss: 0.016970501041599834\n",
      "Epoch 94/300, Loss: 0.016189864912816838\n",
      "Epoch 95/300, Loss: 0.015466961821655275\n",
      "Epoch 96/300, Loss: 0.01474217860306913\n",
      "Epoch 97/300, Loss: 0.014170508089963854\n",
      "Epoch 98/300, Loss: 0.013542170645192237\n",
      "Epoch 99/300, Loss: 0.012992781051920503\n",
      "Epoch 100/300, Loss: 0.012416560565242799\n",
      "Epoch 101/300, Loss: 0.011931132731111\n",
      "Epoch 102/300, Loss: 0.011479823526734293\n",
      "Epoch 103/300, Loss: 0.011040756282191836\n",
      "Epoch 104/300, Loss: 0.010603054204019339\n",
      "Epoch 105/300, Loss: 0.010250917201284284\n",
      "Epoch 106/300, Loss: 0.009881587843134622\n",
      "Epoch 107/300, Loss: 0.009542090315513937\n",
      "Epoch 108/300, Loss: 0.009181868278785305\n",
      "Epoch 109/300, Loss: 0.008917752741884433\n",
      "Epoch 110/300, Loss: 0.008614044664552284\n",
      "Epoch 111/300, Loss: 0.008370508056600471\n",
      "Epoch 112/300, Loss: 0.008086243083501577\n",
      "Epoch 113/300, Loss: 0.007863427650866905\n",
      "Epoch 114/300, Loss: 0.0076304519727023274\n",
      "Epoch 115/300, Loss: 0.007446936402007289\n",
      "Epoch 116/300, Loss: 0.007237402136362412\n",
      "Epoch 117/300, Loss: 0.007052949468710897\n",
      "Epoch 118/300, Loss: 0.006872586585099445\n",
      "Epoch 119/300, Loss: 0.006720150798571467\n",
      "Epoch 120/300, Loss: 0.00655783733844828\n",
      "Epoch 121/300, Loss: 0.006427184393105888\n",
      "Epoch 122/300, Loss: 0.006275675671607435\n",
      "Epoch 123/300, Loss: 0.006135575466844655\n",
      "Epoch 124/300, Loss: 0.006022795809009213\n",
      "Epoch 125/300, Loss: 0.005921943752715227\n",
      "Epoch 126/300, Loss: 0.0057953119319146635\n",
      "Epoch 127/300, Loss: 0.005694735183962642\n",
      "Epoch 128/300, Loss: 0.005600455348593684\n",
      "Epoch 129/300, Loss: 0.005514552818242789\n",
      "Epoch 130/300, Loss: 0.005429758371146168\n",
      "Epoch 131/300, Loss: 0.005330883610007558\n",
      "Epoch 132/300, Loss: 0.0052538795949994025\n",
      "Epoch 133/300, Loss: 0.005175293641768587\n",
      "Epoch 134/300, Loss: 0.0051113172820957216\n",
      "Epoch 135/300, Loss: 0.005034317989211125\n",
      "Epoch 136/300, Loss: 0.004970196081211768\n",
      "Epoch 137/300, Loss: 0.004906958292481466\n",
      "Epoch 138/300, Loss: 0.004848068479478094\n",
      "Epoch 139/300, Loss: 0.004780166815895167\n",
      "Epoch 140/300, Loss: 0.004739649203229203\n",
      "Epoch 141/300, Loss: 0.004677107495889946\n",
      "Epoch 142/300, Loss: 0.004632547551566512\n",
      "Epoch 143/300, Loss: 0.004583696142146566\n",
      "Epoch 144/300, Loss: 0.004518414823483649\n",
      "Epoch 145/300, Loss: 0.004494492568975669\n",
      "Epoch 146/300, Loss: 0.004426405785882926\n",
      "Epoch 147/300, Loss: 0.00439273044281925\n",
      "Epoch 148/300, Loss: 0.0043452397262138545\n",
      "Epoch 149/300, Loss: 0.004301655722488597\n",
      "Epoch 150/300, Loss: 0.004255701426943928\n",
      "Epoch 151/300, Loss: 0.004216279919422051\n",
      "Epoch 152/300, Loss: 0.004184183642315563\n",
      "Epoch 153/300, Loss: 0.004143456518174001\n",
      "Epoch 154/300, Loss: 0.004109403843872663\n",
      "Epoch 155/300, Loss: 0.004071442889577678\n",
      "Epoch 156/300, Loss: 0.004041394058184521\n",
      "Epoch 157/300, Loss: 0.0039867681553978854\n",
      "Epoch 158/300, Loss: 0.003971179041102483\n",
      "Epoch 159/300, Loss: 0.003930480213690622\n",
      "Epoch 160/300, Loss: 0.0038984209403288787\n",
      "Epoch 161/300, Loss: 0.003879106178390215\n",
      "Epoch 162/300, Loss: 0.0038336107895809725\n",
      "Epoch 163/300, Loss: 0.0037960123287834706\n",
      "Epoch 164/300, Loss: 0.00377374217927822\n",
      "Epoch 165/300, Loss: 0.0037412834938741583\n",
      "Epoch 166/300, Loss: 0.003712122304618233\n",
      "Epoch 167/300, Loss: 0.0036976414536137475\n",
      "Epoch 168/300, Loss: 0.003662031104324049\n",
      "Epoch 169/300, Loss: 0.003636700711483529\n",
      "Epoch 170/300, Loss: 0.0036078106615784903\n",
      "Epoch 171/300, Loss: 0.0035945264942089834\n",
      "Epoch 172/300, Loss: 0.0035661183952855864\n",
      "Epoch 173/300, Loss: 0.0035487745324402063\n",
      "Epoch 174/300, Loss: 0.003509650064231011\n",
      "Epoch 175/300, Loss: 0.003494476790938384\n",
      "Epoch 176/300, Loss: 0.003474383720068173\n",
      "Epoch 177/300, Loss: 0.003461547426976437\n",
      "Epoch 178/300, Loss: 0.0034252814663143097\n",
      "Epoch 179/300, Loss: 0.003418871363702299\n",
      "Epoch 180/300, Loss: 0.0033979924019026967\n",
      "Epoch 181/300, Loss: 0.0033715996448939003\n",
      "Epoch 182/300, Loss: 0.0033483565127433394\n",
      "Epoch 183/300, Loss: 0.003335482954502922\n",
      "Epoch 184/300, Loss: 0.0033279086999387615\n",
      "Epoch 185/300, Loss: 0.003308378538244669\n",
      "Epoch 186/300, Loss: 0.0032867288779330735\n",
      "Epoch 187/300, Loss: 0.0032763765763559115\n",
      "Epoch 188/300, Loss: 0.003260162591365301\n",
      "Epoch 189/300, Loss: 0.0032522506652871946\n",
      "Epoch 190/300, Loss: 0.0032390210529883285\n",
      "Epoch 191/300, Loss: 0.003229687841365432\n",
      "Epoch 192/300, Loss: 0.0032105970697710938\n",
      "Epoch 193/300, Loss: 0.0031982598044796888\n",
      "Epoch 194/300, Loss: 0.0031919978134899466\n",
      "Epoch 195/300, Loss: 0.003173312371077721\n",
      "Epoch 196/300, Loss: 0.003166126397557872\n",
      "Epoch 197/300, Loss: 0.0031657250227721044\n",
      "Epoch 198/300, Loss: 0.0031422191284855997\n",
      "Epoch 199/300, Loss: 0.00313265251972295\n",
      "Epoch 200/300, Loss: 0.003130389008160301\n",
      "Epoch 201/300, Loss: 0.003123902779888948\n",
      "Epoch 202/300, Loss: 0.00311490309441142\n",
      "Epoch 203/300, Loss: 0.003090045804462958\n",
      "Epoch 204/300, Loss: 0.0031006157563631966\n",
      "Epoch 205/300, Loss: 0.003090158452924353\n",
      "Epoch 206/300, Loss: 0.0030670193932022657\n",
      "Epoch 207/300, Loss: 0.0030670197287164854\n",
      "Epoch 208/300, Loss: 0.003060469418062896\n",
      "Epoch 209/300, Loss: 0.003057937028418458\n",
      "Epoch 210/300, Loss: 0.003051866684640509\n",
      "Epoch 211/300, Loss: 0.0030464005171264574\n",
      "Epoch 212/300, Loss: 0.0030420091428973806\n",
      "Epoch 213/300, Loss: 0.0030348543659945463\n",
      "Epoch 214/300, Loss: 0.0030243311407583276\n",
      "Epoch 215/300, Loss: 0.003026530232175145\n",
      "Epoch 216/300, Loss: 0.003017348987408871\n",
      "Epoch 217/300, Loss: 0.003011368961546354\n",
      "Epoch 218/300, Loss: 0.0030059882079252346\n",
      "Epoch 219/300, Loss: 0.003004773782999068\n",
      "Epoch 220/300, Loss: 0.003010618238531088\n",
      "Epoch 221/300, Loss: 0.002998456283562217\n",
      "Epoch 222/300, Loss: 0.0029907525446927893\n",
      "Epoch 223/300, Loss: 0.002996970258875126\n",
      "Epoch 224/300, Loss: 0.0029831815658474575\n",
      "Epoch 225/300, Loss: 0.0029880808204191837\n",
      "Epoch 226/300, Loss: 0.002974064514654561\n",
      "Epoch 227/300, Loss: 0.0029784355590487216\n",
      "Epoch 228/300, Loss: 0.0029783264432014433\n",
      "Epoch 229/300, Loss: 0.0029710696836994814\n",
      "Epoch 230/300, Loss: 0.0029685856930186105\n",
      "Epoch 231/300, Loss: 0.002967140433502737\n",
      "Epoch 232/300, Loss: 0.002959920709837691\n",
      "Epoch 233/300, Loss: 0.0029574021626241156\n",
      "Epoch 234/300, Loss: 0.002966079189653282\n",
      "Epoch 235/300, Loss: 0.002953084636140989\n",
      "Epoch 236/300, Loss: 0.0029470556147486384\n",
      "Epoch 237/300, Loss: 0.002958017454867599\n",
      "Epoch 238/300, Loss: 0.0029465040308062655\n",
      "Epoch 239/300, Loss: 0.002945541488218155\n",
      "Epoch 240/300, Loss: 0.00294691730029939\n",
      "Epoch 241/300, Loss: 0.002946042655052579\n",
      "Epoch 242/300, Loss: 0.0029482150791427636\n",
      "Epoch 243/300, Loss: 0.0029423005389072423\n",
      "Epoch 244/300, Loss: 0.0029412424720310546\n",
      "Epoch 245/300, Loss: 0.0029392086882382494\n",
      "Epoch 246/300, Loss: 0.00293343311010965\n",
      "Epoch 247/300, Loss: 0.002937412511614517\n",
      "Epoch 248/300, Loss: 0.0029304849481370694\n",
      "Epoch 249/300, Loss: 0.0029232176243777445\n",
      "Epoch 250/300, Loss: 0.0029324940413791487\n",
      "Epoch 251/300, Loss: 0.0029339282224483333\n",
      "Epoch 252/300, Loss: 0.0029261494541530163\n",
      "Epoch 253/300, Loss: 0.0029303000090699187\n",
      "Epoch 254/300, Loss: 0.0029194991351290797\n",
      "Epoch 255/300, Loss: 0.0029122577087441585\n",
      "Epoch 256/300, Loss: 0.0029283364238894124\n",
      "Epoch 257/300, Loss: 0.002923769849172821\n",
      "Epoch 258/300, Loss: 0.002917486542357107\n",
      "Epoch 259/300, Loss: 0.0029257111272633957\n",
      "Epoch 260/300, Loss: 0.002916695241623978\n",
      "Epoch 261/300, Loss: 0.002913182707060747\n",
      "Epoch 262/300, Loss: 0.0029153136195498334\n",
      "Epoch 263/300, Loss: 0.0029124623538113576\n",
      "Epoch 264/300, Loss: 0.002908439556080788\n",
      "Epoch 265/300, Loss: 0.002916101871721686\n",
      "Epoch 266/300, Loss: 0.0029127322344540594\n",
      "Epoch 267/300, Loss: 0.002914513794880777\n",
      "Epoch 268/300, Loss: 0.0028999057004895014\n",
      "Epoch 269/300, Loss: 0.002917667417228082\n",
      "Epoch 270/300, Loss: 0.0028989357527359786\n",
      "Epoch 271/300, Loss: 0.002906128569043582\n",
      "Epoch 272/300, Loss: 0.0029054903375680396\n",
      "Epoch 273/300, Loss: 0.0028947579797494157\n",
      "Epoch 274/300, Loss: 0.0029049519741320075\n",
      "Epoch 275/300, Loss: 0.002894447321695214\n",
      "Epoch 276/300, Loss: 0.002894479267902333\n",
      "Epoch 277/300, Loss: 0.002904659563638136\n",
      "Epoch 278/300, Loss: 0.0028973949923021165\n",
      "Epoch 279/300, Loss: 0.002895399942600209\n",
      "Epoch 280/300, Loss: 0.0028926594964416167\n",
      "Epoch 281/300, Loss: 0.0029007813634060703\n",
      "Epoch 282/300, Loss: 0.0028928645838588877\n",
      "Epoch 283/300, Loss: 0.0028816835043198267\n",
      "Epoch 284/300, Loss: 0.002889601694504119\n",
      "Epoch 285/300, Loss: 0.002889854924507604\n",
      "Epoch 286/300, Loss: 0.0028868641147371797\n",
      "Epoch 287/300, Loss: 0.002890859636548406\n",
      "Epoch 288/300, Loss: 0.002892374622865375\n",
      "Epoch 289/300, Loss: 0.002881217870414806\n",
      "Epoch 290/300, Loss: 0.002883030126211917\n",
      "Epoch 291/300, Loss: 0.002883497472836217\n",
      "Epoch 292/300, Loss: 0.0028840068338389473\n",
      "Epoch 293/300, Loss: 0.0028891751699540065\n",
      "Epoch 294/300, Loss: 0.0028796252945526356\n",
      "Epoch 295/300, Loss: 0.002885826697721649\n",
      "Epoch 296/300, Loss: 0.002881756940061293\n",
      "Epoch 297/300, Loss: 0.002874535156888804\n",
      "Epoch 298/300, Loss: 0.002878749761475909\n",
      "Epoch 299/300, Loss: 0.00287655070522622\n",
      "Epoch 300/300, Loss: 0.0028865953034863338\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "train_losses = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for data in train_loader:                                               # Iterate over each batch (here, each batch is one patient)\n",
    "                                                                            # Data object contains 'x' (features), 'edge_index' (graph edges), 'y' (labels)\n",
    "        patient_features = data.x                                           # Shape: (num_nodes, in_channels)\n",
    "        patient_edges = data.edge_index                                     # Shape: (2, num_edges)\n",
    "        patient_label = data.y.float()                                      # Target label\n",
    "        batch = data.batch\n",
    "\n",
    "        # Ensure correct format\n",
    "        patient_features = patient_features.float()\n",
    "        patient_edges = patient_edges.to(torch.long)                 \n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(patient_features, patient_edges, batch)               # Output shape: (1, 1)\n",
    "        \n",
    "        # Binary Classification Loss\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(output.view(-1), patient_label)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Print loss after each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 66.66666666666667%\n",
      "Precision: 0.5833333333333334\n",
      "Recall: 0.7777777777777778\n",
      "F1-Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "model.eval() \n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:                            # Iterate over each batch (i.e. one patient)\n",
    "        patient_features = data.x                       # Get features (shape: [num_nodes, in_channels])\n",
    "        patient_edges = data.edge_index                 # Get edges (shape: [2, num_edges])\n",
    "        patient_label = data.y.float()                  # Get label (shape: [1])\n",
    "\n",
    "        # Ensure correct format\n",
    "        patient_features = patient_features.float()    \n",
    "        patient_edges = patient_edges.to(torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(patient_features, patient_edges, data.batch)  # Use the batch info to aggregate across nodes\n",
    "\n",
    "        # Apply sigmoid to the output logits and get the predicted class (0 or 1)\n",
    "        pred = torch.sigmoid(output.squeeze())\n",
    "        predicted_class = (pred >= 0.5).float()                     # Threshold at 0.5 to classify as 0 or 1\n",
    "        \n",
    "        # Collect the labels and predictions for metrics\n",
    "        all_labels.append(patient_label.cpu().numpy())\n",
    "        all_predictions.append(predicted_class.cpu().numpy())\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct += (predicted_class == patient_label).sum().item()\n",
    "        total += patient_label.size(0)  # Increment by the number of samples in this batch\n",
    "\n",
    "# Accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy}%\")\n",
    "\n",
    "# Calculate Metrics\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "Test classification with clinical and image embeddings only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)             # Binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Clinical-Only Model\n",
      "Train Features:  torch.Size([84, 4864])\n",
      "Test Features:  torch.Size([21, 4864])\n",
      "Train Labels:  torch.Size([84, 1])\n",
      "Test Labels:  torch.Size([21, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usama.Khatab\\Projects\\miniconda3\\envs\\hackathon\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.805207200525772\n",
      "Epoch 2/300, Loss: 0.717716484108851\n",
      "Epoch 3/300, Loss: 0.6779803152284807\n",
      "Epoch 4/300, Loss: 0.631925510153884\n",
      "Epoch 5/300, Loss: 0.6164368123836106\n",
      "Epoch 6/300, Loss: 0.5889543978985221\n",
      "Epoch 7/300, Loss: 0.5273988866247237\n",
      "Epoch 8/300, Loss: 0.5107260075608446\n",
      "Epoch 9/300, Loss: 0.5157342536951459\n",
      "Epoch 10/300, Loss: 0.48263290931381997\n",
      "Epoch 11/300, Loss: 0.4422927867077912\n",
      "Epoch 12/300, Loss: 0.45178593498643577\n",
      "Epoch 13/300, Loss: 0.4650294520529098\n",
      "Epoch 14/300, Loss: 0.4739010718289397\n",
      "Epoch 15/300, Loss: 0.430178089280768\n",
      "Epoch 16/300, Loss: 0.4055940891308377\n",
      "Epoch 17/300, Loss: 0.39742400297490393\n",
      "Epoch 18/300, Loss: 0.3576480856389112\n",
      "Epoch 19/300, Loss: 0.3897494666932852\n",
      "Epoch 20/300, Loss: 0.3641873297199795\n",
      "Epoch 21/300, Loss: 0.35736193207404066\n",
      "Epoch 22/300, Loss: 0.30325554532464594\n",
      "Epoch 23/300, Loss: 0.32348449399831314\n",
      "Epoch 24/300, Loss: 0.3654999934385506\n",
      "Epoch 25/300, Loss: 0.2941426988151458\n",
      "Epoch 26/300, Loss: 0.2874343356801373\n",
      "Epoch 27/300, Loss: 0.25341721783379834\n",
      "Epoch 28/300, Loss: 0.270042912141716\n",
      "Epoch 29/300, Loss: 0.26770147750453144\n",
      "Epoch 30/300, Loss: 0.21662402832310232\n",
      "Epoch 31/300, Loss: 0.2521555168314946\n",
      "Epoch 32/300, Loss: 0.27213628059914863\n",
      "Epoch 33/300, Loss: 0.24863821561851296\n",
      "Epoch 34/300, Loss: 0.23196763226371786\n",
      "Epoch 35/300, Loss: 0.2207849479986006\n",
      "Epoch 36/300, Loss: 0.21458714655793593\n",
      "Epoch 37/300, Loss: 0.2293791580572485\n",
      "Epoch 38/300, Loss: 0.22416876427461743\n",
      "Epoch 39/300, Loss: 0.26513312968156455\n",
      "Epoch 40/300, Loss: 0.1855257210594183\n",
      "Epoch 41/300, Loss: 0.23436076308171513\n",
      "Epoch 42/300, Loss: 0.1700624576619486\n",
      "Epoch 43/300, Loss: 0.16370193210957995\n",
      "Epoch 44/300, Loss: 0.19080375009544814\n",
      "Epoch 45/300, Loss: 0.2014290667114727\n",
      "Epoch 46/300, Loss: 0.1484395402889644\n",
      "Epoch 47/300, Loss: 0.14859483145569166\n",
      "Epoch 48/300, Loss: 0.12621072016635462\n",
      "Epoch 49/300, Loss: 0.14655654478404148\n",
      "Epoch 50/300, Loss: 0.10109004765618308\n",
      "Epoch 51/300, Loss: 0.20154253799583985\n",
      "Epoch 52/300, Loss: 0.20835481980719872\n",
      "Epoch 53/300, Loss: 0.16502456742488614\n",
      "Epoch 54/300, Loss: 0.13808321312803531\n",
      "Epoch 55/300, Loss: 0.15367714384268666\n",
      "Epoch 56/300, Loss: 0.18187476665523372\n",
      "Epoch 57/300, Loss: 0.14652104930912946\n",
      "Epoch 58/300, Loss: 0.13503323784715002\n",
      "Epoch 59/300, Loss: 0.11977271206067455\n",
      "Epoch 60/300, Loss: 0.13620897311128305\n",
      "Epoch 61/300, Loss: 0.10880262038009478\n",
      "Epoch 62/300, Loss: 0.10132100576457706\n",
      "Epoch 63/300, Loss: 0.0834684573678037\n",
      "Epoch 64/300, Loss: 0.1152593302881704\n",
      "Epoch 65/300, Loss: 0.09041631763470387\n",
      "Epoch 66/300, Loss: 0.11464525599944322\n",
      "Epoch 67/300, Loss: 0.13042013862789217\n",
      "Epoch 68/300, Loss: 0.09368644294378765\n",
      "Epoch 69/300, Loss: 0.06867084977553913\n",
      "Epoch 70/300, Loss: 0.0938772199143288\n",
      "Epoch 71/300, Loss: 0.061871622560009394\n",
      "Epoch 72/300, Loss: 0.14501478603756504\n",
      "Epoch 73/300, Loss: 0.10324345696546705\n",
      "Epoch 74/300, Loss: 0.09739577480514691\n",
      "Epoch 75/300, Loss: 0.09634835212052525\n",
      "Epoch 76/300, Loss: 0.0651907528689083\n",
      "Epoch 77/300, Loss: 0.11114365593460361\n",
      "Epoch 78/300, Loss: 0.09327010221651291\n",
      "Epoch 79/300, Loss: 0.10109573171894885\n",
      "Epoch 80/300, Loss: 0.05937076032226659\n",
      "Epoch 81/300, Loss: 0.06449101537342752\n",
      "Epoch 82/300, Loss: 0.0944231534334459\n",
      "Epoch 83/300, Loss: 0.06021063482809713\n",
      "Epoch 84/300, Loss: 0.06736475597849398\n",
      "Epoch 85/300, Loss: 0.09935087144889093\n",
      "Epoch 86/300, Loss: 0.06868869122727321\n",
      "Epoch 87/300, Loss: 0.06869048315335019\n",
      "Epoch 88/300, Loss: 0.0869109702601731\n",
      "Epoch 89/300, Loss: 0.07007580085841661\n",
      "Epoch 90/300, Loss: 0.03464223269311423\n",
      "Epoch 91/300, Loss: 0.08012411032701768\n",
      "Epoch 92/300, Loss: 0.0713550035492167\n",
      "Epoch 93/300, Loss: 0.06343288252971975\n",
      "Epoch 94/300, Loss: 0.04186389125576497\n",
      "Epoch 95/300, Loss: 0.05370609695452858\n",
      "Epoch 96/300, Loss: 0.0700812130493367\n",
      "Epoch 97/300, Loss: 0.04963785842756793\n",
      "Epoch 98/300, Loss: 0.09581186070933172\n",
      "Epoch 99/300, Loss: 0.06254659866466569\n",
      "Epoch 100/300, Loss: 0.07685534325819617\n",
      "Epoch 101/300, Loss: 0.056195846020476434\n",
      "Epoch 102/300, Loss: 0.045080514680932146\n",
      "Epoch 103/300, Loss: 0.04996442729108031\n",
      "Epoch 104/300, Loss: 0.044150668596859954\n",
      "Epoch 105/300, Loss: 0.05003823902502658\n",
      "Epoch 106/300, Loss: 0.049263821139990886\n",
      "Epoch 107/300, Loss: 0.039601501760684145\n",
      "Epoch 108/300, Loss: 0.08197339189397831\n",
      "Epoch 109/300, Loss: 0.059913197589342784\n",
      "Epoch 110/300, Loss: 0.042363699600240945\n",
      "Epoch 111/300, Loss: 0.049620986408554286\n",
      "Epoch 112/300, Loss: 0.11720432609225834\n",
      "Epoch 113/300, Loss: 0.08962779783105561\n",
      "Epoch 114/300, Loss: 0.06587261502256385\n",
      "Epoch 115/300, Loss: 0.12682009237816255\n",
      "Epoch 116/300, Loss: 0.03932864452661489\n",
      "Epoch 117/300, Loss: 0.07090945409049963\n",
      "Epoch 118/300, Loss: 0.05062268706205654\n",
      "Epoch 119/300, Loss: 0.0505963441916309\n",
      "Epoch 120/300, Loss: 0.028045978491125452\n",
      "Epoch 121/300, Loss: 0.0709010510294545\n",
      "Epoch 122/300, Loss: 0.03376708808209702\n",
      "Epoch 123/300, Loss: 0.03798209247606222\n",
      "Epoch 124/300, Loss: 0.025527667462279947\n",
      "Epoch 125/300, Loss: 0.025400279013725408\n",
      "Epoch 126/300, Loss: 0.04791183818628956\n",
      "Epoch 127/300, Loss: 0.05068515463512357\n",
      "Epoch 128/300, Loss: 0.05598124189497298\n",
      "Epoch 129/300, Loss: 0.056725482948359704\n",
      "Epoch 130/300, Loss: 0.042653722108740154\n",
      "Epoch 131/300, Loss: 0.05861591837680206\n",
      "Epoch 132/300, Loss: 0.05601770480119194\n",
      "Epoch 133/300, Loss: 0.06633987925970472\n",
      "Epoch 134/300, Loss: 0.06955368021214203\n",
      "Epoch 135/300, Loss: 0.03840803297913521\n",
      "Epoch 136/300, Loss: 0.11783287421421114\n",
      "Epoch 137/300, Loss: 0.04909473271852342\n",
      "Epoch 138/300, Loss: 0.01919750830965989\n",
      "Epoch 139/300, Loss: 0.03234353427893708\n",
      "Epoch 140/300, Loss: 0.013083264963345442\n",
      "Epoch 141/300, Loss: 0.038481008268541776\n",
      "Epoch 142/300, Loss: 0.022675759089837485\n",
      "Epoch 143/300, Loss: 0.025905561767148876\n",
      "Epoch 144/300, Loss: 0.03397385499537085\n",
      "Epoch 145/300, Loss: 0.07986887531920266\n",
      "Epoch 146/300, Loss: 0.04386780345796221\n",
      "Epoch 147/300, Loss: 0.025660225413354592\n",
      "Epoch 148/300, Loss: 0.012117838683662726\n",
      "Epoch 149/300, Loss: 0.11798118055492626\n",
      "Epoch 150/300, Loss: 0.024736865310719076\n",
      "Epoch 151/300, Loss: 0.029211800509668557\n",
      "Epoch 152/300, Loss: 0.02397630268802331\n",
      "Epoch 153/300, Loss: 0.05290854882217786\n",
      "Epoch 154/300, Loss: 0.013935244259896657\n",
      "Epoch 155/300, Loss: 0.05196225056498476\n",
      "Epoch 156/300, Loss: 0.059745339969750386\n",
      "Epoch 157/300, Loss: 0.04624342055557983\n",
      "Epoch 158/300, Loss: 0.07540554250511414\n",
      "Epoch 159/300, Loss: 0.03977378089165105\n",
      "Epoch 160/300, Loss: 0.02757492460275606\n",
      "Epoch 161/300, Loss: 0.027770635995449953\n",
      "Epoch 162/300, Loss: 0.05717559234924819\n",
      "Epoch 163/300, Loss: 0.02538098154940202\n",
      "Epoch 164/300, Loss: 0.033670095677353275\n",
      "Epoch 165/300, Loss: 0.015041739039425803\n",
      "Epoch 166/300, Loss: 0.034559763780140126\n",
      "Epoch 167/300, Loss: 0.0052097372641989066\n",
      "Epoch 168/300, Loss: 0.01306204424023774\n",
      "Epoch 169/300, Loss: 0.019085643341347134\n",
      "Epoch 170/300, Loss: 0.02459351603722617\n",
      "Epoch 171/300, Loss: 0.02289105965803603\n",
      "Epoch 172/300, Loss: 0.02840459536999222\n",
      "Epoch 173/300, Loss: 0.025271320328090277\n",
      "Epoch 174/300, Loss: 0.04011404383560531\n",
      "Epoch 175/300, Loss: 0.056882502648764564\n",
      "Epoch 176/300, Loss: 0.0421450409850918\n",
      "Epoch 177/300, Loss: 0.024813366961274803\n",
      "Epoch 178/300, Loss: 0.062064318602199735\n",
      "Epoch 179/300, Loss: 0.0056763036425315876\n",
      "Epoch 180/300, Loss: 0.0306093342818756\n",
      "Epoch 181/300, Loss: 0.0509832213663782\n",
      "Epoch 182/300, Loss: 0.012205576756177939\n",
      "Epoch 183/300, Loss: 0.013286534848885216\n",
      "Epoch 184/300, Loss: 0.0177625589853506\n",
      "Epoch 185/300, Loss: 0.03696438893797855\n",
      "Epoch 186/300, Loss: 0.03966254158075302\n",
      "Epoch 187/300, Loss: 0.02617503852284124\n",
      "Epoch 188/300, Loss: 0.06145679755554845\n",
      "Epoch 189/300, Loss: 0.03485482380024558\n",
      "Epoch 190/300, Loss: 0.011645008269095822\n",
      "Epoch 191/300, Loss: 0.04776036513042731\n",
      "Epoch 192/300, Loss: 0.047071086033719144\n",
      "Epoch 193/300, Loss: 0.0643753985645427\n",
      "Epoch 194/300, Loss: 0.0293574863032883\n",
      "Epoch 195/300, Loss: 0.10027774143355726\n",
      "Epoch 196/300, Loss: 0.04933032432531329\n",
      "Epoch 197/300, Loss: 0.025385794309474904\n",
      "Epoch 198/300, Loss: 0.0584620172751186\n",
      "Epoch 199/300, Loss: 0.024246970752977114\n",
      "Epoch 200/300, Loss: 0.02828789240960698\n",
      "Epoch 201/300, Loss: 0.024584851252793527\n",
      "Epoch 202/300, Loss: 0.15074071181002002\n",
      "Epoch 203/300, Loss: 0.04835742947524038\n",
      "Epoch 204/300, Loss: 0.02369569256435264\n",
      "Epoch 205/300, Loss: 0.05559789772577838\n",
      "Epoch 206/300, Loss: 0.048832509036588825\n",
      "Epoch 207/300, Loss: 0.010612628860776343\n",
      "Epoch 208/300, Loss: 0.021813479151098256\n",
      "Epoch 209/300, Loss: 0.015813329109867456\n",
      "Epoch 210/300, Loss: 0.05267489667448705\n",
      "Epoch 211/300, Loss: 0.0661298991928202\n",
      "Epoch 212/300, Loss: 0.03980411814873812\n",
      "Epoch 213/300, Loss: 0.027578535841154744\n",
      "Epoch 214/300, Loss: 0.03325871819328855\n",
      "Epoch 215/300, Loss: 0.034780368862043734\n",
      "Epoch 216/300, Loss: 0.006050186912708998\n",
      "Epoch 217/300, Loss: 0.009581511032696761\n",
      "Epoch 218/300, Loss: 0.026482088539054364\n",
      "Epoch 219/300, Loss: 0.013458095805355745\n",
      "Epoch 220/300, Loss: 0.012376239052174094\n",
      "Epoch 221/300, Loss: 0.03012164737740492\n",
      "Epoch 222/300, Loss: 0.031752848856577455\n",
      "Epoch 223/300, Loss: 0.016915137836395304\n",
      "Epoch 224/300, Loss: 0.016832393348578938\n",
      "Epoch 225/300, Loss: 0.055545420800777734\n",
      "Epoch 226/300, Loss: 0.013775653878916216\n",
      "Epoch 227/300, Loss: 0.014889718201011534\n",
      "Epoch 228/300, Loss: 0.011314198387947914\n",
      "Epoch 229/300, Loss: 0.022236279089252436\n",
      "Epoch 230/300, Loss: 0.01264992629039434\n",
      "Epoch 231/300, Loss: 0.0033504666256368985\n",
      "Epoch 232/300, Loss: 0.010601520266178084\n",
      "Epoch 233/300, Loss: 0.050442968809659185\n",
      "Epoch 234/300, Loss: 0.02275633671312884\n",
      "Epoch 235/300, Loss: 0.013907910723616655\n",
      "Epoch 236/300, Loss: 0.034232642627737994\n",
      "Epoch 237/300, Loss: 0.01353894395684952\n",
      "Epoch 238/300, Loss: 0.006503881329245213\n",
      "Epoch 239/300, Loss: 0.006589051908145548\n",
      "Epoch 240/300, Loss: 0.007170918866564217\n",
      "Epoch 241/300, Loss: 0.006145939312074115\n",
      "Epoch 242/300, Loss: 0.00612895482721769\n",
      "Epoch 243/300, Loss: 0.005783331481590672\n",
      "Epoch 244/300, Loss: 0.008081733199440711\n",
      "Epoch 245/300, Loss: 0.028616973098068533\n",
      "Epoch 246/300, Loss: 0.03040342170845363\n",
      "Epoch 247/300, Loss: 0.01211481574121077\n",
      "Epoch 248/300, Loss: 0.0594816897493984\n",
      "Epoch 249/300, Loss: 0.035470027487163304\n",
      "Epoch 250/300, Loss: 0.013203304157260629\n",
      "Epoch 251/300, Loss: 0.020082113987552896\n",
      "Epoch 252/300, Loss: 0.025375521475771885\n",
      "Epoch 253/300, Loss: 0.04365115577313107\n",
      "Epoch 254/300, Loss: 0.11898579324586452\n",
      "Epoch 255/300, Loss: 0.03149081595489151\n",
      "Epoch 256/300, Loss: 0.009140992030246426\n",
      "Epoch 257/300, Loss: 0.01509578805473742\n",
      "Epoch 258/300, Loss: 0.013172838039593956\n",
      "Epoch 259/300, Loss: 0.006741944756617289\n",
      "Epoch 260/300, Loss: 0.003461885840711652\n",
      "Epoch 261/300, Loss: 0.007531613146808338\n",
      "Epoch 262/300, Loss: 0.008309201292764527\n",
      "Epoch 263/300, Loss: 0.0038567197656130233\n",
      "Epoch 264/300, Loss: 0.056284309860495305\n",
      "Epoch 265/300, Loss: 0.036109721816598765\n",
      "Epoch 266/300, Loss: 0.00871938840974062\n",
      "Epoch 267/300, Loss: 0.02900033235237191\n",
      "Epoch 268/300, Loss: 0.01379080589718765\n",
      "Epoch 269/300, Loss: 0.014073195759179905\n",
      "Epoch 270/300, Loss: 0.04535447340748347\n",
      "Epoch 271/300, Loss: 0.029291654997175936\n",
      "Epoch 272/300, Loss: 0.004618796919913848\n",
      "Epoch 273/300, Loss: 0.019061093168534122\n",
      "Epoch 274/300, Loss: 0.00645731042017459\n",
      "Epoch 275/300, Loss: 0.01373186922082038\n",
      "Epoch 276/300, Loss: 0.006886024417118212\n",
      "Epoch 277/300, Loss: 0.006642837260489981\n",
      "Epoch 278/300, Loss: 0.011629393223241127\n",
      "Epoch 279/300, Loss: 0.020687252431419063\n",
      "Epoch 280/300, Loss: 0.008695960890098539\n",
      "Epoch 281/300, Loss: 0.05103313102556203\n",
      "Epoch 282/300, Loss: 0.014163796099129485\n",
      "Epoch 283/300, Loss: 0.013367007977087331\n",
      "Epoch 284/300, Loss: 0.03864405331123337\n",
      "Epoch 285/300, Loss: 0.022660243617820135\n",
      "Epoch 286/300, Loss: 0.010232278573654966\n",
      "Epoch 287/300, Loss: 0.02223469823988627\n",
      "Epoch 288/300, Loss: 0.013501471283257769\n",
      "Epoch 289/300, Loss: 0.03641856948806067\n",
      "Epoch 290/300, Loss: 0.02627768267105483\n",
      "Epoch 291/300, Loss: 0.01925210251318295\n",
      "Epoch 292/300, Loss: 0.0304576629089923\n",
      "Epoch 293/300, Loss: 0.009165297902407266\n",
      "Epoch 294/300, Loss: 0.022307410985829213\n",
      "Epoch 295/300, Loss: 0.04923177973792952\n",
      "Epoch 296/300, Loss: 0.013621615243123705\n",
      "Epoch 297/300, Loss: 0.010986252600017668\n",
      "Epoch 298/300, Loss: 0.020725708899975544\n",
      "Epoch 299/300, Loss: 0.014739040603732096\n",
      "Epoch 300/300, Loss: 0.05563456820468341\n",
      "Clinical-Only Model\n",
      "Test Accuracy: 71.42857142857143%\n",
      "Precision: 0.6363636363636364\n",
      "Recall: 0.7777777777777778\n",
      "F1-Score: 0.7\n",
      "\n",
      "Training Image-Only Model\n",
      "Train Features:  torch.Size([84, 4608])\n",
      "Test Features:  torch.Size([21, 4608])\n",
      "Train Labels:  torch.Size([84, 1])\n",
      "Test Labels:  torch.Size([21, 1])\n",
      "Epoch 1/300, Loss: 11.446351941119204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usama.Khatab\\AppData\\Local\\Temp\\ipykernel_63320\\4210919272.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_features = torch.tensor(feature_set.reshape(len(feature_set), -1))\n",
      "C:\\Users\\Usama.Khatab\\AppData\\Local\\Temp\\ipykernel_63320\\4210919272.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_features = torch.tensor((test_clinical_embeddings if modality == 'Clinical' else test_image_features).reshape(len(test_labels), -1))\n",
      "c:\\Users\\Usama.Khatab\\Projects\\miniconda3\\envs\\hackathon\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300, Loss: 6.939684470906553\n",
      "Epoch 3/300, Loss: 2.142030619373643\n",
      "Epoch 4/300, Loss: 0.7001745959832555\n",
      "Epoch 5/300, Loss: 1.1064925378027728\n",
      "Epoch 6/300, Loss: 0.7420004567220098\n",
      "Epoch 7/300, Loss: 0.7229403146415107\n",
      "Epoch 8/300, Loss: 0.8903085277194068\n",
      "Epoch 9/300, Loss: 0.6895597619669778\n",
      "Epoch 10/300, Loss: 0.8404884851243835\n",
      "Epoch 11/300, Loss: 0.7001750199567705\n",
      "Epoch 12/300, Loss: 0.6894860452129727\n",
      "Epoch 13/300, Loss: 0.6894318071149644\n",
      "Epoch 14/300, Loss: 0.6893805059648695\n",
      "Epoch 15/300, Loss: 0.6893293453114373\n",
      "Epoch 16/300, Loss: 0.6892785586061931\n",
      "Epoch 17/300, Loss: 0.6892276150839669\n",
      "Epoch 18/300, Loss: 0.6891770341566631\n",
      "Epoch 19/300, Loss: 0.6891265461842219\n",
      "Epoch 20/300, Loss: 0.6890759226821718\n",
      "Epoch 21/300, Loss: 0.689027183112644\n",
      "Epoch 22/300, Loss: 0.6889768157686506\n",
      "Epoch 23/300, Loss: 0.6889266832953408\n",
      "Epoch 24/300, Loss: 0.6888797545716876\n",
      "Epoch 25/300, Loss: 0.6888296355803808\n",
      "Epoch 26/300, Loss: 0.6887809336185455\n",
      "Epoch 27/300, Loss: 0.68873297671477\n",
      "Epoch 28/300, Loss: 0.6886849233082363\n",
      "Epoch 29/300, Loss: 0.6886363157204219\n",
      "Epoch 30/300, Loss: 0.6885904265301568\n",
      "Epoch 31/300, Loss: 0.6885435765697843\n",
      "Epoch 32/300, Loss: 0.6884970657882237\n",
      "Epoch 33/300, Loss: 0.6884508934758958\n",
      "Epoch 34/300, Loss: 0.6884050645998546\n",
      "Epoch 35/300, Loss: 0.6883595763217836\n",
      "Epoch 36/300, Loss: 0.6883144470907393\n",
      "Epoch 37/300, Loss: 0.6882696556193488\n",
      "Epoch 38/300, Loss: 0.6882252111321404\n",
      "Epoch 39/300, Loss: 0.6881811051141649\n",
      "Epoch 40/300, Loss: 0.6881373524665833\n",
      "Epoch 41/300, Loss: 0.6880939361594972\n",
      "Epoch 42/300, Loss: 0.6880508732228052\n",
      "Epoch 43/300, Loss: 0.6880081444978714\n",
      "Epoch 44/300, Loss: 0.687965764885857\n",
      "Epoch 45/300, Loss: 0.6879237194856008\n",
      "Epoch 46/300, Loss: 0.6878820132641565\n",
      "Epoch 47/300, Loss: 0.687840644802366\n",
      "Epoch 48/300, Loss: 0.6877996105523336\n",
      "Epoch 49/300, Loss: 0.6877589098044804\n",
      "Epoch 50/300, Loss: 0.6877185368821734\n",
      "Epoch 51/300, Loss: 0.6876784988812038\n",
      "Epoch 52/300, Loss: 0.6876387794812521\n",
      "Epoch 53/300, Loss: 0.687599397840954\n",
      "Epoch 54/300, Loss: 0.6875603255771455\n",
      "Epoch 55/300, Loss: 0.6875215868155161\n",
      "Epoch 56/300, Loss: 0.687483161687851\n",
      "Epoch 57/300, Loss: 0.6874450558707828\n",
      "Epoch 58/300, Loss: 0.6874072594302041\n",
      "Epoch 59/300, Loss: 0.6873697759140105\n",
      "Epoch 60/300, Loss: 0.6873326088700976\n",
      "Epoch 61/300, Loss: 0.6872957341727757\n",
      "Epoch 62/300, Loss: 0.6872591837531045\n",
      "Epoch 63/300, Loss: 0.6872229327758154\n",
      "Epoch 64/300, Loss: 0.6871869734355381\n",
      "Epoch 65/300, Loss: 0.6871513156663804\n",
      "Epoch 66/300, Loss: 0.6871159488246554\n",
      "Epoch 67/300, Loss: 0.6870808885211036\n",
      "Epoch 68/300, Loss: 0.6870461077917189\n",
      "Epoch 69/300, Loss: 0.6870116272142955\n",
      "Epoch 70/300, Loss: 0.6869774325972512\n",
      "Epoch 71/300, Loss: 0.6869435182639531\n",
      "Epoch 72/300, Loss: 0.6869098870527177\n",
      "Epoch 73/300, Loss: 0.6868765339964912\n",
      "Epoch 74/300, Loss: 0.686843467610223\n",
      "Epoch 75/300, Loss: 0.6868106729927517\n",
      "Epoch 76/300, Loss: 0.686778147305761\n",
      "Epoch 77/300, Loss: 0.6867458870013555\n",
      "Epoch 78/300, Loss: 0.6867139112381708\n",
      "Epoch 79/300, Loss: 0.7117038677845683\n",
      "Epoch 80/300, Loss: 0.6866658124185744\n",
      "Epoch 81/300, Loss: 0.6866344617945808\n",
      "Epoch 82/300, Loss: 0.6866034446727662\n",
      "Epoch 83/300, Loss: 0.6865726837090084\n",
      "Epoch 84/300, Loss: 0.6865421803224654\n",
      "Epoch 85/300, Loss: 0.6865119330939793\n",
      "Epoch 86/300, Loss: 0.686481923574493\n",
      "Epoch 87/300, Loss: 0.6864521737609591\n",
      "Epoch 88/300, Loss: 0.6864226644947415\n",
      "Epoch 89/300, Loss: 0.6863934014524732\n",
      "Epoch 90/300, Loss: 0.686364390310787\n",
      "Epoch 91/300, Loss: 0.6863356119110471\n",
      "Epoch 92/300, Loss: 0.6863070690915698\n",
      "Epoch 93/300, Loss: 0.6862787668194089\n",
      "Epoch 94/300, Loss: 0.6862507029658272\n",
      "Epoch 95/300, Loss: 0.6862228690158754\n",
      "Epoch 96/300, Loss: 0.6861952678078697\n",
      "Epoch 97/300, Loss: 0.686167897213073\n",
      "Epoch 98/300, Loss: 0.686140755812327\n",
      "Epoch 99/300, Loss: 0.68611383721942\n",
      "Epoch 100/300, Loss: 0.6860871499493009\n",
      "Epoch 101/300, Loss: 0.6860606854870206\n",
      "Epoch 102/300, Loss: 0.6860344324793134\n",
      "Epoch 103/300, Loss: 0.6860084065369197\n",
      "Epoch 104/300, Loss: 0.685982597725732\n",
      "Epoch 105/300, Loss: 0.6859570010786965\n",
      "Epoch 106/300, Loss: 0.6859316187245506\n",
      "Epoch 107/300, Loss: 0.6859064542111897\n",
      "Epoch 108/300, Loss: 0.685881488379978\n",
      "Epoch 109/300, Loss: 0.6858567354224977\n",
      "Epoch 110/300, Loss: 0.6858322081111726\n",
      "Epoch 111/300, Loss: 0.6858078603233609\n",
      "Epoch 112/300, Loss: 0.6857837282475971\n",
      "Epoch 113/300, Loss: 0.6857597948539824\n",
      "Epoch 114/300, Loss: 0.6857360580137798\n",
      "Epoch 115/300, Loss: 0.6857125241132009\n",
      "Epoch 116/300, Loss: 0.6856891888947714\n",
      "Epoch 117/300, Loss: 0.6856660417148045\n",
      "Epoch 118/300, Loss: 0.6856430953457242\n",
      "Epoch 119/300, Loss: 0.6856203384342647\n",
      "Epoch 120/300, Loss: 0.6855977745283217\n",
      "Epoch 121/300, Loss: 0.6855754043374743\n",
      "Epoch 122/300, Loss: 0.6855532143797193\n",
      "Epoch 123/300, Loss: 0.685531212460427\n",
      "Epoch 124/300, Loss: 0.6855093950317019\n",
      "Epoch 125/300, Loss: 0.6854877549977529\n",
      "Epoch 126/300, Loss: 0.6854663015831084\n",
      "Epoch 127/300, Loss: 0.6854450213057655\n",
      "Epoch 128/300, Loss: 0.6854239240998313\n",
      "Epoch 129/300, Loss: 0.6854030028695152\n",
      "Epoch 130/300, Loss: 0.6853822632914498\n",
      "Epoch 131/300, Loss: 0.6853616897548948\n",
      "Epoch 132/300, Loss: 0.6853412836790085\n",
      "Epoch 133/300, Loss: 0.6853210557074774\n",
      "Epoch 134/300, Loss: 0.685300982424191\n",
      "Epoch 135/300, Loss: 0.6852810971793675\n",
      "Epoch 136/300, Loss: 0.6852613644940513\n",
      "Epoch 137/300, Loss: 0.6852418127514067\n",
      "Epoch 138/300, Loss: 0.6852224128586906\n",
      "Epoch 139/300, Loss: 0.685410955122539\n",
      "Epoch 140/300, Loss: 0.6851840317249298\n",
      "Epoch 141/300, Loss: 0.6851651278280076\n",
      "Epoch 142/300, Loss: 0.6851463779097512\n",
      "Epoch 143/300, Loss: 0.6851277819701603\n",
      "Epoch 144/300, Loss: 0.6851093357517606\n",
      "Epoch 145/300, Loss: 0.6850910520269757\n",
      "Epoch 146/300, Loss: 0.6850729045413789\n",
      "Epoch 147/300, Loss: 0.6850549238068717\n",
      "Epoch 148/300, Loss: 0.6850370920839763\n",
      "Epoch 149/300, Loss: 0.6850194065343767\n",
      "Epoch 150/300, Loss: 0.6850018841879708\n",
      "Epoch 151/300, Loss: 0.6849844888562248\n",
      "Epoch 152/300, Loss: 0.6849672432456698\n",
      "Epoch 153/300, Loss: 0.6849501416796729\n",
      "Epoch 154/300, Loss: 0.6849331898348672\n",
      "Epoch 155/300, Loss: 0.6972203151810736\n",
      "Epoch 156/300, Loss: 0.6848947200037184\n",
      "Epoch 157/300, Loss: 0.684878266283444\n",
      "Epoch 158/300, Loss: 0.6848619232575098\n",
      "Epoch 159/300, Loss: 0.6848457207282385\n",
      "Epoch 160/300, Loss: 0.6848296530189968\n",
      "Epoch 161/300, Loss: 0.6848137250968388\n",
      "Epoch 162/300, Loss: 0.8605972314271801\n",
      "Epoch 163/300, Loss: 0.6931754457099097\n",
      "Epoch 164/300, Loss: 0.6848073949416479\n",
      "Epoch 165/300, Loss: 0.6847916131927854\n",
      "Epoch 166/300, Loss: 0.6847759910992214\n",
      "Epoch 167/300, Loss: 0.6847605052448454\n",
      "Epoch 168/300, Loss: 0.6847451435668128\n",
      "Epoch 169/300, Loss: 0.6847299159992308\n",
      "Epoch 170/300, Loss: 0.6847148126079923\n",
      "Epoch 171/300, Loss: 0.6846998376505715\n",
      "Epoch 172/300, Loss: 0.6846849833215986\n",
      "Epoch 173/300, Loss: 0.6846702482019152\n",
      "Epoch 174/300, Loss: 0.6846556379681542\n",
      "Epoch 175/300, Loss: 0.6846411568777901\n",
      "Epoch 176/300, Loss: 0.6846267914488202\n",
      "Epoch 177/300, Loss: 0.6846125452291398\n",
      "Epoch 178/300, Loss: 0.6845984160900116\n",
      "Epoch 179/300, Loss: 0.6853427837292353\n",
      "Epoch 180/300, Loss: 0.7739979345767226\n",
      "Epoch 181/300, Loss: 0.6845331270070303\n",
      "Epoch 182/300, Loss: 0.6845196946745827\n",
      "Epoch 183/300, Loss: 0.6845063673598426\n",
      "Epoch 184/300, Loss: 0.6844931486107054\n",
      "Epoch 185/300, Loss: 0.6844800377175921\n",
      "Epoch 186/300, Loss: 0.6844670417762938\n",
      "Epoch 187/300, Loss: 0.6844541465952283\n",
      "Epoch 188/300, Loss: 0.6844413628180822\n",
      "Epoch 189/300, Loss: 0.6844286790915898\n",
      "Epoch 190/300, Loss: 0.6844160968349093\n",
      "Epoch 191/300, Loss: 0.6844036295300439\n",
      "Epoch 192/300, Loss: 0.684391255180041\n",
      "Epoch 193/300, Loss: 0.6843789908147994\n",
      "Epoch 194/300, Loss: 0.6843668215331578\n",
      "Epoch 195/300, Loss: 0.6843547608171191\n",
      "Epoch 196/300, Loss: 0.6843427973134177\n",
      "Epoch 197/300, Loss: 0.6843309296028954\n",
      "Epoch 198/300, Loss: 0.684319161943027\n",
      "Epoch 199/300, Loss: 0.9085490086829937\n",
      "Epoch 200/300, Loss: 0.68431560979003\n",
      "Epoch 201/300, Loss: 0.6843040691954749\n",
      "Epoch 202/300, Loss: 0.6842921191737765\n",
      "Epoch 203/300, Loss: 0.684280704174723\n",
      "Epoch 204/300, Loss: 2.242221566449129\n",
      "Epoch 205/300, Loss: 0.6842859004225049\n",
      "Epoch 206/300, Loss: 0.6943676478805996\n",
      "Epoch 207/300, Loss: 0.6842631122895649\n",
      "Epoch 208/300, Loss: 0.6842519051971889\n",
      "Epoch 209/300, Loss: 0.6842408102183115\n",
      "Epoch 210/300, Loss: 0.6842298046464012\n",
      "Epoch 211/300, Loss: 0.6842188835144043\n",
      "Epoch 212/300, Loss: 0.6842080652713776\n",
      "Epoch 213/300, Loss: 0.6842233070305416\n",
      "Epoch 214/300, Loss: 0.6841866913295928\n",
      "Epoch 215/300, Loss: 0.8210782197171024\n",
      "Epoch 216/300, Loss: 0.6857034847849891\n",
      "Epoch 217/300, Loss: 0.6845546804723286\n",
      "Epoch 218/300, Loss: 0.6841566527173633\n",
      "Epoch 219/300, Loss: 0.6987137645483017\n",
      "Epoch 220/300, Loss: 0.6841381646337963\n",
      "Epoch 221/300, Loss: 0.6841279459851128\n",
      "Epoch 222/300, Loss: 0.6841183255116144\n",
      "Epoch 223/300, Loss: 0.6841079259202594\n",
      "Epoch 224/300, Loss: 0.7585637782301221\n",
      "Epoch 225/300, Loss: 0.6840905391034626\n",
      "Epoch 226/300, Loss: 0.6840806993700209\n",
      "Epoch 227/300, Loss: 0.6840710476750419\n",
      "Epoch 228/300, Loss: 0.7133355623199826\n",
      "Epoch 229/300, Loss: 0.6840563054595675\n",
      "Epoch 230/300, Loss: 0.7468782238706592\n",
      "Epoch 231/300, Loss: 0.6840288859038126\n",
      "Epoch 232/300, Loss: 0.6840196798245112\n",
      "Epoch 233/300, Loss: 0.684010549670174\n",
      "Epoch 234/300, Loss: 0.6840014876354308\n",
      "Epoch 235/300, Loss: 0.6848446826140085\n",
      "Epoch 236/300, Loss: 0.6839837297087624\n",
      "Epoch 237/300, Loss: 0.6861601159686134\n",
      "Epoch 238/300, Loss: 0.6839741113640013\n",
      "Epoch 239/300, Loss: 0.7463617076476415\n",
      "Epoch 240/300, Loss: 0.6874579262165796\n",
      "Epoch 241/300, Loss: 0.6839412543035689\n",
      "Epoch 242/300, Loss: 0.6839327585129511\n",
      "Epoch 243/300, Loss: 0.6839244479224795\n",
      "Epoch 244/300, Loss: 0.6839160465058827\n",
      "Epoch 245/300, Loss: 0.6839077259813037\n",
      "Epoch 246/300, Loss: 0.7105426355486825\n",
      "Epoch 247/300, Loss: 0.6838931632893426\n",
      "Epoch 248/300, Loss: 0.766177128468241\n",
      "Epoch 249/300, Loss: 0.7138506141269491\n",
      "Epoch 250/300, Loss: 0.6876960361287707\n",
      "Epoch 251/300, Loss: 0.7295979562969435\n",
      "Epoch 252/300, Loss: 0.758592518573139\n",
      "Epoch 253/300, Loss: 0.6838297006629762\n",
      "Epoch 254/300, Loss: 0.683822172738257\n",
      "Epoch 255/300, Loss: 0.6838856481370472\n",
      "Epoch 256/300, Loss: 0.8166825283551589\n",
      "Epoch 257/300, Loss: 0.6838236238275256\n",
      "Epoch 258/300, Loss: 0.6838148023400988\n",
      "Epoch 259/300, Loss: 0.8176236812557492\n",
      "Epoch 260/300, Loss: 0.6838021108082363\n",
      "Epoch 261/300, Loss: 0.7438837274731624\n",
      "Epoch 262/300, Loss: 0.6837908299196334\n",
      "Epoch 263/300, Loss: 0.6837834566831589\n",
      "Epoch 264/300, Loss: 0.6837763112215769\n",
      "Epoch 265/300, Loss: 0.683769214011374\n",
      "Epoch 266/300, Loss: 0.6837621778249741\n",
      "Epoch 267/300, Loss: 0.6837551891803741\n",
      "Epoch 268/300, Loss: 0.6837482693649474\n",
      "Epoch 269/300, Loss: 0.6837413942530042\n",
      "Epoch 270/300, Loss: 0.6837345759073893\n",
      "Epoch 271/300, Loss: 0.6837278143281028\n",
      "Epoch 272/300, Loss: 0.6855351726214091\n",
      "Epoch 273/300, Loss: 0.683714149963288\n",
      "Epoch 274/300, Loss: 0.9468014835756982\n",
      "Epoch 275/300, Loss: 0.6836849195616586\n",
      "Epoch 276/300, Loss: 0.6909745498782113\n",
      "Epoch 277/300, Loss: 0.683670882667814\n",
      "Epoch 278/300, Loss: 0.7693439814306441\n",
      "Epoch 279/300, Loss: 0.6836622116111574\n",
      "Epoch 280/300, Loss: 0.683655763665835\n",
      "Epoch 281/300, Loss: 0.683649676896277\n",
      "Epoch 282/300, Loss: 0.6836436454738889\n",
      "Epoch 283/300, Loss: 0.6836376566262472\n",
      "Epoch 284/300, Loss: 0.6836317160299846\n",
      "Epoch 285/300, Loss: 0.6841105967760086\n",
      "Epoch 286/300, Loss: 0.6836198923133668\n",
      "Epoch 287/300, Loss: 0.6836140915041878\n",
      "Epoch 288/300, Loss: 0.6841752876838049\n",
      "Epoch 289/300, Loss: 0.6836027034691402\n",
      "Epoch 290/300, Loss: 0.6835970360608328\n",
      "Epoch 291/300, Loss: 0.6929278753343082\n",
      "Epoch 292/300, Loss: 0.7425157904979729\n",
      "Epoch 293/300, Loss: 0.6852544609989438\n",
      "Epoch 294/300, Loss: 0.6835822683005106\n",
      "Epoch 295/300, Loss: 0.6835772373846599\n",
      "Epoch 296/300, Loss: 0.6842634869473321\n",
      "Epoch 297/300, Loss: 0.6835658081940242\n",
      "Epoch 298/300, Loss: 0.6835604501622063\n",
      "Epoch 299/300, Loss: 0.6845481750510988\n",
      "Epoch 300/300, Loss: 0.6868831508216404\n",
      "Image-Only Model\n",
      "Test Accuracy: 57.142857142857146%\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usama.Khatab\\Projects\\miniconda3\\envs\\hackathon\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Experiment: Train Clinical-only and Image-only Models\n",
    "for modality, feature_set in [('Clinical', train_clinical_embeddings), ('Image', train_image_features)]:\n",
    "    print(f\"\\nTraining {modality}-Only Model\")\n",
    "    \n",
    "    train_labels = train_labels.clone().detach().float().view(-1, 1)\n",
    "    test_labels = test_labels.clone().detach().float().view(-1, 1)\n",
    "    train_features = torch.tensor(feature_set.reshape(len(feature_set), -1))\n",
    "    test_features = torch.tensor((test_clinical_embeddings if modality == 'Clinical' else test_image_features).reshape(len(test_labels), -1))\n",
    "\n",
    "    print(\"Train Features: \", train_features.shape)\n",
    "    print(\"Test Features: \", test_features.shape)\n",
    "    print(\"Train Labels: \", train_labels.shape)\n",
    "    print(\"Test Labels: \", test_labels.shape)\n",
    "    \n",
    "    train_dataset = TensorDataset(train_features, train_labels)\n",
    "    test_dataset = TensorDataset(test_features, test_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    test_model = MLP(input_dim=train_features.shape[1])\n",
    "    test_optimizer = torch.optim.Adam(test_model.parameters(), lr=learning_rate, weight_decay=w_decay)\n",
    "    \n",
    "    epochs = 300\n",
    "    for epoch in range(epochs):\n",
    "        test_model.train()\n",
    "        total_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            test_optimizer.zero_grad()\n",
    "            output = test_model(features.float())\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(output.view(-1), labels.view(-1))\n",
    "            loss.backward()\n",
    "            test_optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "    \n",
    "    test_model.eval()\n",
    "    all_labels, all_predictions = [], []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            output = test_model(features.float())\n",
    "\n",
    "            pred = torch.sigmoid(output.squeeze()) >= 0.5\n",
    "            \n",
    "            all_labels.append(labels.cpu().numpy().flatten())\n",
    "            all_predictions.append(pred.cpu().numpy().flatten())\n",
    "\n",
    "            # Count correct predictions\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)  # Increment by the number of samples in this batch\n",
    "    \n",
    "    print(f\"{modality}-Only Model\")\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy}%\")\n",
    "\n",
    "    # Calculate Metrics\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
